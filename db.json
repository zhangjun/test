{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/sidebar.js","path":"js/sidebar.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"af9cd1ee9671ac0b852da61def0887fb8f72ce88","modified":1759157989932},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1759156796330},{"_id":"themes/next/.gitattributes","hash":"ec43734985e1cafd53d88ded3020103f7416123c","modified":1759156796331},{"_id":"themes/next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1759156796332},{"_id":"themes/next/.gitignore","hash":"417520c4dbbeab9c7e3ab10d944da0886366a0ee","modified":1759156796332},{"_id":"themes/next/README.md","hash":"6f1bf93dbccc8545872fe27b4693fda59cdbfb89","modified":1759156796332},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1759156796332},{"_id":"themes/next/eslint.config.js","hash":"14b5636ce297048ea6835f7db4b1e4cf625a2c79","modified":1759156796333},{"_id":"themes/next/.stylelintrc","hash":"9346cca6a24256a79f433670da634e8329f9a2a9","modified":1759156796332},{"_id":"themes/next/_config.yml","hash":"d2439bf154add96bc71d1bbf7dc01fde2b4ba287","modified":1759156796332},{"_id":"themes/next/.githooks/pre-commit","hash":"f473eac1aaaa96c947d67988bbed140bbab1a821","modified":1759156796331},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"e78ec4eb6ce49a44ec321c49424ee0a0c1846537","modified":1759156796331},{"_id":"themes/next/.githooks/install.js","hash":"0977cb57c8b91ea166b5bbc481e4ddeaf77e9b18","modified":1759156796331},{"_id":"themes/next/renovate.json","hash":"cb29cc16e61b0b8a6dac34657d76822ae29ad5aa","modified":1759156796340},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"330656d93b6c03df9fb1f2f0e3534c971969473b","modified":1759156796331},{"_id":"themes/next/_vendors.yml","hash":"3a282d441261c607928c34bea71ccbf92f910a97","modified":1759156796332},{"_id":"themes/next/.github/config.yml","hash":"7984e665e9de481a0e0e51fca5668337713f810b","modified":1759156796331},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"2296426023773991b0c48c7fffeb790baca59b19","modified":1759156796331},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1759156796331},{"_id":"themes/next/package.json","hash":"9bc8c49e092de0ba63c327e316c09a00b657e481","modified":1759156796340},{"_id":"themes/next/.github/label-commenter-config.yml","hash":"22d7dd8661cb2f22ff271ee96e444457814c66fd","modified":1759156796331},{"_id":"themes/next/.github/labeler.yml","hash":"b7f67daa031c535940be82ea96f0fb9db59e1022","modified":1759156796331},{"_id":"themes/next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1759156796333},{"_id":"themes/next/.github/release.yml","hash":"112310b81f959747f8eaafc2ca2150e1dcf916d8","modified":1759156796331},{"_id":"themes/next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1759156796333},{"_id":"themes/next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1759156796334},{"_id":"themes/next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1759156796334},{"_id":"themes/next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1759156796333},{"_id":"themes/next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1759156796334},{"_id":"themes/next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1759156796334},{"_id":"themes/next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1759156796334},{"_id":"themes/next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1759156796334},{"_id":"themes/next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1759156796334},{"_id":"themes/next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1759156796334},{"_id":"themes/next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1759156796334},{"_id":"themes/next/languages/default.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1759156796334},{"_id":"themes/next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1759156796334},{"_id":"themes/next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1759156796334},{"_id":"themes/next/languages/pt.yml","hash":"b62faaa767a45a613dd042b5f1903675eb5a8cf9","modified":1759156796334},{"_id":"themes/next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1759156796335},{"_id":"themes/next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1759156796335},{"_id":"themes/next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1759156796334},{"_id":"themes/next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1759156796334},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1759156796333},{"_id":"themes/next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1759156796334},{"_id":"themes/next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1759156796335},{"_id":"themes/next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1759156796335},{"_id":"themes/next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1759156796335},{"_id":"themes/next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1759156796335},{"_id":"themes/next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1759156796335},{"_id":"themes/next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1759156796335},{"_id":"themes/next/layout/_layout.njk","hash":"b17d44bd7379c23241053a0b7fbd38c9c43cc239","modified":1759156796335},{"_id":"themes/next/languages/zh-TW.yml","hash":"5c0f00cdac3f4727b880dd223f622a535736fa8e","modified":1759156796335},{"_id":"themes/next/languages/zh-HK.yml","hash":"8eb6a9f231ce1bfa54cc54418ccf14f01dcc9a31","modified":1759156796335},{"_id":"themes/next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1759156796340},{"_id":"themes/next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1759156796340},{"_id":"themes/next/layout/page.njk","hash":"af6d7570621be760536c216a56d74e40a1cceae2","modified":1759156796340},{"_id":"themes/next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1759156796340},{"_id":"themes/next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1759156796340},{"_id":"themes/next/test/index.js","hash":"b08b244fb6fa940aa2ea186e699edaaff7825d8f","modified":1759156796352},{"_id":"themes/next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1759156796340},{"_id":"themes/next/.github/ISSUE_TEMPLATE/config.yml","hash":"c40ae7903b6cc99f94c9d45ac7ba8c2850bb1309","modified":1759156796331},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.yml","hash":"3f19cbf0c2b2fee6bf3788870b842c9ccc1425ca","modified":1759156796331},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.yml","hash":"10eca518b91a19984f6a5a912d41222042f61d63","modified":1759156796331},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.yml","hash":"fbc3062cd4591c8329fab9db72803746f0d11920","modified":1759156796331},{"_id":"themes/next/.github/workflows/label-commenter.yml","hash":"28efdf3aa8c8e352a832e845012ae8168366bbe5","modified":1759156796332},{"_id":"themes/next/.github/workflows/codeql.yml","hash":"a2bd1ba81aad0d320ca92d09b42a7a7d807b634f","modified":1759156796332},{"_id":"themes/next/.github/workflows/npm-publish.yml","hash":"9c0f654e4c8ce57ae0efe3b4c5b988464a048b1f","modified":1759156796332},{"_id":"themes/next/.github/workflows/linter.yml","hash":"79707fba5919a843d18fd40c2e7f8c73387c162a","modified":1759156796332},{"_id":"themes/next/.github/workflows/lock.yml","hash":"275650a678684523f37187d0532f96aa0f49ce00","modified":1759156796332},{"_id":"themes/next/.github/workflows/labeler.yml","hash":"48e6e1e1406705379112766f4fd8388e1c53b281","modified":1759156796332},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1759156796333},{"_id":"themes/next/docs/zh-CN/README.md","hash":"93064dbd1a461d55c7c07a04626294c8150b4d1b","modified":1759156796333},{"_id":"themes/next/docs/ru/README.md","hash":"30e929e1138445534a6f46d64667c17273337acf","modified":1759156796333},{"_id":"themes/next/layout/_macro/post-collapse.njk","hash":"313637fe3569f98fd926e8cd0fcc75d098eb6e6e","modified":1759156796335},{"_id":"themes/next/layout/_macro/sidebar.njk","hash":"85f3a2ab22601a9606f2f630289db1363b98018f","modified":1759156796336},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1759156796333},{"_id":"themes/next/layout/_macro/post.njk","hash":"ec9bb9c5ede773c02f0c8d8475245a8a437a2b71","modified":1759156796335},{"_id":"themes/next/layout/_partials/comments.njk","hash":"390d6cc85dca43541bd957a8a35c72d75b37ca72","modified":1759156796336},{"_id":"themes/next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1759156796336},{"_id":"themes/next/layout/_partials/footer.njk","hash":"fbf8232cacf0df87e88e74860be66c9f86018302","modified":1759156796336},{"_id":"themes/next/layout/_partials/widgets.njk","hash":"d83fb59f02c5e6630a7770401a05c02f6f07358b","modified":1759156796337},{"_id":"themes/next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1759156796337},{"_id":"themes/next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1759156796337},{"_id":"themes/next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1759156796339},{"_id":"themes/next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1759156796339},{"_id":"themes/next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1759156796339},{"_id":"themes/next/layout/_scripts/vendors.njk","hash":"7261e24287984853c8ef08cda8bbc80cacf9bd6f","modified":1759156796337},{"_id":"themes/next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1759156796339},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1759156796341},{"_id":"themes/next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1759156796340},{"_id":"themes/next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1759156796341},{"_id":"themes/next/layout/_scripts/index.njk","hash":"2a7dfffebad19b67dc9e3b2a6b2986d0630ef930","modified":1759156796337},{"_id":"themes/next/scripts/helpers/engine.js","hash":"83235f2879567eb8686431c9554a4b99f14ab665","modified":1759156796341},{"_id":"themes/next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1759156796341},{"_id":"themes/next/scripts/filters/minify.js","hash":"43db8690d67c7545c5f081dbdc2601a0b2a16c5a","modified":1759156796341},{"_id":"themes/next/scripts/helpers/font.js","hash":"4c84d45daac86396edf656d2a8abe6e7583491ea","modified":1759156796342},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1759156796342},{"_id":"themes/next/.github/workflows/tester.yml","hash":"d08ddee2e1bf7e2916a8618621b41f854e88ba43","modified":1759156796332},{"_id":"themes/next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1759156796342},{"_id":"themes/next/source/css/_colors.styl","hash":"ebfe0954e3931431f46f913abe08d0212e06e7c2","modified":1759156796343},{"_id":"themes/next/scripts/helpers/next-vendors.js","hash":"af3946a595f997eb43d9af87428e4898c9acbc82","modified":1759156796342},{"_id":"themes/next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1759156796342},{"_id":"themes/next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1759156796349},{"_id":"themes/next/source/css/_mixins.styl","hash":"7946f1e32a76dd682ceebe374e1c9e21c9ac24e8","modified":1759156796347},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"4bc2eb87f3fa26981652f517d1ab3f81de2ab89d","modified":1759156796342},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1759156796342},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1759156796342},{"_id":"themes/next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1759156796342},{"_id":"themes/next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1759156796348},{"_id":"themes/next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1759156796342},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"f57f7e09eb6220f681fa8385082b0960502ce5c4","modified":1759156796342},{"_id":"themes/next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1759156796342},{"_id":"themes/next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1759156796342},{"_id":"themes/next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1759156796342},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"7d7bbc4a9970bd4c5449bc71b94364a8ec61e5d2","modified":1759156796342},{"_id":"themes/next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1759156796342},{"_id":"themes/next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1759156796343},{"_id":"themes/next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1759156796342},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1759156796349},{"_id":"themes/next/source/images/logo-algolia-nebula-blue-full.svg","hash":"a38c6d92b368bfc42c72ad799ad03f3274957065","modified":1759156796349},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1759156796349},{"_id":"themes/next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1759156796349},{"_id":"themes/next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1759156796349},{"_id":"themes/next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1759156796349},{"_id":"themes/next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1759156796349},{"_id":"themes/next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1759156796342},{"_id":"themes/next/source/js/motion.js","hash":"6f751f5c9499a39d7c5e1d323db3260342dd9431","modified":1759156796349},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1759156796349},{"_id":"themes/next/source/js/next-boot.js","hash":"d434a2a8543fb09245eaf2bc6ca123435bfa4dbb","modified":1759156796349},{"_id":"themes/next/source/js/bookmark.js","hash":"9ba4cceafd12c6d5ba8a6b986a046ef8319a7811","modified":1759156796349},{"_id":"themes/next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1759156796349},{"_id":"themes/next/source/js/pjax.js","hash":"694b271819aab37ce473b15db9e6aded971d82e5","modified":1759156796350},{"_id":"themes/next/source/js/schedule.js","hash":"9c41a73ed3e8db8ca4cb53633b6f616279a5a7bd","modified":1759156796350},{"_id":"themes/next/source/js/utils.js","hash":"6734719bb74e4d9818992b0e4a745c2a1aefd5e2","modified":1759156796351},{"_id":"themes/next/source/js/sidebar.js","hash":"2ee359ae48273b01ba1e0768704524e08702c7eb","modified":1759156796350},{"_id":"themes/next/test/helpers/font.js","hash":"a1e7c7f7cff915c137ae8bbebfa032b656cc19e1","modified":1759156796352},{"_id":"themes/next/test/helpers/index.js","hash":"63ba28afed697f7b3574436b1133b8ecc9c0c357","modified":1759156796352},{"_id":"themes/next/test/tags/button.js","hash":"17a82f3962472277dc33d99d9590ac0c926e634c","modified":1759156796352},{"_id":"themes/next/test/helpers/next-url.js","hash":"52497c848a74f5716f3de0d20d7803944e139ceb","modified":1759156796352},{"_id":"themes/next/test/tags/center-quote.js","hash":"170312d9721897289c8f4bf8285ce1c9a0ba8aed","modified":1759156796352},{"_id":"themes/next/test/tags/label.js","hash":"7a86d6c9e6c9774a375498c024c9c3f15e719ba1","modified":1759156796352},{"_id":"themes/next/test/tags/caniuse.js","hash":"dad9fddae90beef82a7c77420b4063f6c4862d66","modified":1759156796352},{"_id":"themes/next/test/tags/index.js","hash":"e8779e54f0979b221858f8bb74dd081bb503b910","modified":1759156796352},{"_id":"themes/next/test/tags/group-pictures.js","hash":"3ca6fc85f078ec1ff3d8df438bfbce438390b815","modified":1759156796352},{"_id":"themes/next/test/tags/link-grid.js","hash":"7f1a1b4189309278164fc5e70ed7cee32609d835","modified":1759156796352},{"_id":"themes/next/test/tags/mermaid.js","hash":"df5ca57bb57c517b2381580b81f7deda7d2fff65","modified":1759156796352},{"_id":"themes/next/test/tags/note.js","hash":"81737656ebb8050fda3af129454d26650d201a0f","modified":1759156796352},{"_id":"themes/next/test/tags/pdf.js","hash":"69be476322e19f25447c8a4ef55e5c96df829fc1","modified":1759156796352},{"_id":"themes/next/test/tags/video.js","hash":"8acc7ddd41975b6dba4263ee99c87c821d64b663","modified":1759156796352},{"_id":"themes/next/test/tags/tabs.js","hash":"6aa68012f7cfd0e7cb1587f74d775b91fc014535","modified":1759156796352},{"_id":"themes/next/test/validate/index.js","hash":"54d9225b07c7c3313d63da96261b092406e7e225","modified":1759156796352},{"_id":"themes/next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1759156796336},{"_id":"themes/next/layout/_partials/head/head-unique.njk","hash":"93c1d103d9d16581c944c51f3d0638f57c80ee41","modified":1759156796336},{"_id":"themes/next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1759156796336},{"_id":"themes/next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1759156796336},{"_id":"themes/next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1759156796336},{"_id":"themes/next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1759156796336},{"_id":"themes/next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1759156796336},{"_id":"themes/next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1759156796336},{"_id":"themes/next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1759156796336},{"_id":"themes/next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1759156796336},{"_id":"themes/next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1759156796337},{"_id":"themes/next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1759156796336},{"_id":"themes/next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1759156796336},{"_id":"themes/next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1759156796337},{"_id":"themes/next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1759156796337},{"_id":"themes/next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1759156796337},{"_id":"themes/next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1759156796337},{"_id":"themes/next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1759156796337},{"_id":"themes/next/layout/_partials/search/index.njk","hash":"6ad43135bd3aecf933ffdd750763e27ade36f97c","modified":1759156796337},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1759156796338},{"_id":"themes/next/layout/_partials/sidebar/site-overview.njk","hash":"bc5708e38b6070dff0cab6bf9480971017ce4dda","modified":1759156796337},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"1efeeda00db08a3c033798228dd0092ee532cc36","modified":1759156796338},{"_id":"themes/next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1759156796338},{"_id":"themes/next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1759156796338},{"_id":"themes/next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1759156796339},{"_id":"themes/next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1759156796338},{"_id":"themes/next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1759156796338},{"_id":"themes/next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1759156796339},{"_id":"themes/next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1759156796339},{"_id":"themes/next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1759156796339},{"_id":"themes/next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1759156796339},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"55c2468b2b7f035881d494085527d6554f37b556","modified":1759156796339},{"_id":"themes/next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1759156796339},{"_id":"themes/next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1759156796339},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1759156796339},{"_id":"themes/next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1759156796339},{"_id":"themes/next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1759156796339},{"_id":"themes/next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1759156796338},{"_id":"themes/next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1759156796339},{"_id":"themes/next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1759156796338},{"_id":"themes/next/scripts/events/lib/highlight.js","hash":"8a8f752260be5b8098393f9879b61ffe904465e8","modified":1759156796340},{"_id":"themes/next/scripts/events/lib/config.js","hash":"00af4f5f9a79eaccf051f9e372d233d65d44c8a5","modified":1759156796340},{"_id":"themes/next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1759156796340},{"_id":"themes/next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1759156796340},{"_id":"themes/next/scripts/events/lib/utils.js","hash":"5942feb3f31ed3480bf50b0f5a4a305b5bdca3d6","modified":1759156796340},{"_id":"themes/next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1759156796340},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1759156796340},{"_id":"themes/next/layout/_third-party/search/algolia-search.njk","hash":"41b28f05e6233fb37700f7151f55868be10a0965","modified":1759156796339},{"_id":"themes/next/scripts/events/lib/vendors.js","hash":"e2b4a9d6b08155735ec336eedc506763d5671821","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"5dcaefacbcb9e99d87348d2f7158dbfb4d47b405","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1759156796341},{"_id":"themes/next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1759156796339},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1759156796341},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1759156796341},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"20d5c6aa136bbb55e03906d98ee90ad3fbaa80a7","modified":1759156796348},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"879b49f693af0c04c285b2dd0c9cccaf77347b7c","modified":1759156796348},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b724edca546373d5eaf9b3602868f971c9094cf6","modified":1759156796348},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1759156796348},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"2c800eaab6c613e5d091be2111aaa786641aa0c2","modified":1759156796348},{"_id":"themes/next/source/js/third-party/addtoany.js","hash":"a772605646dcfb67620a10ee8ef23c38a6d19d80","modified":1759156796350},{"_id":"themes/next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1759156796345},{"_id":"themes/next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1759156796351},{"_id":"themes/next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1759156796351},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"48b2dfc04df6409c6e0736ccc11462ad97d349b1","modified":1759156796345},{"_id":"themes/next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1759156796351},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b8445c828d78a38e2de50bdc86b3bff66285ea0f","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1759156796343},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f316ba87f8d3299677fbf8345e1e993c35210e2e","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"69c66aab4651e2e7ae9e65f08600144970648c60","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1759156796346},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e31f6adbb22a451f07e4661cff9a2f12e4e99a36","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"c29a827e82d2820ed8977c92994da73721200fac","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"bcbf498d8d3ecea84324f0a59b7f95f389a52b8d","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f23c53e32d140091b819be2603d1afbbb5d66933","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"a03f16ffc7dfdbdc6053f9fd68d77257ba0c559e","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1759156796347},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"dc03835e42d82eaf2633cf3b627990ad3e1f5967","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"a92c4eb16bdb7806079467eb022ccf193bb0f794","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1759156796348},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e792a6233e1d4dbc5fd2f10ae97b7a790b82568b","modified":1759156796348},{"_id":"themes/next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1759156796350},{"_id":"themes/next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1759156796350},{"_id":"themes/next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1759156796350},{"_id":"themes/next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1759156796350},{"_id":"themes/next/source/js/third-party/comments/disqus.js","hash":"3631db0315bdeaa420091a9febb6fa3421a2bdb4","modified":1759156796350},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1759156796348},{"_id":"themes/next/source/js/third-party/comments/changyan.js","hash":"6c65d5a585b7dd75e5f0fa6ef2dc85d0bcd1e58f","modified":1759156796350},{"_id":"themes/next/source/js/third-party/comments/disqusjs.js","hash":"e01b42846ffcabc676c3bdd9d89e8cafc084e20b","modified":1759156796350},{"_id":"themes/next/source/js/third-party/comments/isso.js","hash":"917d1a2bbae6cc8817ce37abc17800b1740b2517","modified":1759156796350},{"_id":"themes/next/source/js/third-party/comments/gitalk.js","hash":"03eb13679fc701c2ab91e502ccd26aacc37e7999","modified":1759156796350},{"_id":"themes/next/source/js/third-party/comments/utterances.js","hash":"743f389fc5669e486c8804d7199a11542ff9bc11","modified":1759156796351},{"_id":"themes/next/source/js/third-party/comments/livere.js","hash":"e35e5a90a70a96117509368423726c6a56041ea2","modified":1759156796350},{"_id":"themes/next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1759156796350},{"_id":"themes/next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1759156796351},{"_id":"themes/next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1759156796350},{"_id":"themes/next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1759156796351},{"_id":"themes/next/source/js/third-party/search/local-search.js","hash":"3968d972f47b79acc6c3fe44028bad77c9c5aab7","modified":1759156796351},{"_id":"themes/next/source/js/third-party/search/algolia-search.js","hash":"6b3fa841e48d8637a33530dd48c8ab1ef317323c","modified":1759156796351},{"_id":"themes/next/source/js/third-party/tags/mermaid.js","hash":"ae1c0c6c079594936de1aea756eb58992f8fb0e0","modified":1759156796351},{"_id":"themes/next/source/js/third-party/tags/pdf.js","hash":"7e6ad201d2c9d682261209db5dba07e9608fb42a","modified":1759156796351},{"_id":"themes/next/source/js/third-party/statistics/firestore.js","hash":"fec1c5c913237112b2cc6fb7d1e73b789bf508f8","modified":1759156796351},{"_id":"themes/next/source/js/third-party/tags/wavedrom.js","hash":"71efb52a4c44c64c2b17edd4638d54ec884bd4c7","modified":1759156796351},{"_id":"themes/next/source/js/third-party/statistics/lean-analytics.js","hash":"171889aaab60704f87cfe9a05871f493ac292b47","modified":1759156796351},{"_id":"themes/next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1759156796344},{"_id":"themes/next/source/css/_common/outline/sidebar/index.styl","hash":"21acb11e397526132605eef23bde7b307aa1eab5","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"ce36bf1602233298e0351b4babc592315529eb26","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-copyright.styl","hash":"56805b77fe236fac19e19c716a49363bcf986311","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1759156796346},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1759156796346},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"741566d6ac5f852b5c8dee6a8996b65e48e7c97f","modified":1759156796346},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1759156796344},{"_id":"themes/next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"a3dd3edea9c01b66b28a8367185269b9dcc3bdee","modified":1759156796345},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1759156796345},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"51a97a33879289904cb523ddc2d88b5b0c60fa72","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/post/post-body.styl","hash":"a2e977137892d4fa234ef5fdc6a92926d259b19d","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"809bab3414b1eb1ae44444eb821126868f764414","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/post/post-footer.styl","hash":"bb089299f87793bd5eff80c6375d4e796367b67b","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"026cd5735fd2a75bb60b7bf8bd09139583d602b9","modified":1759156796343},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"424de4f64b12c521e8c6bfbc711d7961490ab36e","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"b47fb36915962309553ff7fb1782341585ed2b76","modified":1759156796344},{"_id":"themes/next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/highlight/index.styl","hash":"9b0217e1caecd91e05572c7e8e52d32016ca312f","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"5c31f3a86e4e6fbf2f8419415620461fa8a63c56","modified":1759156796346},{"_id":"themes/next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1759156796344},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1759156796347},{"_id":"themes/next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1759156796344},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"1874e2b5d86cdeeaf2ccdc2669146a2b0c72d9db","modified":1759156796344},{"_id":"themes/next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"49329a7144f3413d1c832e52a1f4954171ef11e1","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"8213015d9cae45d2c9945f8aba9d8db39c734efc","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1759156796346},{"_id":"themes/next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"c3be8b0738f693e750486bb71769c3dbbec174cc","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1759156796347},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1759156796347},{"_id":"source/_posts/TNN.md","hash":"65d92e3550b9e76448759823705ec71a22e0cd76","modified":1759157989932},{"_id":"source/about-me/index.md","hash":"1e79e163010002416df49462639d17895e92e1fb","modified":1759157989940},{"_id":"source/_posts/article/cmake.md","hash":"793f7d3871494b8b098d6f738fd225bde0d85b6f","modified":1759157989939},{"_id":"source/_posts/article/conv.md","hash":"5597706208868155be65bab85303e9cbd1486213","modified":1759157989935},{"_id":"source/_posts/article/cudnn.md","hash":"f4ad53b2951573a1e51efb459b0463720b2ecfa1","modified":1759157989934},{"_id":"source/_posts/article/daily_note.md","hash":"3f0d686855fa27bfb24b514f3802416e229e1c6a","modified":1759157989935},{"_id":"source/_posts/article/docker.md","hash":"d9faa0ad25bc43d1cf572908ec73ecc83885706d","modified":1759157989938},{"_id":"source/_posts/article/flags.md","hash":"cc7b2a08a58b2ff3c6114468e430384576ebe8c7","modified":1759157989938},{"_id":"source/_posts/article/flags-1.md","hash":"31ad3ea3eaed71953bc6a50b814c3f650a6e2ce2","modified":1759157989935},{"_id":"source/_posts/article/hexo.md","hash":"12fe1eb80cb5375a1cffe2256acdfdf8bf18d2fe","modified":1759157989937},{"_id":"source/_posts/article/models.md","hash":"d9543432b1adf85d194f68503b22675d14e55bf6","modified":1759157989937},{"_id":"source/_posts/article/modern_cpp.md","hash":"0975498654ab39aa4b9ebdf79f3b85698fa7df72","modified":1759157989936},{"_id":"source/_posts/article/metal_basic.md","hash":"69676eeb5120b274d8606c173b1417f1eb81f44e","modified":1759157989937},{"_id":"source/_posts/article/nsight-system.md","hash":"afe52d0f7d0fd73bc3a6a9be20e9ebffee08e674","modified":1759157989934},{"_id":"source/_posts/article/notes.md","hash":"aed23b0a83bb515157603043dc93fba796be0dee","modified":1759157989938},{"_id":"source/_posts/article/opencl.md","hash":"3280dcbb2a3cc22f0b0a75b3757fcf5c34f660d8","modified":1759157989933},{"_id":"source/_posts/article/mactex.md","hash":"89a2b04ee3cc4798276e63b5d1f0d1a92ef7cdb3","modified":1759157989939},{"_id":"source/_posts/article/paddle-model.md","hash":"193961415c36bd0557a75ffcaa372e8202be9f19","modified":1759157989938},{"_id":"source/_posts/article/mnn.md","hash":"cb5fa4448340743cb29fb78bbf563e71746c265d","modified":1759157989933},{"_id":"source/_posts/article/paddlelite-metal.md","hash":"abc169a49d3ba91803a1c95a66859f15feb05555","modified":1759157989936},{"_id":"source/_posts/article/std-async.md","hash":"a5cc419d375f19f3f1bf49db172d360220a4f9ba","modified":1759157989939},{"_id":"source/_posts/article/paddle_inference.md","hash":"1e1d56f046e57a619c4757145ad8539ae056eea8","modified":1759157989935},{"_id":"source/_posts/article/shell.md","hash":"49d6a38381138588742a456a5966c6e9a215b44d","modified":1759157989939},{"_id":"source/_posts/article/paddle-lite.md","hash":"185716286225c06426dd9bb08c1c1ef3e1a6f5c2","modified":1759157989934},{"_id":"source/_posts/article/tensorrt.md","hash":"e3836d414ca638788588f5999efd604a409f8618","modified":1759157989935},{"_id":"source/_posts/article/vpn-service.md","hash":"6406fdf8fbbcd03e76a0be1458eaf8dc8cff5347","modified":1759157989934},{"_id":"source/_posts/article/thread-pool.md","hash":"421d91663d164b94e112e8ff1c947293f2259f85","modified":1759157989936},{"_id":"source/_posts/article/cplusplus/gcc.md","hash":"9b85b0858b79eaab5ebae54bddef855043ad967a","modified":1759157989934},{"_id":"source/_posts/article/workstealing.md","hash":"8ae71a533cbb29a9e5e0f1082b939b9ac0521485","modified":1759157989937},{"_id":"source/_posts/article/variadic-templates.md","hash":"d9ca0335a1c1fc39da139e488b0f99a9dc2ce3a6","modified":1759157989936},{"_id":"source/_posts/article/use-macos.md","hash":"b228b882b93853f55a262d194b25f0b253e08c67","modified":1759157989937}],"Category":[],"Data":[],"Page":[{"title":"about me","date":"2021-03-05T23:21:10.000Z","_content":"","source":"about-me/index.md","raw":"---\ntitle: about me\ndate: 2021-03-06 07:21:10\n---\n","updated":"2025-09-29T14:59:49.940Z","path":"about-me/index.html","comments":1,"layout":"page","_id":"cmg59905t0000otzk8q59cxrx","content":"","excerpt":"","more":""}],"Post":[{"title":"Hello World","date":"2020-12-31T23:17:58.000Z","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2021-01-01 07:17:58\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"updated":"2025-09-29T14:59:49.932Z","_id":"cmg58tjfa0000j4zk2mj8h77m","comments":1,"layout":"post","photos":[],"content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"TNN","date":"2021-03-05T23:17:58.000Z","_content":"本文主要介绍Tencent TNN编译使用。\n\n# 下载编译\n## 下载\n``` shell\ngit clone github.com/Tencent/TNN\n```\n其他需要cmake、opencv，单独安装\n## linux x86\n```\nmkdir build && cd build\ncmake .. -DTNN_X86_ENABLE=ON\n```\n\n\n# 模型部署示例\n``` c\nint main(){\n  return main();\n}\n```","source":"_posts/TNN.md","raw":"---\ntitle: TNN\ndate: 2021-03-06 07:17:58\ntags:\n---\n本文主要介绍Tencent TNN编译使用。\n\n# 下载编译\n## 下载\n``` shell\ngit clone github.com/Tencent/TNN\n```\n其他需要cmake、opencv，单独安装\n## linux x86\n```\nmkdir build && cd build\ncmake .. -DTNN_X86_ENABLE=ON\n```\n\n\n# 模型部署示例\n``` c\nint main(){\n  return main();\n}\n```","slug":"TNN","published":1,"updated":"2025-09-29T14:59:49.932Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905u0001otzkgxhbcee3","content":"<p>本文主要介绍Tencent TNN编译使用。</p>\n<h1 id=\"下载编译\"><a href=\"#下载编译\" class=\"headerlink\" title=\"下载编译\"></a>下载编译</h1><h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone github.com/Tencent/TNN</span><br></pre></td></tr></table></figure>\n<p>其他需要cmake、opencv，单独安装</p>\n<h2 id=\"linux-x86\"><a href=\"#linux-x86\" class=\"headerlink\" title=\"linux x86\"></a>linux x86</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build &amp;&amp; cd build</span><br><span class=\"line\">cmake .. -DTNN_X86_ENABLE=ON</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"模型部署示例\"><a href=\"#模型部署示例\" class=\"headerlink\" title=\"模型部署示例\"></a>模型部署示例</h1><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> main();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<p>本文主要介绍Tencent TNN编译使用。</p>\n<h1 id=\"下载编译\"><a href=\"#下载编译\" class=\"headerlink\" title=\"下载编译\"></a>下载编译</h1><h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone github.com/Tencent/TNN</span><br></pre></td></tr></table></figure>\n<p>其他需要cmake、opencv，单独安装</p>\n<h2 id=\"linux-x86\"><a href=\"#linux-x86\" class=\"headerlink\" title=\"linux x86\"></a>linux x86</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build &amp;&amp; cd build</span><br><span class=\"line\">cmake .. -DTNN_X86_ENABLE=ON</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"模型部署示例\"><a href=\"#模型部署示例\" class=\"headerlink\" title=\"模型部署示例\"></a>模型部署示例</h1><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> main();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"cmake","date":"2021-02-28T23:17:58.000Z","_content":"# cmake\n\n## cmake usage\n","source":"_posts/article/cmake.md","raw":"---\ntitle: cmake\ndate: 2021-03-01 07:17:58\ntags:\n---\n# cmake\n\n## cmake usage\n","slug":"article/cmake","published":1,"updated":"2025-09-29T14:59:49.939Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905v0002otzkdx3kfdu9","content":"<h1 id=\"cmake\"><a href=\"#cmake\" class=\"headerlink\" title=\"cmake\"></a>cmake</h1><h2 id=\"cmake-usage\"><a href=\"#cmake-usage\" class=\"headerlink\" title=\"cmake usage\"></a>cmake usage</h2>","excerpt":"","more":"<h1 id=\"cmake\"><a href=\"#cmake\" class=\"headerlink\" title=\"cmake\"></a>cmake</h1><h2 id=\"cmake-usage\"><a href=\"#cmake-usage\" class=\"headerlink\" title=\"cmake usage\"></a>cmake usage</h2>"},{"title":"cudnn","date":"2022-04-05T19:14:08.000Z","_content":"\n# cudnn 优化设置\n\n## cudnn deterministic\n设置为true，cudnn使用非确定性算法，能够自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。","source":"_posts/article/cudnn.md","raw":"---\ntitle: cudnn\ndate: 2022-04-06 03:14:08\ntags:\n---\n\n# cudnn 优化设置\n\n## cudnn deterministic\n设置为true，cudnn使用非确定性算法，能够自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。","slug":"article/cudnn","published":1,"updated":"2025-09-29T14:59:49.934Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0003otzk1l7v9fhh","content":"<h1 id=\"cudnn-优化设置\"><a href=\"#cudnn-优化设置\" class=\"headerlink\" title=\"cudnn 优化设置\"></a>cudnn 优化设置</h1><h2 id=\"cudnn-deterministic\"><a href=\"#cudnn-deterministic\" class=\"headerlink\" title=\"cudnn deterministic\"></a>cudnn deterministic</h2><p>设置为true，cudnn使用非确定性算法，能够自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</p>\n","excerpt":"","more":"<h1 id=\"cudnn-优化设置\"><a href=\"#cudnn-优化设置\" class=\"headerlink\" title=\"cudnn 优化设置\"></a>cudnn 优化设置</h1><h2 id=\"cudnn-deterministic\"><a href=\"#cudnn-deterministic\" class=\"headerlink\" title=\"cudnn deterministic\"></a>cudnn deterministic</h2><p>设置为true，cudnn使用非确定性算法，能够自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。</p>\n"},{"title":"docker","date":"2021-12-08T22:33:23.000Z","_content":"# docker build\nhttps://docs.docker.com/engine/reference/builder/\n\n## install docker\n\n```\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n# ubuntu docker\n\n https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04\n```\nFROM ubuntu:16.04\n\nLABEL com.zhangjun.image.authors=\"ewalker.zj@gmail.com\"\n\nENV TZ \"Asia/Shanghai\"\n\nRUN apt update && \\\n    apt -qqy install software-properties-common && \\\n    add-apt-repository -y  ppa:ubuntu-toolchain-r/test && \\\n    add-apt-repository -y ppa:deadsnakes/ppa && \\\n    apt update && \\\n    apt -qqy install gcc-9 g++-9 && \\\n    apt -qqy install python3.7 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python3.7 10 && \\\n    update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 40 --slave /usr/bin/g++ g++ /usr/bin/g++-9 && \\\n    wget https://bootstrap.pypa.io/get-pip.py && \\\n    python3.7 get-pip.py && \\\n    python3.7 -m pip install pre-commit && \\\n    apt-get -qqy clean && \\\n    rm -rf get-pip.py && rm -rf /var/lib/apt/lists/*\n\n#    update-alternatives --config gcc\n#    update-alternatives --config python\n```","source":"_posts/article/docker.md","raw":"---\ntitle: docker\ndate: 2021-12-09 06:33:23\ntags:\n---\n# docker build\nhttps://docs.docker.com/engine/reference/builder/\n\n## install docker\n\n```\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n# ubuntu docker\n\n https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04\n```\nFROM ubuntu:16.04\n\nLABEL com.zhangjun.image.authors=\"ewalker.zj@gmail.com\"\n\nENV TZ \"Asia/Shanghai\"\n\nRUN apt update && \\\n    apt -qqy install software-properties-common && \\\n    add-apt-repository -y  ppa:ubuntu-toolchain-r/test && \\\n    add-apt-repository -y ppa:deadsnakes/ppa && \\\n    apt update && \\\n    apt -qqy install gcc-9 g++-9 && \\\n    apt -qqy install python3.7 && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python3.7 10 && \\\n    update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 40 --slave /usr/bin/g++ g++ /usr/bin/g++-9 && \\\n    wget https://bootstrap.pypa.io/get-pip.py && \\\n    python3.7 get-pip.py && \\\n    python3.7 -m pip install pre-commit && \\\n    apt-get -qqy clean && \\\n    rm -rf get-pip.py && rm -rf /var/lib/apt/lists/*\n\n#    update-alternatives --config gcc\n#    update-alternatives --config python\n```","slug":"article/docker","published":1,"updated":"2025-09-29T14:59:49.938Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0004otzk81jjce2z","content":"<h1 id=\"docker-build\"><a href=\"#docker-build\" class=\"headerlink\" title=\"docker build\"></a>docker build</h1><p><a href=\"https://docs.docker.com/engine/reference/builder/\">https://docs.docker.com/engine/reference/builder/</a></p>\n<h2 id=\"install-docker\"><a href=\"#install-docker\" class=\"headerlink\" title=\"install docker\"></a>install docker</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y yum-utils</span><br><span class=\"line\">sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>\n<h1 id=\"ubuntu-docker\"><a href=\"#ubuntu-docker\" class=\"headerlink\" title=\"ubuntu docker\"></a>ubuntu docker</h1><p> <a href=\"https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04\">https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM ubuntu:16.04</span><br><span class=\"line\"></span><br><span class=\"line\">LABEL com.zhangjun.image.authors=&quot;ewalker.zj@gmail.com&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV TZ &quot;Asia/Shanghai&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt update &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install software-properties-common &amp;&amp; \\</span><br><span class=\"line\">    add-apt-repository -y  ppa:ubuntu-toolchain-r/test &amp;&amp; \\</span><br><span class=\"line\">    add-apt-repository -y ppa:deadsnakes/ppa &amp;&amp; \\</span><br><span class=\"line\">    apt update &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install gcc-9 g++-9 &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install python3.7 &amp;&amp; \\</span><br><span class=\"line\">    update-alternatives --install /usr/bin/python python /usr/bin/python3.7 10 &amp;&amp; \\</span><br><span class=\"line\">    update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 40 --slave /usr/bin/g++ g++ /usr/bin/g++-9 &amp;&amp; \\</span><br><span class=\"line\">    wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; \\</span><br><span class=\"line\">    python3.7 get-pip.py &amp;&amp; \\</span><br><span class=\"line\">    python3.7 -m pip install pre-commit &amp;&amp; \\</span><br><span class=\"line\">    apt-get -qqy clean &amp;&amp; \\</span><br><span class=\"line\">    rm -rf get-pip.py &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">#    update-alternatives --config gcc</span><br><span class=\"line\">#    update-alternatives --config python</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"docker-build\"><a href=\"#docker-build\" class=\"headerlink\" title=\"docker build\"></a>docker build</h1><p><a href=\"https://docs.docker.com/engine/reference/builder/\">https://docs.docker.com/engine/reference/builder/</a></p>\n<h2 id=\"install-docker\"><a href=\"#install-docker\" class=\"headerlink\" title=\"install docker\"></a>install docker</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y yum-utils</span><br><span class=\"line\">sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>\n<h1 id=\"ubuntu-docker\"><a href=\"#ubuntu-docker\" class=\"headerlink\" title=\"ubuntu docker\"></a>ubuntu docker</h1><p> <a href=\"https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04\">https://askubuntu.com/questions/1140183/install-gcc-9-on-ubuntu-18-04</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM ubuntu:16.04</span><br><span class=\"line\"></span><br><span class=\"line\">LABEL com.zhangjun.image.authors=&quot;ewalker.zj@gmail.com&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ENV TZ &quot;Asia/Shanghai&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">RUN apt update &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install software-properties-common &amp;&amp; \\</span><br><span class=\"line\">    add-apt-repository -y  ppa:ubuntu-toolchain-r/test &amp;&amp; \\</span><br><span class=\"line\">    add-apt-repository -y ppa:deadsnakes/ppa &amp;&amp; \\</span><br><span class=\"line\">    apt update &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install gcc-9 g++-9 &amp;&amp; \\</span><br><span class=\"line\">    apt -qqy install python3.7 &amp;&amp; \\</span><br><span class=\"line\">    update-alternatives --install /usr/bin/python python /usr/bin/python3.7 10 &amp;&amp; \\</span><br><span class=\"line\">    update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 40 --slave /usr/bin/g++ g++ /usr/bin/g++-9 &amp;&amp; \\</span><br><span class=\"line\">    wget https://bootstrap.pypa.io/get-pip.py &amp;&amp; \\</span><br><span class=\"line\">    python3.7 get-pip.py &amp;&amp; \\</span><br><span class=\"line\">    python3.7 -m pip install pre-commit &amp;&amp; \\</span><br><span class=\"line\">    apt-get -qqy clean &amp;&amp; \\</span><br><span class=\"line\">    rm -rf get-pip.py &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class=\"line\"></span><br><span class=\"line\">#    update-alternatives --config gcc</span><br><span class=\"line\">#    update-alternatives --config python</span><br></pre></td></tr></table></figure>"},{"title":"daily note","date":"2021-03-15T21:24:11.000Z","_content":"\n## gpu code\n\nhttps://github.com/Oramy/m2-cgpu\n\n\n## dl deploy\nhttps://github.com/uber/neuropod\n\n## github\nhttps://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\n\n## gpu resource\n\n* Arm Mali GPU Best Practices Developer Guide\n  https://developer.arm.com/documentation/101897/latest\n* Arm Mali Bifrost and Valhall OpenCL Developer Guide\n  https://developer.arm.com/documentation/101574/latest/\n\nhttps://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/\n### mail\n\nMidgard\n\nwrite one 32-bit pixel per core per clock, 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle\n\n![gpu arch](https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/8473.mali_2D00_top_2D00_level.png)\n\n![shader core](https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/1440.mali_2D00_top_2D00_core.png)\n\n#### Tripipe design: \n* arithmetic pipeline\n  \n  simd向量处理引擎，作用于128 bit 4字的寄存器，可以有多个，一般是每个shader core两个。能够弹性访问的数据类型包括2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8。\n\n  OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle。\n\n  For Mali T604 and T628, peak performance is 17 FP32 FLOPS per ALU per cycle.\n\n    |flops|instruction|\n    |-|-|\n    |7|dot product (4 Muls, 3 adds)|\n    |1|scalar add|\n    |4|vec4 add|\n    |4|vec4 multiply|\n    |1|scalar multiply|\n  \n  Mali-T760：600MHz，16 cores， 浮点计算性能为326 FP32 GFLOPS， 16 * 600M * 2 * 17 FP32 FLOPS。包含两个arithmetic pipeline，17 FP32 FLOPS per pipeline per clock cycle。\n\n* load/store pipeline\n* texture pipeline.\n  \n  texuture访存，bilinear filtering 一个时钟周期，trilinear filtering从两个不同mipmaps memory加载，需要两个时钟周期\n\n#### memory system\n每个shader core包含两个16KB L1 数据cache，分别用于texture和常规数据访问。\n一个逻辑L2 cache，所有的shader core共享，由厂商来配置，通常每个实例shader core 32KB或者64KB。\n\nhttps://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals","source":"_posts/article/daily_note.md","raw":"---\ntitle: daily note\ndate: 2021-03-16 05:24:11\ntags:\n---\n\n## gpu code\n\nhttps://github.com/Oramy/m2-cgpu\n\n\n## dl deploy\nhttps://github.com/uber/neuropod\n\n## github\nhttps://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\n\n## gpu resource\n\n* Arm Mali GPU Best Practices Developer Guide\n  https://developer.arm.com/documentation/101897/latest\n* Arm Mali Bifrost and Valhall OpenCL Developer Guide\n  https://developer.arm.com/documentation/101574/latest/\n\nhttps://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/\n### mail\n\nMidgard\n\nwrite one 32-bit pixel per core per clock, 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle\n\n![gpu arch](https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/8473.mali_2D00_top_2D00_level.png)\n\n![shader core](https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/1440.mali_2D00_top_2D00_core.png)\n\n#### Tripipe design: \n* arithmetic pipeline\n  \n  simd向量处理引擎，作用于128 bit 4字的寄存器，可以有多个，一般是每个shader core两个。能够弹性访问的数据类型包括2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8。\n\n  OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle。\n\n  For Mali T604 and T628, peak performance is 17 FP32 FLOPS per ALU per cycle.\n\n    |flops|instruction|\n    |-|-|\n    |7|dot product (4 Muls, 3 adds)|\n    |1|scalar add|\n    |4|vec4 add|\n    |4|vec4 multiply|\n    |1|scalar multiply|\n  \n  Mali-T760：600MHz，16 cores， 浮点计算性能为326 FP32 GFLOPS， 16 * 600M * 2 * 17 FP32 FLOPS。包含两个arithmetic pipeline，17 FP32 FLOPS per pipeline per clock cycle。\n\n* load/store pipeline\n* texture pipeline.\n  \n  texuture访存，bilinear filtering 一个时钟周期，trilinear filtering从两个不同mipmaps memory加载，需要两个时钟周期\n\n#### memory system\n每个shader core包含两个16KB L1 数据cache，分别用于texture和常规数据访问。\n一个逻辑L2 cache，所有的shader core共享，由厂商来配置，通常每个实例shader core 32KB或者64KB。\n\nhttps://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals","slug":"article/daily_note","published":1,"updated":"2025-09-29T14:59:49.935Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0005otzkhx42e888","content":"<h2 id=\"gpu-code\"><a href=\"#gpu-code\" class=\"headerlink\" title=\"gpu code\"></a>gpu code</h2><p><a href=\"https://github.com/Oramy/m2-cgpu\">https://github.com/Oramy/m2-cgpu</a></p>\n<h2 id=\"dl-deploy\"><a href=\"#dl-deploy\" class=\"headerlink\" title=\"dl deploy\"></a>dl deploy</h2><p><a href=\"https://github.com/uber/neuropod\">https://github.com/uber/neuropod</a></p>\n<h2 id=\"github\"><a href=\"#github\" class=\"headerlink\" title=\"github\"></a>github</h2><p><a href=\"https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\">https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html</a></p>\n<h2 id=\"gpu-resource\"><a href=\"#gpu-resource\" class=\"headerlink\" title=\"gpu resource\"></a>gpu resource</h2><ul>\n<li>Arm Mali GPU Best Practices Developer Guide<br><a href=\"https://developer.arm.com/documentation/101897/latest\">https://developer.arm.com/documentation/101897/latest</a></li>\n<li>Arm Mali Bifrost and Valhall OpenCL Developer Guide<br><a href=\"https://developer.arm.com/documentation/101574/latest/\">https://developer.arm.com/documentation/101574/latest/</a></li>\n</ul>\n<p><a href=\"https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/\">https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/</a></p>\n<h3 id=\"mail\"><a href=\"#mail\" class=\"headerlink\" title=\"mail\"></a>mail</h3><p>Midgard</p>\n<p>write one 32-bit pixel per core per clock, 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle</p>\n<p><img src=\"https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/8473.mali_2D00_top_2D00_level.png\" alt=\"gpu arch\"></p>\n<p><img src=\"https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/1440.mali_2D00_top_2D00_core.png\" alt=\"shader core\"></p>\n<h4 id=\"Tripipe-design\"><a href=\"#Tripipe-design\" class=\"headerlink\" title=\"Tripipe design:\"></a>Tripipe design:</h4><ul>\n<li><p>arithmetic pipeline</p>\n<p>simd向量处理引擎，作用于128 bit 4字的寄存器，可以有多个，一般是每个shader core两个。能够弹性访问的数据类型包括2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8。</p>\n<p>OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle。</p>\n<p>For Mali T604 and T628, peak performance is 17 FP32 FLOPS per ALU per cycle.</p>\n<table>\n<thead>\n<tr>\n<th>flops</th>\n<th>instruction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>7</td>\n<td>dot product (4 Muls, 3 adds)</td>\n</tr>\n<tr>\n<td>1</td>\n<td>scalar add</td>\n</tr>\n<tr>\n<td>4</td>\n<td>vec4 add</td>\n</tr>\n<tr>\n<td>4</td>\n<td>vec4 multiply</td>\n</tr>\n<tr>\n<td>1</td>\n<td>scalar multiply</td>\n</tr>\n</tbody></table>\n<p>Mali-T760：600MHz，16 cores， 浮点计算性能为326 FP32 GFLOPS， 16 * 600M * 2 * 17 FP32 FLOPS。包含两个arithmetic pipeline，17 FP32 FLOPS per pipeline per clock cycle。</p>\n</li>\n<li><p>load&#x2F;store pipeline</p>\n</li>\n<li><p>texture pipeline.</p>\n<p>texuture访存，bilinear filtering 一个时钟周期，trilinear filtering从两个不同mipmaps memory加载，需要两个时钟周期</p>\n</li>\n</ul>\n<h4 id=\"memory-system\"><a href=\"#memory-system\" class=\"headerlink\" title=\"memory system\"></a>memory system</h4><p>每个shader core包含两个16KB L1 数据cache，分别用于texture和常规数据访问。<br>一个逻辑L2 cache，所有的shader core共享，由厂商来配置，通常每个实例shader core 32KB或者64KB。</p>\n<p><a href=\"https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals\">https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals</a></p>\n","excerpt":"","more":"<h2 id=\"gpu-code\"><a href=\"#gpu-code\" class=\"headerlink\" title=\"gpu code\"></a>gpu code</h2><p><a href=\"https://github.com/Oramy/m2-cgpu\">https://github.com/Oramy/m2-cgpu</a></p>\n<h2 id=\"dl-deploy\"><a href=\"#dl-deploy\" class=\"headerlink\" title=\"dl deploy\"></a>dl deploy</h2><p><a href=\"https://github.com/uber/neuropod\">https://github.com/uber/neuropod</a></p>\n<h2 id=\"github\"><a href=\"#github\" class=\"headerlink\" title=\"github\"></a>github</h2><p><a href=\"https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\">https://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html</a></p>\n<h2 id=\"gpu-resource\"><a href=\"#gpu-resource\" class=\"headerlink\" title=\"gpu resource\"></a>gpu resource</h2><ul>\n<li>Arm Mali GPU Best Practices Developer Guide<br><a href=\"https://developer.arm.com/documentation/101897/latest\">https://developer.arm.com/documentation/101897/latest</a></li>\n<li>Arm Mali Bifrost and Valhall OpenCL Developer Guide<br><a href=\"https://developer.arm.com/documentation/101574/latest/\">https://developer.arm.com/documentation/101574/latest/</a></li>\n</ul>\n<p><a href=\"https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/\">https://www.edge-ai-vision.com/2015/10/a-quick-guide-to-writing-opencl-kernels-for-powervr-rogue-gpus/</a></p>\n<h3 id=\"mail\"><a href=\"#mail\" class=\"headerlink\" title=\"mail\"></a>mail</h3><p>Midgard</p>\n<p>write one 32-bit pixel per core per clock, 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle</p>\n<p><img src=\"https://community.arm.com/resized-image/__size/1040x0/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/8473.mali_2D00_top_2D00_level.png\" alt=\"gpu arch\"></p>\n<p><img src=\"https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/1440.mali_2D00_top_2D00_core.png\" alt=\"shader core\"></p>\n<h4 id=\"Tripipe-design\"><a href=\"#Tripipe-design\" class=\"headerlink\" title=\"Tripipe design:\"></a>Tripipe design:</h4><ul>\n<li><p>arithmetic pipeline</p>\n<p>simd向量处理引擎，作用于128 bit 4字的寄存器，可以有多个，一般是每个shader core两个。能够弹性访问的数据类型包括2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8。</p>\n<p>OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle。</p>\n<p>For Mali T604 and T628, peak performance is 17 FP32 FLOPS per ALU per cycle.</p>\n<table>\n<thead>\n<tr>\n<th>flops</th>\n<th>instruction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>7</td>\n<td>dot product (4 Muls, 3 adds)</td>\n</tr>\n<tr>\n<td>1</td>\n<td>scalar add</td>\n</tr>\n<tr>\n<td>4</td>\n<td>vec4 add</td>\n</tr>\n<tr>\n<td>4</td>\n<td>vec4 multiply</td>\n</tr>\n<tr>\n<td>1</td>\n<td>scalar multiply</td>\n</tr>\n</tbody></table>\n<p>Mali-T760：600MHz，16 cores， 浮点计算性能为326 FP32 GFLOPS， 16 * 600M * 2 * 17 FP32 FLOPS。包含两个arithmetic pipeline，17 FP32 FLOPS per pipeline per clock cycle。</p>\n</li>\n<li><p>load&#x2F;store pipeline</p>\n</li>\n<li><p>texture pipeline.</p>\n<p>texuture访存，bilinear filtering 一个时钟周期，trilinear filtering从两个不同mipmaps memory加载，需要两个时钟周期</p>\n</li>\n</ul>\n<h4 id=\"memory-system\"><a href=\"#memory-system\" class=\"headerlink\" title=\"memory system\"></a>memory system</h4><p>每个shader core包含两个16KB L1 数据cache，分别用于texture和常规数据访问。<br>一个逻辑L2 cache，所有的shader core共享，由厂商来配置，通常每个实例shader core 32KB或者64KB。</p>\n<p><a href=\"https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals\">https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/arm-mali-compute-architecture-fundamentals</a></p>\n"},{"title":"conv","date":"2021-04-10T03:04:13.000Z","_content":"\nconv详细介绍。\n\n# conv2d\n```\ninline bool IsExpand(const std::vector<int64_t>& filter_dim,\n                     const std::vector<int>& strides,\n                     const std::vector<int>& paddings,\n                     const std::vector<int>& dilations) {\n  bool filter_1 = true, strides_1 = true, padding_0 = true, dilation_1 = true;\n  for (size_t j = 0; j < strides.size(); ++j) {\n    filter_1 = filter_1 && (static_cast<int>(filter_dim[j + 2]) == 1);\n    strides_1 = strides_1 && (strides[j] == 1);\n    padding_0 = padding_0 && (paddings[j] == 0);\n    dilation_1 = dilation_1 && (dilations[j] == 1);\n  }\n  return !(filter_1 && strides_1 && padding_0 && dilation_1);\n}\n```\n```\n// use col_shape in the im2col calculation\n// col_shape_vec:\n// {i_c/g, k_h, k_w, o_h, o_w} or {i_c/g, k_d, k_h, k_w, o_d,o_h, o_w}\n```\ngemm calc\n```\n// use col_matrix_shape in the gemm calculation size:\n// (i_c/g * k_h * k_w, o_h * o_w) or (i_c/g * k_d * k_h * k_w, o_d * o_h * o_w)\n```\n\n```\nstatic inline size_t naive_conv_out_size(size_t in_size, size_t pad,\n                                         size_t dilation, size_t ksize,\n                                         size_t stride) {\n    return (in_size + 2 * pad - dilation * (ksize - 1) - 1) / stride + 1;\n}\n\nstatic inline void naive_conv_fwd_nchw(const float *src, const float *filter,\n                                       float *dst, size_t n, size_t w, size_t h,\n                                       size_t c, size_t k, size_t fx, size_t fy,\n                                       size_t px, size_t py, size_t sx,\n                                       size_t sy, size_t dx, size_t dy, size_t group) {\n    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);\n    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);\n    assert((group >= 1) && (c % group == 0) && (k % group == 0));\n    size_t k_per_group = k / group;\n    size_t c_per_group = c / group;\n        size_t ig, in, ik, ioh, iow, ic, is, ir;\n    size_t cur_h, cur_w, o_idx, i_idx, f_idx;\n    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]\n    for (ig = 0; ig < group; ig++) {\n        for (in = 0; in < n; in++) {\n            for (ik = 0; ik < k_per_group; ik++) {\n                for (ioh = 0; ioh < oh; ioh++) {\n                    for (iow = 0; iow < ow; iow++) {\n                        // sliding window for this filter\n                        float value = .0f;\n                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;\n                        for (ic = 0; ic < c_per_group; ic++) {\n                            for (ir = 0; ir < fy; ir++) {\n                                cur_h = sy * ioh - py + dy * ir;\n                                if (cur_h < 0 || cur_h >= h)\n                                    continue;\n                                for (is = 0; is < fx; is++) {\n                                    cur_w = sx * iow - px + dx * is;\n                                    if (cur_w < 0 || cur_w >= w)\n                                        continue;\n                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +\n                                            cur_h * w + cur_w;\n                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +\n                                            ir * fx + is;\n                                    value += src[i_idx] * filter[f_idx];\n                                }\n                            }\n                        }\n                        dst[o_idx] = value;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// group = 1\nstatic inline void naive_conv_fwd_nchw(const float *src, const float *filter,\n                                       float *dst, size_t n, size_t w, size_t h,\n                                       size_t c, size_t k, size_t fx, size_t fy,\n                                       size_t px, size_t py, size_t sx,\n                                       size_t sy, size_t dx, size_t dy, size_t group) {\n    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);\n    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);\n    assert((group >= 1) && (c % group == 0) && (k % group == 0));\n    size_t k_per_group = k / group;\n    size_t c_per_group = c / group;\n        size_t ig, in, ik, ioh, iow, ic, is, ir;\n    size_t cur_h, cur_w, o_idx, i_idx, f_idx;\n    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]\n    for (ig = 0; ig < group; ig++) {\n        for (in = 0; in < n; in++) {\n            for (ik = 0; ik < k_per_group; ik++) {\n                for (ioh = 0; ioh < oh; ioh++) {\n                    for (iow = 0; iow < ow; iow++) {\n                        // sliding window for this filter\n                        float value = .0f;\n                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;\n                        for (ic = 0; ic < c_per_group; ic++) {\n                            for (ir = 0; ir < fy; ir++) {\n                                cur_h = sy * ioh - py + dy * ir;\n                                if (cur_h < 0 || cur_h >= h)\n                                    continue;\n                                for (is = 0; is < fx; is++) {\n                                    cur_w = sx * iow - px + dx * is;\n                                    if (cur_w < 0 || cur_w >= w)\n                                        continue;\n                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +\n                                            cur_h * w + cur_w;\n                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +\n                                            ir * fx + is;\n                                    value += src[i_idx] * filter[f_idx];\n                                }\n                            }\n                        }\n                        dst[o_idx] = value;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n```\n// [bs, ic, ih, iw] & pack_size=8 => [bs, ic/8, ih, iw, 8]\n// [bs, ic, ih, iw] & pack_size=4 => [bs, ic/4, ih, iw, 4]\n\n// filter [oc, ic, kh, kw] & pack_in=8, pack_out=8 => [oc/8, ic/8, kh, kw, 8, 8]\n// filter [oc, ic, kh, kw] & pack_in=4, pack_out=4 => [ic/4, ic/4, kh, kw, 4, 4]\n\n// [bs, ]\n```\n# conv3d\n\n# conv_depthwise\n```\n// [bs, ic, ih, iw] & pack_size=8 => [bs, ic/8, ih, iw, 8]\n// [bs, ic, ih, iw] & pack_size=4 => [bs, ic/4, ih, iw, 4]\n\n// filter [oc, ic/groups=1, kh, kw]\n// filter [oc, 1, ih, iw] & pack_size=8 => [oc/8, ih, iw, 8]\n// filter [oc, 1, ih, iw] & pack_size=4 => [ic/4, ih, iw, 4]\n\n// output [bs, oc, oh, ow]\n// output_trans [bs, oc/8, oh, ow, 8]\n// output_trans [bs, oc/4, oh, ow, 4]\n// [bs, oc/8, oh, ow, 8] => [bs, oc, oh, ow]\n```\n","source":"_posts/article/conv.md","raw":"---\ntitle: conv\ndate: 2021-04-10 11:04:13\ntags:\n---\n\nconv详细介绍。\n\n# conv2d\n```\ninline bool IsExpand(const std::vector<int64_t>& filter_dim,\n                     const std::vector<int>& strides,\n                     const std::vector<int>& paddings,\n                     const std::vector<int>& dilations) {\n  bool filter_1 = true, strides_1 = true, padding_0 = true, dilation_1 = true;\n  for (size_t j = 0; j < strides.size(); ++j) {\n    filter_1 = filter_1 && (static_cast<int>(filter_dim[j + 2]) == 1);\n    strides_1 = strides_1 && (strides[j] == 1);\n    padding_0 = padding_0 && (paddings[j] == 0);\n    dilation_1 = dilation_1 && (dilations[j] == 1);\n  }\n  return !(filter_1 && strides_1 && padding_0 && dilation_1);\n}\n```\n```\n// use col_shape in the im2col calculation\n// col_shape_vec:\n// {i_c/g, k_h, k_w, o_h, o_w} or {i_c/g, k_d, k_h, k_w, o_d,o_h, o_w}\n```\ngemm calc\n```\n// use col_matrix_shape in the gemm calculation size:\n// (i_c/g * k_h * k_w, o_h * o_w) or (i_c/g * k_d * k_h * k_w, o_d * o_h * o_w)\n```\n\n```\nstatic inline size_t naive_conv_out_size(size_t in_size, size_t pad,\n                                         size_t dilation, size_t ksize,\n                                         size_t stride) {\n    return (in_size + 2 * pad - dilation * (ksize - 1) - 1) / stride + 1;\n}\n\nstatic inline void naive_conv_fwd_nchw(const float *src, const float *filter,\n                                       float *dst, size_t n, size_t w, size_t h,\n                                       size_t c, size_t k, size_t fx, size_t fy,\n                                       size_t px, size_t py, size_t sx,\n                                       size_t sy, size_t dx, size_t dy, size_t group) {\n    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);\n    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);\n    assert((group >= 1) && (c % group == 0) && (k % group == 0));\n    size_t k_per_group = k / group;\n    size_t c_per_group = c / group;\n        size_t ig, in, ik, ioh, iow, ic, is, ir;\n    size_t cur_h, cur_w, o_idx, i_idx, f_idx;\n    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]\n    for (ig = 0; ig < group; ig++) {\n        for (in = 0; in < n; in++) {\n            for (ik = 0; ik < k_per_group; ik++) {\n                for (ioh = 0; ioh < oh; ioh++) {\n                    for (iow = 0; iow < ow; iow++) {\n                        // sliding window for this filter\n                        float value = .0f;\n                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;\n                        for (ic = 0; ic < c_per_group; ic++) {\n                            for (ir = 0; ir < fy; ir++) {\n                                cur_h = sy * ioh - py + dy * ir;\n                                if (cur_h < 0 || cur_h >= h)\n                                    continue;\n                                for (is = 0; is < fx; is++) {\n                                    cur_w = sx * iow - px + dx * is;\n                                    if (cur_w < 0 || cur_w >= w)\n                                        continue;\n                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +\n                                            cur_h * w + cur_w;\n                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +\n                                            ir * fx + is;\n                                    value += src[i_idx] * filter[f_idx];\n                                }\n                            }\n                        }\n                        dst[o_idx] = value;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// group = 1\nstatic inline void naive_conv_fwd_nchw(const float *src, const float *filter,\n                                       float *dst, size_t n, size_t w, size_t h,\n                                       size_t c, size_t k, size_t fx, size_t fy,\n                                       size_t px, size_t py, size_t sx,\n                                       size_t sy, size_t dx, size_t dy, size_t group) {\n    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);\n    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);\n    assert((group >= 1) && (c % group == 0) && (k % group == 0));\n    size_t k_per_group = k / group;\n    size_t c_per_group = c / group;\n        size_t ig, in, ik, ioh, iow, ic, is, ir;\n    size_t cur_h, cur_w, o_idx, i_idx, f_idx;\n    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]\n    for (ig = 0; ig < group; ig++) {\n        for (in = 0; in < n; in++) {\n            for (ik = 0; ik < k_per_group; ik++) {\n                for (ioh = 0; ioh < oh; ioh++) {\n                    for (iow = 0; iow < ow; iow++) {\n                        // sliding window for this filter\n                        float value = .0f;\n                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;\n                        for (ic = 0; ic < c_per_group; ic++) {\n                            for (ir = 0; ir < fy; ir++) {\n                                cur_h = sy * ioh - py + dy * ir;\n                                if (cur_h < 0 || cur_h >= h)\n                                    continue;\n                                for (is = 0; is < fx; is++) {\n                                    cur_w = sx * iow - px + dx * is;\n                                    if (cur_w < 0 || cur_w >= w)\n                                        continue;\n                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +\n                                            cur_h * w + cur_w;\n                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +\n                                            ir * fx + is;\n                                    value += src[i_idx] * filter[f_idx];\n                                }\n                            }\n                        }\n                        dst[o_idx] = value;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n```\n// [bs, ic, ih, iw] & pack_size=8 => [bs, ic/8, ih, iw, 8]\n// [bs, ic, ih, iw] & pack_size=4 => [bs, ic/4, ih, iw, 4]\n\n// filter [oc, ic, kh, kw] & pack_in=8, pack_out=8 => [oc/8, ic/8, kh, kw, 8, 8]\n// filter [oc, ic, kh, kw] & pack_in=4, pack_out=4 => [ic/4, ic/4, kh, kw, 4, 4]\n\n// [bs, ]\n```\n# conv3d\n\n# conv_depthwise\n```\n// [bs, ic, ih, iw] & pack_size=8 => [bs, ic/8, ih, iw, 8]\n// [bs, ic, ih, iw] & pack_size=4 => [bs, ic/4, ih, iw, 4]\n\n// filter [oc, ic/groups=1, kh, kw]\n// filter [oc, 1, ih, iw] & pack_size=8 => [oc/8, ih, iw, 8]\n// filter [oc, 1, ih, iw] & pack_size=4 => [ic/4, ih, iw, 4]\n\n// output [bs, oc, oh, ow]\n// output_trans [bs, oc/8, oh, ow, 8]\n// output_trans [bs, oc/4, oh, ow, 4]\n// [bs, oc/8, oh, ow, 8] => [bs, oc, oh, ow]\n```\n","slug":"article/conv","published":1,"updated":"2025-09-29T14:59:49.935Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0006otzkfj3e5exk","content":"<p>conv详细介绍。</p>\n<h1 id=\"conv2d\"><a href=\"#conv2d\" class=\"headerlink\" title=\"conv2d\"></a>conv2d</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline bool IsExpand(const std::vector&lt;int64_t&gt;&amp; filter_dim,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; strides,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; paddings,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; dilations) &#123;</span><br><span class=\"line\">  bool filter_1 = true, strides_1 = true, padding_0 = true, dilation_1 = true;</span><br><span class=\"line\">  for (size_t j = 0; j &lt; strides.size(); ++j) &#123;</span><br><span class=\"line\">    filter_1 = filter_1 &amp;&amp; (static_cast&lt;int&gt;(filter_dim[j + 2]) == 1);</span><br><span class=\"line\">    strides_1 = strides_1 &amp;&amp; (strides[j] == 1);</span><br><span class=\"line\">    padding_0 = padding_0 &amp;&amp; (paddings[j] == 0);</span><br><span class=\"line\">    dilation_1 = dilation_1 &amp;&amp; (dilations[j] == 1);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  return !(filter_1 &amp;&amp; strides_1 &amp;&amp; padding_0 &amp;&amp; dilation_1);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use col_shape in the im2col calculation</span><br><span class=\"line\">// col_shape_vec:</span><br><span class=\"line\">// &#123;i_c/g, k_h, k_w, o_h, o_w&#125; or &#123;i_c/g, k_d, k_h, k_w, o_d,o_h, o_w&#125;</span><br></pre></td></tr></table></figure>\n<p>gemm calc</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use col_matrix_shape in the gemm calculation size:</span><br><span class=\"line\">// (i_c/g * k_h * k_w, o_h * o_w) or (i_c/g * k_d * k_h * k_w, o_d * o_h * o_w)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline size_t naive_conv_out_size(size_t in_size, size_t pad,</span><br><span class=\"line\">                                         size_t dilation, size_t ksize,</span><br><span class=\"line\">                                         size_t stride) &#123;</span><br><span class=\"line\">    return (in_size + 2 * pad - dilation * (ksize - 1) - 1) / stride + 1;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class=\"line\">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class=\"line\">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class=\"line\">                                       size_t px, size_t py, size_t sx,</span><br><span class=\"line\">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class=\"line\">    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class=\"line\">    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class=\"line\">    assert((group &gt;= 1) &amp;&amp; (c % group == 0) &amp;&amp; (k % group == 0));</span><br><span class=\"line\">    size_t k_per_group = k / group;</span><br><span class=\"line\">    size_t c_per_group = c / group;</span><br><span class=\"line\">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class=\"line\">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class=\"line\">    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class=\"line\">    for (ig = 0; ig &lt; group; ig++) &#123;</span><br><span class=\"line\">        for (in = 0; in &lt; n; in++) &#123;</span><br><span class=\"line\">            for (ik = 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class=\"line\">                for (ioh = 0; ioh &lt; oh; ioh++) &#123;</span><br><span class=\"line\">                    for (iow = 0; iow &lt; ow; iow++) &#123;</span><br><span class=\"line\">                        // sliding window for this filter</span><br><span class=\"line\">                        float value = .0f;</span><br><span class=\"line\">                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class=\"line\">                        for (ic = 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class=\"line\">                            for (ir = 0; ir &lt; fy; ir++) &#123;</span><br><span class=\"line\">                                cur_h = sy * ioh - py + dy * ir;</span><br><span class=\"line\">                                if (cur_h &lt; 0 || cur_h &gt;= h)</span><br><span class=\"line\">                                    continue;</span><br><span class=\"line\">                                for (is = 0; is &lt; fx; is++) &#123;</span><br><span class=\"line\">                                    cur_w = sx * iow - px + dx * is;</span><br><span class=\"line\">                                    if (cur_w &lt; 0 || cur_w &gt;= w)</span><br><span class=\"line\">                                        continue;</span><br><span class=\"line\">                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class=\"line\">                                            cur_h * w + cur_w;</span><br><span class=\"line\">                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class=\"line\">                                            ir * fx + is;</span><br><span class=\"line\">                                    value += src[i_idx] * filter[f_idx];</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        dst[o_idx] = value;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// group = 1</span><br><span class=\"line\">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class=\"line\">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class=\"line\">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class=\"line\">                                       size_t px, size_t py, size_t sx,</span><br><span class=\"line\">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class=\"line\">    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class=\"line\">    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class=\"line\">    assert((group &gt;= 1) &amp;&amp; (c % group == 0) &amp;&amp; (k % group == 0));</span><br><span class=\"line\">    size_t k_per_group = k / group;</span><br><span class=\"line\">    size_t c_per_group = c / group;</span><br><span class=\"line\">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class=\"line\">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class=\"line\">    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class=\"line\">    for (ig = 0; ig &lt; group; ig++) &#123;</span><br><span class=\"line\">        for (in = 0; in &lt; n; in++) &#123;</span><br><span class=\"line\">            for (ik = 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class=\"line\">                for (ioh = 0; ioh &lt; oh; ioh++) &#123;</span><br><span class=\"line\">                    for (iow = 0; iow &lt; ow; iow++) &#123;</span><br><span class=\"line\">                        // sliding window for this filter</span><br><span class=\"line\">                        float value = .0f;</span><br><span class=\"line\">                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class=\"line\">                        for (ic = 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class=\"line\">                            for (ir = 0; ir &lt; fy; ir++) &#123;</span><br><span class=\"line\">                                cur_h = sy * ioh - py + dy * ir;</span><br><span class=\"line\">                                if (cur_h &lt; 0 || cur_h &gt;= h)</span><br><span class=\"line\">                                    continue;</span><br><span class=\"line\">                                for (is = 0; is &lt; fx; is++) &#123;</span><br><span class=\"line\">                                    cur_w = sx * iow - px + dx * is;</span><br><span class=\"line\">                                    if (cur_w &lt; 0 || cur_w &gt;= w)</span><br><span class=\"line\">                                        continue;</span><br><span class=\"line\">                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class=\"line\">                                            cur_h * w + cur_w;</span><br><span class=\"line\">                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class=\"line\">                                            ir * fx + is;</span><br><span class=\"line\">                                    value += src[i_idx] * filter[f_idx];</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        dst[o_idx] = value;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=8 =&gt; [bs, ic/8, ih, iw, 8]</span><br><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=4 =&gt; [bs, ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// filter [oc, ic, kh, kw] &amp; pack_in=8, pack_out=8 =&gt; [oc/8, ic/8, kh, kw, 8, 8]</span><br><span class=\"line\">// filter [oc, ic, kh, kw] &amp; pack_in=4, pack_out=4 =&gt; [ic/4, ic/4, kh, kw, 4, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// [bs, ]</span><br></pre></td></tr></table></figure>\n<h1 id=\"conv3d\"><a href=\"#conv3d\" class=\"headerlink\" title=\"conv3d\"></a>conv3d</h1><h1 id=\"conv-depthwise\"><a href=\"#conv-depthwise\" class=\"headerlink\" title=\"conv_depthwise\"></a>conv_depthwise</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=8 =&gt; [bs, ic/8, ih, iw, 8]</span><br><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=4 =&gt; [bs, ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// filter [oc, ic/groups=1, kh, kw]</span><br><span class=\"line\">// filter [oc, 1, ih, iw] &amp; pack_size=8 =&gt; [oc/8, ih, iw, 8]</span><br><span class=\"line\">// filter [oc, 1, ih, iw] &amp; pack_size=4 =&gt; [ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// output [bs, oc, oh, ow]</span><br><span class=\"line\">// output_trans [bs, oc/8, oh, ow, 8]</span><br><span class=\"line\">// output_trans [bs, oc/4, oh, ow, 4]</span><br><span class=\"line\">// [bs, oc/8, oh, ow, 8] =&gt; [bs, oc, oh, ow]</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<p>conv详细介绍。</p>\n<h1 id=\"conv2d\"><a href=\"#conv2d\" class=\"headerlink\" title=\"conv2d\"></a>conv2d</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline bool IsExpand(const std::vector&lt;int64_t&gt;&amp; filter_dim,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; strides,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; paddings,</span><br><span class=\"line\">                     const std::vector&lt;int&gt;&amp; dilations) &#123;</span><br><span class=\"line\">  bool filter_1 = true, strides_1 = true, padding_0 = true, dilation_1 = true;</span><br><span class=\"line\">  for (size_t j = 0; j &lt; strides.size(); ++j) &#123;</span><br><span class=\"line\">    filter_1 = filter_1 &amp;&amp; (static_cast&lt;int&gt;(filter_dim[j + 2]) == 1);</span><br><span class=\"line\">    strides_1 = strides_1 &amp;&amp; (strides[j] == 1);</span><br><span class=\"line\">    padding_0 = padding_0 &amp;&amp; (paddings[j] == 0);</span><br><span class=\"line\">    dilation_1 = dilation_1 &amp;&amp; (dilations[j] == 1);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  return !(filter_1 &amp;&amp; strides_1 &amp;&amp; padding_0 &amp;&amp; dilation_1);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use col_shape in the im2col calculation</span><br><span class=\"line\">// col_shape_vec:</span><br><span class=\"line\">// &#123;i_c/g, k_h, k_w, o_h, o_w&#125; or &#123;i_c/g, k_d, k_h, k_w, o_d,o_h, o_w&#125;</span><br></pre></td></tr></table></figure>\n<p>gemm calc</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use col_matrix_shape in the gemm calculation size:</span><br><span class=\"line\">// (i_c/g * k_h * k_w, o_h * o_w) or (i_c/g * k_d * k_h * k_w, o_d * o_h * o_w)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static inline size_t naive_conv_out_size(size_t in_size, size_t pad,</span><br><span class=\"line\">                                         size_t dilation, size_t ksize,</span><br><span class=\"line\">                                         size_t stride) &#123;</span><br><span class=\"line\">    return (in_size + 2 * pad - dilation * (ksize - 1) - 1) / stride + 1;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class=\"line\">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class=\"line\">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class=\"line\">                                       size_t px, size_t py, size_t sx,</span><br><span class=\"line\">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class=\"line\">    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class=\"line\">    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class=\"line\">    assert((group &gt;= 1) &amp;&amp; (c % group == 0) &amp;&amp; (k % group == 0));</span><br><span class=\"line\">    size_t k_per_group = k / group;</span><br><span class=\"line\">    size_t c_per_group = c / group;</span><br><span class=\"line\">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class=\"line\">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class=\"line\">    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class=\"line\">    for (ig = 0; ig &lt; group; ig++) &#123;</span><br><span class=\"line\">        for (in = 0; in &lt; n; in++) &#123;</span><br><span class=\"line\">            for (ik = 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class=\"line\">                for (ioh = 0; ioh &lt; oh; ioh++) &#123;</span><br><span class=\"line\">                    for (iow = 0; iow &lt; ow; iow++) &#123;</span><br><span class=\"line\">                        // sliding window for this filter</span><br><span class=\"line\">                        float value = .0f;</span><br><span class=\"line\">                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class=\"line\">                        for (ic = 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class=\"line\">                            for (ir = 0; ir &lt; fy; ir++) &#123;</span><br><span class=\"line\">                                cur_h = sy * ioh - py + dy * ir;</span><br><span class=\"line\">                                if (cur_h &lt; 0 || cur_h &gt;= h)</span><br><span class=\"line\">                                    continue;</span><br><span class=\"line\">                                for (is = 0; is &lt; fx; is++) &#123;</span><br><span class=\"line\">                                    cur_w = sx * iow - px + dx * is;</span><br><span class=\"line\">                                    if (cur_w &lt; 0 || cur_w &gt;= w)</span><br><span class=\"line\">                                        continue;</span><br><span class=\"line\">                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class=\"line\">                                            cur_h * w + cur_w;</span><br><span class=\"line\">                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class=\"line\">                                            ir * fx + is;</span><br><span class=\"line\">                                    value += src[i_idx] * filter[f_idx];</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        dst[o_idx] = value;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// group = 1</span><br><span class=\"line\">static inline void naive_conv_fwd_nchw(const float *src, const float *filter,</span><br><span class=\"line\">                                       float *dst, size_t n, size_t w, size_t h,</span><br><span class=\"line\">                                       size_t c, size_t k, size_t fx, size_t fy,</span><br><span class=\"line\">                                       size_t px, size_t py, size_t sx,</span><br><span class=\"line\">                                       size_t sy, size_t dx, size_t dy, size_t group) &#123;</span><br><span class=\"line\">    size_t oh = naive_conv_out_size(h, py, dy, fy, sy);</span><br><span class=\"line\">    size_t ow = naive_conv_out_size(w, px, dx, fx, sx);</span><br><span class=\"line\">    assert((group &gt;= 1) &amp;&amp; (c % group == 0) &amp;&amp; (k % group == 0));</span><br><span class=\"line\">    size_t k_per_group = k / group;</span><br><span class=\"line\">    size_t c_per_group = c / group;</span><br><span class=\"line\">        size_t ig, in, ik, ioh, iow, ic, is, ir;</span><br><span class=\"line\">    size_t cur_h, cur_w, o_idx, i_idx, f_idx;</span><br><span class=\"line\">    // input:[n,c,h,w], filter:[k, c, fx, fy], output: [n, k, out_h, out_w]</span><br><span class=\"line\">    for (ig = 0; ig &lt; group; ig++) &#123;</span><br><span class=\"line\">        for (in = 0; in &lt; n; in++) &#123;</span><br><span class=\"line\">            for (ik = 0; ik &lt; k_per_group; ik++) &#123;</span><br><span class=\"line\">                for (ioh = 0; ioh &lt; oh; ioh++) &#123;</span><br><span class=\"line\">                    for (iow = 0; iow &lt; ow; iow++) &#123;</span><br><span class=\"line\">                        // sliding window for this filter</span><br><span class=\"line\">                        float value = .0f;</span><br><span class=\"line\">                        o_idx = in * k * oh * ow + ig * k_per_group * oh * ow + ik * oh * ow + ioh * ow + iow;</span><br><span class=\"line\">                        for (ic = 0; ic &lt; c_per_group; ic++) &#123;</span><br><span class=\"line\">                            for (ir = 0; ir &lt; fy; ir++) &#123;</span><br><span class=\"line\">                                cur_h = sy * ioh - py + dy * ir;</span><br><span class=\"line\">                                if (cur_h &lt; 0 || cur_h &gt;= h)</span><br><span class=\"line\">                                    continue;</span><br><span class=\"line\">                                for (is = 0; is &lt; fx; is++) &#123;</span><br><span class=\"line\">                                    cur_w = sx * iow - px + dx * is;</span><br><span class=\"line\">                                    if (cur_w &lt; 0 || cur_w &gt;= w)</span><br><span class=\"line\">                                        continue;</span><br><span class=\"line\">                                    i_idx = in * c * h * w + ig * c_per_group * h * w + ic * h * w +</span><br><span class=\"line\">                                            cur_h * w + cur_w;</span><br><span class=\"line\">                                    f_idx = ig * k_per_group * c_per_group * fy * fx + ik * c_per_group * fy * fx + ic * fy * fx +</span><br><span class=\"line\">                                            ir * fx + is;</span><br><span class=\"line\">                                    value += src[i_idx] * filter[f_idx];</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                        dst[o_idx] = value;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=8 =&gt; [bs, ic/8, ih, iw, 8]</span><br><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=4 =&gt; [bs, ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// filter [oc, ic, kh, kw] &amp; pack_in=8, pack_out=8 =&gt; [oc/8, ic/8, kh, kw, 8, 8]</span><br><span class=\"line\">// filter [oc, ic, kh, kw] &amp; pack_in=4, pack_out=4 =&gt; [ic/4, ic/4, kh, kw, 4, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// [bs, ]</span><br></pre></td></tr></table></figure>\n<h1 id=\"conv3d\"><a href=\"#conv3d\" class=\"headerlink\" title=\"conv3d\"></a>conv3d</h1><h1 id=\"conv-depthwise\"><a href=\"#conv-depthwise\" class=\"headerlink\" title=\"conv_depthwise\"></a>conv_depthwise</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=8 =&gt; [bs, ic/8, ih, iw, 8]</span><br><span class=\"line\">// [bs, ic, ih, iw] &amp; pack_size=4 =&gt; [bs, ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// filter [oc, ic/groups=1, kh, kw]</span><br><span class=\"line\">// filter [oc, 1, ih, iw] &amp; pack_size=8 =&gt; [oc/8, ih, iw, 8]</span><br><span class=\"line\">// filter [oc, 1, ih, iw] &amp; pack_size=4 =&gt; [ic/4, ih, iw, 4]</span><br><span class=\"line\"></span><br><span class=\"line\">// output [bs, oc, oh, ow]</span><br><span class=\"line\">// output_trans [bs, oc/8, oh, ow, 8]</span><br><span class=\"line\">// output_trans [bs, oc/4, oh, ow, 4]</span><br><span class=\"line\">// [bs, oc/8, oh, ow, 8] =&gt; [bs, oc, oh, ow]</span><br></pre></td></tr></table></figure>\n"},{"title":"gflags","date":"2021-03-10T23:40:22.000Z","_content":"主要介绍google gflags使用\n\n# 常见用法\n## 定义flag\n一般.cc中定义flag，.h进行声明，其他包含该.h的文件就可以使用.cc定义的flag变量。\n## flag与参数解析\n```\ngflags::ParseCommandLineFlags(&argc, &argv, true); \n```\n告诉程序处理命令行传入参数。最后一个参数为remove_flags，值为true，会移除相应flag和对应值并且修改argc值，argv只保留命令行参数；值为false，会保持argc不变，会调整argv中存储的内容顺序，flag放命令行参数前面。\n## 命令行设置flag\n## 更改flag默认值\n## ","source":"_posts/article/flags.md","raw":"---\ntitle: gflags\ndate: 2021-03-11 07:40:22\ntags:\n---\n主要介绍google gflags使用\n\n# 常见用法\n## 定义flag\n一般.cc中定义flag，.h进行声明，其他包含该.h的文件就可以使用.cc定义的flag变量。\n## flag与参数解析\n```\ngflags::ParseCommandLineFlags(&argc, &argv, true); \n```\n告诉程序处理命令行传入参数。最后一个参数为remove_flags，值为true，会移除相应flag和对应值并且修改argc值，argv只保留命令行参数；值为false，会保持argc不变，会调整argv中存储的内容顺序，flag放命令行参数前面。\n## 命令行设置flag\n## 更改flag默认值\n## ","slug":"article/flags","published":1,"updated":"2025-09-29T14:59:49.938Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0007otzka1vk9ewh","content":"<p>主要介绍google gflags使用</p>\n<h1 id=\"常见用法\"><a href=\"#常见用法\" class=\"headerlink\" title=\"常见用法\"></a>常见用法</h1><h2 id=\"定义flag\"><a href=\"#定义flag\" class=\"headerlink\" title=\"定义flag\"></a>定义flag</h2><p>一般.cc中定义flag，.h进行声明，其他包含该.h的文件就可以使用.cc定义的flag变量。</p>\n<h2 id=\"flag与参数解析\"><a href=\"#flag与参数解析\" class=\"headerlink\" title=\"flag与参数解析\"></a>flag与参数解析</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gflags::ParseCommandLineFlags(&amp;argc, &amp;argv, true); </span><br></pre></td></tr></table></figure>\n<p>告诉程序处理命令行传入参数。最后一个参数为remove_flags，值为true，会移除相应flag和对应值并且修改argc值，argv只保留命令行参数；值为false，会保持argc不变，会调整argv中存储的内容顺序，flag放命令行参数前面。</p>\n<h2 id=\"命令行设置flag\"><a href=\"#命令行设置flag\" class=\"headerlink\" title=\"命令行设置flag\"></a>命令行设置flag</h2><h2 id=\"更改flag默认值\"><a href=\"#更改flag默认值\" class=\"headerlink\" title=\"更改flag默认值\"></a>更改flag默认值</h2><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2>","excerpt":"","more":"<p>主要介绍google gflags使用</p>\n<h1 id=\"常见用法\"><a href=\"#常见用法\" class=\"headerlink\" title=\"常见用法\"></a>常见用法</h1><h2 id=\"定义flag\"><a href=\"#定义flag\" class=\"headerlink\" title=\"定义flag\"></a>定义flag</h2><p>一般.cc中定义flag，.h进行声明，其他包含该.h的文件就可以使用.cc定义的flag变量。</p>\n<h2 id=\"flag与参数解析\"><a href=\"#flag与参数解析\" class=\"headerlink\" title=\"flag与参数解析\"></a>flag与参数解析</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gflags::ParseCommandLineFlags(&amp;argc, &amp;argv, true); </span><br></pre></td></tr></table></figure>\n<p>告诉程序处理命令行传入参数。最后一个参数为remove_flags，值为true，会移除相应flag和对应值并且修改argc值，argv只保留命令行参数；值为false，会保持argc不变，会调整argv中存储的内容顺序，flag放命令行参数前面。</p>\n<h2 id=\"命令行设置flag\"><a href=\"#命令行设置flag\" class=\"headerlink\" title=\"命令行设置flag\"></a>命令行设置flag</h2><h2 id=\"更改flag默认值\"><a href=\"#更改flag默认值\" class=\"headerlink\" title=\"更改flag默认值\"></a>更改flag默认值</h2><h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2>"},{"title":"常用命令","date":"2021-03-11T00:47:15.000Z","_content":"","source":"_posts/article/flags-1.md","raw":"---\ntitle: 常用命令\ndate: 2021-03-11 08:47:15\ntags:\n---\n","slug":"article/flags-1","published":1,"updated":"2025-09-29T14:59:49.935Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905w0008otzk8gckf050","content":"","excerpt":"","more":""},{"title":"hexo","date":"2021-02-28T23:17:58.000Z","_content":"# hexo\n# hexo init\n该命令初始化博客\n## new post\n```\nhexo new [layout] titile\n```\n-p 自定义新文章路径, 如下面会在source/article目录下新建tensorrt.md\n```\nhexo new -p article/tensorrt \"TensorRT\"\n```\n## admin\n\n## ref\n[hexo next后台管理](https://www.cnblogs.com/xingyunblog/p/8681205.html)","source":"_posts/article/hexo.md","raw":"---\ntitle: hexo\ndate: 2021-03-01 07:17:58\ntags:\n---\n# hexo\n# hexo init\n该命令初始化博客\n## new post\n```\nhexo new [layout] titile\n```\n-p 自定义新文章路径, 如下面会在source/article目录下新建tensorrt.md\n```\nhexo new -p article/tensorrt \"TensorRT\"\n```\n## admin\n\n## ref\n[hexo next后台管理](https://www.cnblogs.com/xingyunblog/p/8681205.html)","slug":"article/hexo","published":1,"updated":"2025-09-29T14:59:49.937Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x0009otzkhw9n5wyz","content":"<h1 id=\"hexo\"><a href=\"#hexo\" class=\"headerlink\" title=\"hexo\"></a>hexo</h1><h1 id=\"hexo-init\"><a href=\"#hexo-init\" class=\"headerlink\" title=\"hexo init\"></a>hexo init</h1><p>该命令初始化博客</p>\n<h2 id=\"new-post\"><a href=\"#new-post\" class=\"headerlink\" title=\"new post\"></a>new post</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new [layout] titile</span><br></pre></td></tr></table></figure>\n<p>-p 自定义新文章路径, 如下面会在source&#x2F;article目录下新建tensorrt.md</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new -p article/tensorrt &quot;TensorRT&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"admin\"><a href=\"#admin\" class=\"headerlink\" title=\"admin\"></a>admin</h2><h2 id=\"ref\"><a href=\"#ref\" class=\"headerlink\" title=\"ref\"></a>ref</h2><p><a href=\"https://www.cnblogs.com/xingyunblog/p/8681205.html\">hexo next后台管理</a></p>\n","excerpt":"","more":"<h1 id=\"hexo\"><a href=\"#hexo\" class=\"headerlink\" title=\"hexo\"></a>hexo</h1><h1 id=\"hexo-init\"><a href=\"#hexo-init\" class=\"headerlink\" title=\"hexo init\"></a>hexo init</h1><p>该命令初始化博客</p>\n<h2 id=\"new-post\"><a href=\"#new-post\" class=\"headerlink\" title=\"new post\"></a>new post</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new [layout] titile</span><br></pre></td></tr></table></figure>\n<p>-p 自定义新文章路径, 如下面会在source&#x2F;article目录下新建tensorrt.md</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new -p article/tensorrt &quot;TensorRT&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"admin\"><a href=\"#admin\" class=\"headerlink\" title=\"admin\"></a>admin</h2><h2 id=\"ref\"><a href=\"#ref\" class=\"headerlink\" title=\"ref\"></a>ref</h2><p><a href=\"https://www.cnblogs.com/xingyunblog/p/8681205.html\">hexo next后台管理</a></p>\n"},{"title":"MacTex","date":"2022-03-02T07:21:17.000Z","_content":"\n# install\n\n## macOS\n```\nbrew install mactex \n```\n\n## ubuntu\n```\napt update\napt-get install texlive-xetex latex-cjk-all texmaker\n```\n\n# link\nhttps://github.com/FengMengZhao/LaTeX_generate_Chinese_resume\nhttps://github.com/billryan/resume/tree/zh_CN\nhttps://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91","source":"_posts/article/mactex.md","raw":"---\ntitle: MacTex\ndate: 2022-03-02 15:21:17\ntags:\n---\n\n# install\n\n## macOS\n```\nbrew install mactex \n```\n\n## ubuntu\n```\napt update\napt-get install texlive-xetex latex-cjk-all texmaker\n```\n\n# link\nhttps://github.com/FengMengZhao/LaTeX_generate_Chinese_resume\nhttps://github.com/billryan/resume/tree/zh_CN\nhttps://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91","slug":"article/mactex","published":1,"updated":"2025-09-29T14:59:49.939Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x000aotzkf1qc7as1","content":"<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><h2 id=\"macOS\"><a href=\"#macOS\" class=\"headerlink\" title=\"macOS\"></a>macOS</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">brew install mactex </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ubuntu\"><a href=\"#ubuntu\" class=\"headerlink\" title=\"ubuntu\"></a>ubuntu</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt update</span><br><span class=\"line\">apt-get install texlive-xetex latex-cjk-all texmaker</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume\">https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume</a><br><a href=\"https://github.com/billryan/resume/tree/zh_CN\">https://github.com/billryan/resume/tree/zh_CN</a><br><a href=\"https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91\">https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91</a></p>\n","excerpt":"","more":"<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><h2 id=\"macOS\"><a href=\"#macOS\" class=\"headerlink\" title=\"macOS\"></a>macOS</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">brew install mactex </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ubuntu\"><a href=\"#ubuntu\" class=\"headerlink\" title=\"ubuntu\"></a>ubuntu</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt update</span><br><span class=\"line\">apt-get install texlive-xetex latex-cjk-all texmaker</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume\">https://github.com/FengMengZhao/LaTeX_generate_Chinese_resume</a><br><a href=\"https://github.com/billryan/resume/tree/zh_CN\">https://github.com/billryan/resume/tree/zh_CN</a><br><a href=\"https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91\">https://herechen.github.io/post/latex-skills/#latexmk-%E8%87%AA%E5%8A%A8%E5%8C%96%E7%BC%96%E8%AF%91</a></p>\n"},{"title":"Metal Basic","date":"2021-09-05T02:01:57.000Z","_content":"# metal buffer anc texture\n```\nid<MTLDevice> device = MTLCreateSystemDefaultDevice();\n\nMTLTextureDescriptor* desc = [[MTLTextureDescriptor alloc] init];\n[desc setTextureType:MTLTextureType2DArray];\n[desc setDepth:1];\ndesc.width = static_cast<NSUInteger>(dim[2]);\ndesc.height = static_cast<NSUInteger>(dim[1]);\ndesc.arrayLength = static_cast<NSUInteger>(((dim[0]) * (dim[3]) + 3) / 4);\ndesc.pixelFormat = MTLPixelFormatRGBA16Float;\ndesc.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;\ndesc.storageMode = MTLStorageModeShared;\n\nid<MTLTexture> image_ = [device newTextureWithDescriptor:desc];\n\nint channels_per_pixel_ = 4;\nint array_length_ = desc_.arrayLength;\n\nauto count = image_.width * image_.height * array_length_ * channels_per_pixel_;\nauto buffer = static_cast<uint16_t*>(malloc(sizeof(uint16_t) * count));\n\nauto bytes_per_row = image_.width * image_.depth * channels_per_pixel_ * sizeof(uint16_t);\nauto bytes_per_image = image_.height * bytes_per_row;\nconst MTLRegion region {\n    .origin = {0, 0, 0},\n    .size ={\n        image_.width, image_.height, image_.depth,\n    }\n};\n\n// copy from cpu to gpu\nfor (int i = 0; i < array_length_; ++i) {\n    auto p = buffer + image_.width * image_.height * channels_per_pixel_ * i;\n    [image_ replaceRegion:region\n              mipmapLevel:0\n                    slice:static_cast<NSUInteger>(i)\n                withBytes:p\n              bytesPerRow:bytes_per_row\n            bytesPerImage:bytes_per_image];\n}\n\n// copy from gpu to cpu\nauto* out_buffer = static_cast<uint16_t*>(malloc(sizeof(uint16_t) * count));\nfor (int i = 0; i < array_length_; ++i) {\n    auto p = out_buffer + image_.width * image_.height * channels_per_pixel_ * i;\n\n    [image_ getBytes:p\n         bytesPerRow:bytes_per_row\n       bytesPerImage:bytes_per_image\n          fromRegion:region\n         mipmapLevel:0\n               slice:static_cast<NSUInteger>(i)];\n}\n```\n\nswift \n```\nlet device = MTLCreateSystemDefaultDevice()!\nlet queue = device.makeCommandQueue()!\nlet textureDescriptor = MTLTextureDescriptor()\ntextureDescriptor.textureType = .type2D\ntextureDescriptor.pixelFormat = .r16Uint\ntextureDescriptor.width = bufferWidth\ntextureDescriptor.height = 256\ntextureDescriptor.usage = [.shaderRead, .shaderWrite]\n\n\nlet texture = buffer?.makeTexture(descriptor: textureDescriptor, offset: 0, bytesPerRow: bufferWidth*MemoryLayout<UInt16>.stride)\n\nlet texture = device.makeTexture(descriptor: textureDescriptor)\ntexture?.replace(region: MTLRegionMake2D(0, 0, w, h), mipmapLevel: 0, withBytes: data, bytesPerRow: 4 * w)\n\n# buffer\nlet count = 1500\nvar myVector = [Float](repeating: 0, count: count)\nvar length = count * MemoryLayout< Float >.stride\nvar outBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])\nfor (index, value) in myVector.enumerated() { myVector[index] = Float(index) }\nvar inBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])\n```","source":"_posts/article/metal_basic.md","raw":"---\ntitle: Metal Basic\ndate: 2021-09-05 10:01:57\ntags:\n---\n# metal buffer anc texture\n```\nid<MTLDevice> device = MTLCreateSystemDefaultDevice();\n\nMTLTextureDescriptor* desc = [[MTLTextureDescriptor alloc] init];\n[desc setTextureType:MTLTextureType2DArray];\n[desc setDepth:1];\ndesc.width = static_cast<NSUInteger>(dim[2]);\ndesc.height = static_cast<NSUInteger>(dim[1]);\ndesc.arrayLength = static_cast<NSUInteger>(((dim[0]) * (dim[3]) + 3) / 4);\ndesc.pixelFormat = MTLPixelFormatRGBA16Float;\ndesc.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;\ndesc.storageMode = MTLStorageModeShared;\n\nid<MTLTexture> image_ = [device newTextureWithDescriptor:desc];\n\nint channels_per_pixel_ = 4;\nint array_length_ = desc_.arrayLength;\n\nauto count = image_.width * image_.height * array_length_ * channels_per_pixel_;\nauto buffer = static_cast<uint16_t*>(malloc(sizeof(uint16_t) * count));\n\nauto bytes_per_row = image_.width * image_.depth * channels_per_pixel_ * sizeof(uint16_t);\nauto bytes_per_image = image_.height * bytes_per_row;\nconst MTLRegion region {\n    .origin = {0, 0, 0},\n    .size ={\n        image_.width, image_.height, image_.depth,\n    }\n};\n\n// copy from cpu to gpu\nfor (int i = 0; i < array_length_; ++i) {\n    auto p = buffer + image_.width * image_.height * channels_per_pixel_ * i;\n    [image_ replaceRegion:region\n              mipmapLevel:0\n                    slice:static_cast<NSUInteger>(i)\n                withBytes:p\n              bytesPerRow:bytes_per_row\n            bytesPerImage:bytes_per_image];\n}\n\n// copy from gpu to cpu\nauto* out_buffer = static_cast<uint16_t*>(malloc(sizeof(uint16_t) * count));\nfor (int i = 0; i < array_length_; ++i) {\n    auto p = out_buffer + image_.width * image_.height * channels_per_pixel_ * i;\n\n    [image_ getBytes:p\n         bytesPerRow:bytes_per_row\n       bytesPerImage:bytes_per_image\n          fromRegion:region\n         mipmapLevel:0\n               slice:static_cast<NSUInteger>(i)];\n}\n```\n\nswift \n```\nlet device = MTLCreateSystemDefaultDevice()!\nlet queue = device.makeCommandQueue()!\nlet textureDescriptor = MTLTextureDescriptor()\ntextureDescriptor.textureType = .type2D\ntextureDescriptor.pixelFormat = .r16Uint\ntextureDescriptor.width = bufferWidth\ntextureDescriptor.height = 256\ntextureDescriptor.usage = [.shaderRead, .shaderWrite]\n\n\nlet texture = buffer?.makeTexture(descriptor: textureDescriptor, offset: 0, bytesPerRow: bufferWidth*MemoryLayout<UInt16>.stride)\n\nlet texture = device.makeTexture(descriptor: textureDescriptor)\ntexture?.replace(region: MTLRegionMake2D(0, 0, w, h), mipmapLevel: 0, withBytes: data, bytesPerRow: 4 * w)\n\n# buffer\nlet count = 1500\nvar myVector = [Float](repeating: 0, count: count)\nvar length = count * MemoryLayout< Float >.stride\nvar outBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])\nfor (index, value) in myVector.enumerated() { myVector[index] = Float(index) }\nvar inBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])\n```","slug":"article/metal_basic","published":1,"updated":"2025-09-29T14:59:49.937Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x000botzkbbps2e8l","content":"<h1 id=\"metal-buffer-anc-texture\"><a href=\"#metal-buffer-anc-texture\" class=\"headerlink\" title=\"metal buffer anc texture\"></a>metal buffer anc texture</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id&lt;MTLDevice&gt; device = MTLCreateSystemDefaultDevice();</span><br><span class=\"line\"></span><br><span class=\"line\">MTLTextureDescriptor* desc = [[MTLTextureDescriptor alloc] init];</span><br><span class=\"line\">[desc setTextureType:MTLTextureType2DArray];</span><br><span class=\"line\">[desc setDepth:1];</span><br><span class=\"line\">desc.width = static_cast&lt;NSUInteger&gt;(dim[2]);</span><br><span class=\"line\">desc.height = static_cast&lt;NSUInteger&gt;(dim[1]);</span><br><span class=\"line\">desc.arrayLength = static_cast&lt;NSUInteger&gt;(((dim[0]) * (dim[3]) + 3) / 4);</span><br><span class=\"line\">desc.pixelFormat = MTLPixelFormatRGBA16Float;</span><br><span class=\"line\">desc.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;</span><br><span class=\"line\">desc.storageMode = MTLStorageModeShared;</span><br><span class=\"line\"></span><br><span class=\"line\">id&lt;MTLTexture&gt; image_ = [device newTextureWithDescriptor:desc];</span><br><span class=\"line\"></span><br><span class=\"line\">int channels_per_pixel_ = 4;</span><br><span class=\"line\">int array_length_ = desc_.arrayLength;</span><br><span class=\"line\"></span><br><span class=\"line\">auto count = image_.width * image_.height * array_length_ * channels_per_pixel_;</span><br><span class=\"line\">auto buffer = static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class=\"line\"></span><br><span class=\"line\">auto bytes_per_row = image_.width * image_.depth * channels_per_pixel_ * sizeof(uint16_t);</span><br><span class=\"line\">auto bytes_per_image = image_.height * bytes_per_row;</span><br><span class=\"line\">const MTLRegion region &#123;</span><br><span class=\"line\">    .origin = &#123;0, 0, 0&#125;,</span><br><span class=\"line\">    .size =&#123;</span><br><span class=\"line\">        image_.width, image_.height, image_.depth,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">// copy from cpu to gpu</span><br><span class=\"line\">for (int i = 0; i &lt; array_length_; ++i) &#123;</span><br><span class=\"line\">    auto p = buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class=\"line\">    [image_ replaceRegion:region</span><br><span class=\"line\">              mipmapLevel:0</span><br><span class=\"line\">                    slice:static_cast&lt;NSUInteger&gt;(i)</span><br><span class=\"line\">                withBytes:p</span><br><span class=\"line\">              bytesPerRow:bytes_per_row</span><br><span class=\"line\">            bytesPerImage:bytes_per_image];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// copy from gpu to cpu</span><br><span class=\"line\">auto* out_buffer = static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class=\"line\">for (int i = 0; i &lt; array_length_; ++i) &#123;</span><br><span class=\"line\">    auto p = out_buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class=\"line\"></span><br><span class=\"line\">    [image_ getBytes:p</span><br><span class=\"line\">         bytesPerRow:bytes_per_row</span><br><span class=\"line\">       bytesPerImage:bytes_per_image</span><br><span class=\"line\">          fromRegion:region</span><br><span class=\"line\">         mipmapLevel:0</span><br><span class=\"line\">               slice:static_cast&lt;NSUInteger&gt;(i)];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>swift </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">let device = MTLCreateSystemDefaultDevice()!</span><br><span class=\"line\">let queue = device.makeCommandQueue()!</span><br><span class=\"line\">let textureDescriptor = MTLTextureDescriptor()</span><br><span class=\"line\">textureDescriptor.textureType = .type2D</span><br><span class=\"line\">textureDescriptor.pixelFormat = .r16Uint</span><br><span class=\"line\">textureDescriptor.width = bufferWidth</span><br><span class=\"line\">textureDescriptor.height = 256</span><br><span class=\"line\">textureDescriptor.usage = [.shaderRead, .shaderWrite]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">let texture = buffer?.makeTexture(descriptor: textureDescriptor, offset: 0, bytesPerRow: bufferWidth*MemoryLayout&lt;UInt16&gt;.stride)</span><br><span class=\"line\"></span><br><span class=\"line\">let texture = device.makeTexture(descriptor: textureDescriptor)</span><br><span class=\"line\">texture?.replace(region: MTLRegionMake2D(0, 0, w, h), mipmapLevel: 0, withBytes: data, bytesPerRow: 4 * w)</span><br><span class=\"line\"></span><br><span class=\"line\"># buffer</span><br><span class=\"line\">let count = 1500</span><br><span class=\"line\">var myVector = [Float](repeating: 0, count: count)</span><br><span class=\"line\">var length = count * MemoryLayout&lt; Float &gt;.stride</span><br><span class=\"line\">var outBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])</span><br><span class=\"line\">for (index, value) in myVector.enumerated() &#123; myVector[index] = Float(index) &#125;</span><br><span class=\"line\">var inBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"metal-buffer-anc-texture\"><a href=\"#metal-buffer-anc-texture\" class=\"headerlink\" title=\"metal buffer anc texture\"></a>metal buffer anc texture</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id&lt;MTLDevice&gt; device = MTLCreateSystemDefaultDevice();</span><br><span class=\"line\"></span><br><span class=\"line\">MTLTextureDescriptor* desc = [[MTLTextureDescriptor alloc] init];</span><br><span class=\"line\">[desc setTextureType:MTLTextureType2DArray];</span><br><span class=\"line\">[desc setDepth:1];</span><br><span class=\"line\">desc.width = static_cast&lt;NSUInteger&gt;(dim[2]);</span><br><span class=\"line\">desc.height = static_cast&lt;NSUInteger&gt;(dim[1]);</span><br><span class=\"line\">desc.arrayLength = static_cast&lt;NSUInteger&gt;(((dim[0]) * (dim[3]) + 3) / 4);</span><br><span class=\"line\">desc.pixelFormat = MTLPixelFormatRGBA16Float;</span><br><span class=\"line\">desc.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;</span><br><span class=\"line\">desc.storageMode = MTLStorageModeShared;</span><br><span class=\"line\"></span><br><span class=\"line\">id&lt;MTLTexture&gt; image_ = [device newTextureWithDescriptor:desc];</span><br><span class=\"line\"></span><br><span class=\"line\">int channels_per_pixel_ = 4;</span><br><span class=\"line\">int array_length_ = desc_.arrayLength;</span><br><span class=\"line\"></span><br><span class=\"line\">auto count = image_.width * image_.height * array_length_ * channels_per_pixel_;</span><br><span class=\"line\">auto buffer = static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class=\"line\"></span><br><span class=\"line\">auto bytes_per_row = image_.width * image_.depth * channels_per_pixel_ * sizeof(uint16_t);</span><br><span class=\"line\">auto bytes_per_image = image_.height * bytes_per_row;</span><br><span class=\"line\">const MTLRegion region &#123;</span><br><span class=\"line\">    .origin = &#123;0, 0, 0&#125;,</span><br><span class=\"line\">    .size =&#123;</span><br><span class=\"line\">        image_.width, image_.height, image_.depth,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">// copy from cpu to gpu</span><br><span class=\"line\">for (int i = 0; i &lt; array_length_; ++i) &#123;</span><br><span class=\"line\">    auto p = buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class=\"line\">    [image_ replaceRegion:region</span><br><span class=\"line\">              mipmapLevel:0</span><br><span class=\"line\">                    slice:static_cast&lt;NSUInteger&gt;(i)</span><br><span class=\"line\">                withBytes:p</span><br><span class=\"line\">              bytesPerRow:bytes_per_row</span><br><span class=\"line\">            bytesPerImage:bytes_per_image];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// copy from gpu to cpu</span><br><span class=\"line\">auto* out_buffer = static_cast&lt;uint16_t*&gt;(malloc(sizeof(uint16_t) * count));</span><br><span class=\"line\">for (int i = 0; i &lt; array_length_; ++i) &#123;</span><br><span class=\"line\">    auto p = out_buffer + image_.width * image_.height * channels_per_pixel_ * i;</span><br><span class=\"line\"></span><br><span class=\"line\">    [image_ getBytes:p</span><br><span class=\"line\">         bytesPerRow:bytes_per_row</span><br><span class=\"line\">       bytesPerImage:bytes_per_image</span><br><span class=\"line\">          fromRegion:region</span><br><span class=\"line\">         mipmapLevel:0</span><br><span class=\"line\">               slice:static_cast&lt;NSUInteger&gt;(i)];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>swift </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">let device = MTLCreateSystemDefaultDevice()!</span><br><span class=\"line\">let queue = device.makeCommandQueue()!</span><br><span class=\"line\">let textureDescriptor = MTLTextureDescriptor()</span><br><span class=\"line\">textureDescriptor.textureType = .type2D</span><br><span class=\"line\">textureDescriptor.pixelFormat = .r16Uint</span><br><span class=\"line\">textureDescriptor.width = bufferWidth</span><br><span class=\"line\">textureDescriptor.height = 256</span><br><span class=\"line\">textureDescriptor.usage = [.shaderRead, .shaderWrite]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">let texture = buffer?.makeTexture(descriptor: textureDescriptor, offset: 0, bytesPerRow: bufferWidth*MemoryLayout&lt;UInt16&gt;.stride)</span><br><span class=\"line\"></span><br><span class=\"line\">let texture = device.makeTexture(descriptor: textureDescriptor)</span><br><span class=\"line\">texture?.replace(region: MTLRegionMake2D(0, 0, w, h), mipmapLevel: 0, withBytes: data, bytesPerRow: 4 * w)</span><br><span class=\"line\"></span><br><span class=\"line\"># buffer</span><br><span class=\"line\">let count = 1500</span><br><span class=\"line\">var myVector = [Float](repeating: 0, count: count)</span><br><span class=\"line\">var length = count * MemoryLayout&lt; Float &gt;.stride</span><br><span class=\"line\">var outBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])</span><br><span class=\"line\">for (index, value) in myVector.enumerated() &#123; myVector[index] = Float(index) &#125;</span><br><span class=\"line\">var inBuffer = device.makeBuffer(bytes: myVector, length: length, options: [])</span><br></pre></td></tr></table></figure>"},{"title":"models","date":"2021-03-23T07:14:00.000Z","_content":"介绍paddle、ncnn、tnn使用\n\n# paddle\n## x2paddle\nx2paddle --framework=onnx --model=onnx_model.onnx --save_dir=mobilenet\n\n## paddle2onnx\npaddle2onnx --model_dir paddle_model  --save_file onnx_file --opset_version 10 --enable_onnx_checker True\npaddle2onnx --model_dir paddle_model  --model_filename model_filename --params_filename params_filename --save_file onnx_file --opset_version 10 --enable_onnx_checker True\n","source":"_posts/article/models.md","raw":"---\ntitle: models\ndate: 2021-03-23 15:14:00\ntags:\n---\n介绍paddle、ncnn、tnn使用\n\n# paddle\n## x2paddle\nx2paddle --framework=onnx --model=onnx_model.onnx --save_dir=mobilenet\n\n## paddle2onnx\npaddle2onnx --model_dir paddle_model  --save_file onnx_file --opset_version 10 --enable_onnx_checker True\npaddle2onnx --model_dir paddle_model  --model_filename model_filename --params_filename params_filename --save_file onnx_file --opset_version 10 --enable_onnx_checker True\n","slug":"article/models","published":1,"updated":"2025-09-29T14:59:49.937Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x000cotzkem00agoz","content":"<p>介绍paddle、ncnn、tnn使用</p>\n<h1 id=\"paddle\"><a href=\"#paddle\" class=\"headerlink\" title=\"paddle\"></a>paddle</h1><h2 id=\"x2paddle\"><a href=\"#x2paddle\" class=\"headerlink\" title=\"x2paddle\"></a>x2paddle</h2><p>x2paddle –framework&#x3D;onnx –model&#x3D;onnx_model.onnx –save_dir&#x3D;mobilenet</p>\n<h2 id=\"paddle2onnx\"><a href=\"#paddle2onnx\" class=\"headerlink\" title=\"paddle2onnx\"></a>paddle2onnx</h2><p>paddle2onnx –model_dir paddle_model  –save_file onnx_file –opset_version 10 –enable_onnx_checker True<br>paddle2onnx –model_dir paddle_model  –model_filename model_filename –params_filename params_filename –save_file onnx_file –opset_version 10 –enable_onnx_checker True</p>\n","excerpt":"","more":"<p>介绍paddle、ncnn、tnn使用</p>\n<h1 id=\"paddle\"><a href=\"#paddle\" class=\"headerlink\" title=\"paddle\"></a>paddle</h1><h2 id=\"x2paddle\"><a href=\"#x2paddle\" class=\"headerlink\" title=\"x2paddle\"></a>x2paddle</h2><p>x2paddle –framework&#x3D;onnx –model&#x3D;onnx_model.onnx –save_dir&#x3D;mobilenet</p>\n<h2 id=\"paddle2onnx\"><a href=\"#paddle2onnx\" class=\"headerlink\" title=\"paddle2onnx\"></a>paddle2onnx</h2><p>paddle2onnx –model_dir paddle_model  –save_file onnx_file –opset_version 10 –enable_onnx_checker True<br>paddle2onnx –model_dir paddle_model  –model_filename model_filename –params_filename params_filename –save_file onnx_file –opset_version 10 –enable_onnx_checker True</p>\n"},{"title":"notes","date":"2021-03-11T21:24:19.000Z","_content":"一些记录\n\n# ccache和distcc\n```\nexport USE_CCACHE=1\nexport CCACHE_DIR=/home/xx/tools/.ccache\nccache -M 50G\nccache -s\nccache -C\n```\n```\ncmake中使用ccache的最加方案：cmake > 3.5，命令行上-DCMAKE_CXX_COMPILER_LAUNCHER=ccache配置文件\nfind_program(CCACHE_FOUND ccache)\n    if(CCACHE_FOUND)  \n        set(CMAKE_CXX_COMPILER_LAUNCHER ccache)\n    endif()\n```\n\n# conv\n| input      | filter    | output     |\n| ---------- | --------- | ---------- |\n| 1x32x40x80 | 16x32x3x3 | 1x16x40x80 |\n\nint kernel_size = kernel_w * kernel_h;\nint num_input = weight_data_size / kernel_size / num_output;\n\n# cento8 epel配置aliyun源\n首先安装epel配置包\nyum install -y  https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm\n然后将 repo 配置中的地址替换为阿里云镜像站地址\nsed -i 's|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|' /etc/yum.repos.d/epel*\nsed -i 's|^metalink|#metalink|' /etc/yum.repos.d/epel*\n","source":"_posts/article/notes.md","raw":"---\ntitle: notes\ndate: 2021-03-12 05:24:19\ntags:\n---\n一些记录\n\n# ccache和distcc\n```\nexport USE_CCACHE=1\nexport CCACHE_DIR=/home/xx/tools/.ccache\nccache -M 50G\nccache -s\nccache -C\n```\n```\ncmake中使用ccache的最加方案：cmake > 3.5，命令行上-DCMAKE_CXX_COMPILER_LAUNCHER=ccache配置文件\nfind_program(CCACHE_FOUND ccache)\n    if(CCACHE_FOUND)  \n        set(CMAKE_CXX_COMPILER_LAUNCHER ccache)\n    endif()\n```\n\n# conv\n| input      | filter    | output     |\n| ---------- | --------- | ---------- |\n| 1x32x40x80 | 16x32x3x3 | 1x16x40x80 |\n\nint kernel_size = kernel_w * kernel_h;\nint num_input = weight_data_size / kernel_size / num_output;\n\n# cento8 epel配置aliyun源\n首先安装epel配置包\nyum install -y  https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm\n然后将 repo 配置中的地址替换为阿里云镜像站地址\nsed -i 's|^#baseurl=https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|' /etc/yum.repos.d/epel*\nsed -i 's|^metalink|#metalink|' /etc/yum.repos.d/epel*\n","slug":"article/notes","published":1,"updated":"2025-09-29T14:59:49.938Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x000dotzk6yihd868","content":"<p>一些记录</p>\n<h1 id=\"ccache和distcc\"><a href=\"#ccache和distcc\" class=\"headerlink\" title=\"ccache和distcc\"></a>ccache和distcc</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export USE_CCACHE=1</span><br><span class=\"line\">export CCACHE_DIR=/home/xx/tools/.ccache</span><br><span class=\"line\">ccache -M 50G</span><br><span class=\"line\">ccache -s</span><br><span class=\"line\">ccache -C</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake中使用ccache的最加方案：cmake &gt; 3.5，命令行上-DCMAKE_CXX_COMPILER_LAUNCHER=ccache配置文件</span><br><span class=\"line\">find_program(CCACHE_FOUND ccache)</span><br><span class=\"line\">    if(CCACHE_FOUND)  </span><br><span class=\"line\">        set(CMAKE_CXX_COMPILER_LAUNCHER ccache)</span><br><span class=\"line\">    endif()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"conv\"><a href=\"#conv\" class=\"headerlink\" title=\"conv\"></a>conv</h1><table>\n<thead>\n<tr>\n<th>input</th>\n<th>filter</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1x32x40x80</td>\n<td>16x32x3x3</td>\n<td>1x16x40x80</td>\n</tr>\n</tbody></table>\n<p>int kernel_size &#x3D; kernel_w * kernel_h;<br>int num_input &#x3D; weight_data_size &#x2F; kernel_size &#x2F; num_output;</p>\n<h1 id=\"cento8-epel配置aliyun源\"><a href=\"#cento8-epel配置aliyun源\" class=\"headerlink\" title=\"cento8 epel配置aliyun源\"></a>cento8 epel配置aliyun源</h1><p>首先安装epel配置包<br>yum install -y  <a href=\"https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm\">https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm</a><br>然后将 repo 配置中的地址替换为阿里云镜像站地址<br>sed -i ‘s|^#baseurl&#x3D;<a href=\"https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|\">https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|</a>‘ &#x2F;etc&#x2F;yum.repos.d&#x2F;epel*<br>sed -i ‘s|^metalink|#metalink|’ &#x2F;etc&#x2F;yum.repos.d&#x2F;epel*</p>\n","excerpt":"","more":"<p>一些记录</p>\n<h1 id=\"ccache和distcc\"><a href=\"#ccache和distcc\" class=\"headerlink\" title=\"ccache和distcc\"></a>ccache和distcc</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export USE_CCACHE=1</span><br><span class=\"line\">export CCACHE_DIR=/home/xx/tools/.ccache</span><br><span class=\"line\">ccache -M 50G</span><br><span class=\"line\">ccache -s</span><br><span class=\"line\">ccache -C</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake中使用ccache的最加方案：cmake &gt; 3.5，命令行上-DCMAKE_CXX_COMPILER_LAUNCHER=ccache配置文件</span><br><span class=\"line\">find_program(CCACHE_FOUND ccache)</span><br><span class=\"line\">    if(CCACHE_FOUND)  </span><br><span class=\"line\">        set(CMAKE_CXX_COMPILER_LAUNCHER ccache)</span><br><span class=\"line\">    endif()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"conv\"><a href=\"#conv\" class=\"headerlink\" title=\"conv\"></a>conv</h1><table>\n<thead>\n<tr>\n<th>input</th>\n<th>filter</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1x32x40x80</td>\n<td>16x32x3x3</td>\n<td>1x16x40x80</td>\n</tr>\n</tbody></table>\n<p>int kernel_size &#x3D; kernel_w * kernel_h;<br>int num_input &#x3D; weight_data_size &#x2F; kernel_size &#x2F; num_output;</p>\n<h1 id=\"cento8-epel配置aliyun源\"><a href=\"#cento8-epel配置aliyun源\" class=\"headerlink\" title=\"cento8 epel配置aliyun源\"></a>cento8 epel配置aliyun源</h1><p>首先安装epel配置包<br>yum install -y  <a href=\"https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm\">https://mirrors.aliyun.com/epel/epel-release-latest-8.noarch.rpm</a><br>然后将 repo 配置中的地址替换为阿里云镜像站地址<br>sed -i ‘s|^#baseurl&#x3D;<a href=\"https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|\">https://download.fedoraproject.org/pub|baseurl=https://mirrors.aliyun.com|</a>‘ &#x2F;etc&#x2F;yum.repos.d&#x2F;epel*<br>sed -i ‘s|^metalink|#metalink|’ &#x2F;etc&#x2F;yum.repos.d&#x2F;epel*</p>\n"},{"title":"Modern Cpp","date":"2022-01-08T23:03:59.000Z","_content":"\n# cpp11\n\n## type traits\n* std::integral_constant\n\n    wrap a static constant of specified type. Defined in <type_traits>\n    ```\n    template<class T, T v>\n    struct integral_constant;\n    ``` \n","source":"_posts/article/modern_cpp.md","raw":"---\ntitle: Modern Cpp\ndate: 2022-01-09 07:03:59\ntags:\n---\n\n# cpp11\n\n## type traits\n* std::integral_constant\n\n    wrap a static constant of specified type. Defined in <type_traits>\n    ```\n    template<class T, T v>\n    struct integral_constant;\n    ``` \n","slug":"article/modern_cpp","published":1,"updated":"2025-09-29T14:59:49.936Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905x000eotzk219egeur","content":"<h1 id=\"cpp11\"><a href=\"#cpp11\" class=\"headerlink\" title=\"cpp11\"></a>cpp11</h1><h2 id=\"type-traits\"><a href=\"#type-traits\" class=\"headerlink\" title=\"type traits\"></a>type traits</h2><ul>\n<li><p>std::integral_constant</p>\n<p>  wrap a static constant of specified type. Defined in <type_traits></p>\n  <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;class T, T v&gt;</span><br><span class=\"line\">struct integral_constant;</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"cpp11\"><a href=\"#cpp11\" class=\"headerlink\" title=\"cpp11\"></a>cpp11</h1><h2 id=\"type-traits\"><a href=\"#type-traits\" class=\"headerlink\" title=\"type traits\"></a>type traits</h2><ul>\n<li><p>std::integral_constant</p>\n<p>  wrap a static constant of specified type. Defined in <type_traits></p>\n  <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;class T, T v&gt;</span><br><span class=\"line\">struct integral_constant;</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"Nsight System","date":"2022-03-16T23:48:55.000Z","_content":"\n# Nsight System\n下载地址：https://developer.nvidia.com/gameworksdownload#?search=nsight\n\n# Nsight Compute\n","source":"_posts/article/nsight-system.md","raw":"---\ntitle: Nsight System\ndate: 2022-03-17 07:48:55\ntags:\n---\n\n# Nsight System\n下载地址：https://developer.nvidia.com/gameworksdownload#?search=nsight\n\n# Nsight Compute\n","slug":"article/nsight-system","published":1,"updated":"2025-09-29T14:59:49.934Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000fotzk7rembe1g","content":"<h1 id=\"Nsight-System\"><a href=\"#Nsight-System\" class=\"headerlink\" title=\"Nsight System\"></a>Nsight System</h1><p>下载地址：<a href=\"https://developer.nvidia.com/gameworksdownload#?search=nsight\">https://developer.nvidia.com/gameworksdownload#?search=nsight</a></p>\n<h1 id=\"Nsight-Compute\"><a href=\"#Nsight-Compute\" class=\"headerlink\" title=\"Nsight Compute\"></a>Nsight Compute</h1>","excerpt":"","more":"<h1 id=\"Nsight-System\"><a href=\"#Nsight-System\" class=\"headerlink\" title=\"Nsight System\"></a>Nsight System</h1><p>下载地址：<a href=\"https://developer.nvidia.com/gameworksdownload#?search=nsight\">https://developer.nvidia.com/gameworksdownload#?search=nsight</a></p>\n<h1 id=\"Nsight-Compute\"><a href=\"#Nsight-Compute\" class=\"headerlink\" title=\"Nsight Compute\"></a>Nsight Compute</h1>"},{"title":"MNN","date":"2021-06-07T04:27:48.000Z","_content":"本文主要介绍Alibaba MNN编译使用。\n\n# 下载编译\n## 下载\n``` shell\ngit clone github.com/alibaba/MNN\n```\n## linux x86\n```\n./schema/generate.sh\nmkdir build && cd build && cmake .. && make -j4\n```\nrefer to ```https://www.yuque.com/mnn/en/build_linux```\n\n\n# 模型部署示例\n``` c\nint main(){\n  return main();\n}\n```","source":"_posts/article/mnn.md","raw":"---\ntitle: MNN\ndate: 2021-06-07 12:27:48\ntags:\n---\n本文主要介绍Alibaba MNN编译使用。\n\n# 下载编译\n## 下载\n``` shell\ngit clone github.com/alibaba/MNN\n```\n## linux x86\n```\n./schema/generate.sh\nmkdir build && cd build && cmake .. && make -j4\n```\nrefer to ```https://www.yuque.com/mnn/en/build_linux```\n\n\n# 模型部署示例\n``` c\nint main(){\n  return main();\n}\n```","slug":"article/mnn","published":1,"updated":"2025-09-29T14:59:49.933Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000gotzkf6jshqne","content":"<p>本文主要介绍Alibaba MNN编译使用。</p>\n<h1 id=\"下载编译\"><a href=\"#下载编译\" class=\"headerlink\" title=\"下载编译\"></a>下载编译</h1><h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone github.com/alibaba/MNN</span><br></pre></td></tr></table></figure>\n<h2 id=\"linux-x86\"><a href=\"#linux-x86\" class=\"headerlink\" title=\"linux x86\"></a>linux x86</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./schema/generate.sh</span><br><span class=\"line\">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make -j4</span><br></pre></td></tr></table></figure>\n<p>refer to <code>https://www.yuque.com/mnn/en/build_linux</code></p>\n<h1 id=\"模型部署示例\"><a href=\"#模型部署示例\" class=\"headerlink\" title=\"模型部署示例\"></a>模型部署示例</h1><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> main();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<p>本文主要介绍Alibaba MNN编译使用。</p>\n<h1 id=\"下载编译\"><a href=\"#下载编译\" class=\"headerlink\" title=\"下载编译\"></a>下载编译</h1><h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone github.com/alibaba/MNN</span><br></pre></td></tr></table></figure>\n<h2 id=\"linux-x86\"><a href=\"#linux-x86\" class=\"headerlink\" title=\"linux x86\"></a>linux x86</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./schema/generate.sh</span><br><span class=\"line\">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make -j4</span><br></pre></td></tr></table></figure>\n<p>refer to <code>https://www.yuque.com/mnn/en/build_linux</code></p>\n<h1 id=\"模型部署示例\"><a href=\"#模型部署示例\" class=\"headerlink\" title=\"模型部署示例\"></a>模型部署示例</h1><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">()</span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> main();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"OpenCL","date":"2022-03-05T21:42:42.000Z","_content":"\n![opencl](https://pic2.zhimg.com/80/v2-a7526bdfdb0c6372745f272d6f315291_1440w.jpg)\n![opencl](https://pic2.zhimg.com/80/v2-2ce3ca6ae987befcc7bfb9f3360e0acd_1440w.jpg)\n# OpenCL平台模型\nfiber(work item) - wave - workgroup\n\n(thread - warp - block?)\n# OpenCL执行模型\n## 上下文\n## 命令队列\n## kernel执行\n\n# OpenCL存储器模型\n## 存储类型\n* host memory\n* global memory\n* constant memory\n  片内延迟低，系统RAM延迟高。work group中所有work item的常量数据。\n* local memory\n  一个work group内的所有work item共享。\n  Local Memory coalesced access\n  ![coalesced access](https://pic3.zhimg.com/80/v2-4ab02630d7f1fdc1d01ce2973316788e_1440w.jpg)\n* private memory\n\n## 存储对象类型\n\n* buffer\n* image\n* pipe\n\n# OpenCL API\nclCreateProgramWithSource()\nclBuildProgram()\nclLinkProgram()\nclUnloadPlatformCompiler()\nclCreateProgramWithBinary()\n\nclCreate{Image|Buffer}\nclEnqueueNDRangeKernel()\n\ncl_mem clCreateBuffer (\n    cl_context context,\n    cl_mem_flags flags,\n    size_t size,\n    void *host_ptr,\n    cl_int *errcode_ret)\n# OpenCL性能优化\n\n## 内存\n* local memory\n不同work item之间需要barrier进行同步，操作耗时。\n\n不同work item之间交换数据需要barrier进行同步。\n\nBarrier 经常会导致同步延迟，从而阻塞ALU，导致更低的ALU的使用效率。\n\n在某些情况下，将数据缓冲到本地内存中可能会需要同步，同步产生的延迟将会抵消使用本地内存带来的性能提升。在这种情况下，直接使用全局内存，避免使用barrier可能是更好的选择。\n\n\n# OpenCL 资料\nhttps://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf\nhttps://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html\nhttps://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization\n\n[Qualcomm_Mobile_OpenCL 中文翻译](https://www.cnblogs.com/xiajingwang/p/11120561.html)\n","source":"_posts/article/opencl.md","raw":"---\ntitle: OpenCL\ndate: 2022-03-06 05:42:42\ntags:\n---\n\n![opencl](https://pic2.zhimg.com/80/v2-a7526bdfdb0c6372745f272d6f315291_1440w.jpg)\n![opencl](https://pic2.zhimg.com/80/v2-2ce3ca6ae987befcc7bfb9f3360e0acd_1440w.jpg)\n# OpenCL平台模型\nfiber(work item) - wave - workgroup\n\n(thread - warp - block?)\n# OpenCL执行模型\n## 上下文\n## 命令队列\n## kernel执行\n\n# OpenCL存储器模型\n## 存储类型\n* host memory\n* global memory\n* constant memory\n  片内延迟低，系统RAM延迟高。work group中所有work item的常量数据。\n* local memory\n  一个work group内的所有work item共享。\n  Local Memory coalesced access\n  ![coalesced access](https://pic3.zhimg.com/80/v2-4ab02630d7f1fdc1d01ce2973316788e_1440w.jpg)\n* private memory\n\n## 存储对象类型\n\n* buffer\n* image\n* pipe\n\n# OpenCL API\nclCreateProgramWithSource()\nclBuildProgram()\nclLinkProgram()\nclUnloadPlatformCompiler()\nclCreateProgramWithBinary()\n\nclCreate{Image|Buffer}\nclEnqueueNDRangeKernel()\n\ncl_mem clCreateBuffer (\n    cl_context context,\n    cl_mem_flags flags,\n    size_t size,\n    void *host_ptr,\n    cl_int *errcode_ret)\n# OpenCL性能优化\n\n## 内存\n* local memory\n不同work item之间需要barrier进行同步，操作耗时。\n\n不同work item之间交换数据需要barrier进行同步。\n\nBarrier 经常会导致同步延迟，从而阻塞ALU，导致更低的ALU的使用效率。\n\n在某些情况下，将数据缓冲到本地内存中可能会需要同步，同步产生的延迟将会抵消使用本地内存带来的性能提升。在这种情况下，直接使用全局内存，避免使用barrier可能是更好的选择。\n\n\n# OpenCL 资料\nhttps://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf\nhttps://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html\nhttps://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization\n\n[Qualcomm_Mobile_OpenCL 中文翻译](https://www.cnblogs.com/xiajingwang/p/11120561.html)\n","slug":"article/opencl","published":1,"updated":"2025-09-29T14:59:49.933Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000hotzkf9xvaobw","content":"<p><img src=\"https://pic2.zhimg.com/80/v2-a7526bdfdb0c6372745f272d6f315291_1440w.jpg\" alt=\"opencl\"><br><img src=\"https://pic2.zhimg.com/80/v2-2ce3ca6ae987befcc7bfb9f3360e0acd_1440w.jpg\" alt=\"opencl\"></p>\n<h1 id=\"OpenCL平台模型\"><a href=\"#OpenCL平台模型\" class=\"headerlink\" title=\"OpenCL平台模型\"></a>OpenCL平台模型</h1><p>fiber(work item) - wave - workgroup</p>\n<p>(thread - warp - block?)</p>\n<h1 id=\"OpenCL执行模型\"><a href=\"#OpenCL执行模型\" class=\"headerlink\" title=\"OpenCL执行模型\"></a>OpenCL执行模型</h1><h2 id=\"上下文\"><a href=\"#上下文\" class=\"headerlink\" title=\"上下文\"></a>上下文</h2><h2 id=\"命令队列\"><a href=\"#命令队列\" class=\"headerlink\" title=\"命令队列\"></a>命令队列</h2><h2 id=\"kernel执行\"><a href=\"#kernel执行\" class=\"headerlink\" title=\"kernel执行\"></a>kernel执行</h2><h1 id=\"OpenCL存储器模型\"><a href=\"#OpenCL存储器模型\" class=\"headerlink\" title=\"OpenCL存储器模型\"></a>OpenCL存储器模型</h1><h2 id=\"存储类型\"><a href=\"#存储类型\" class=\"headerlink\" title=\"存储类型\"></a>存储类型</h2><ul>\n<li>host memory</li>\n<li>global memory</li>\n<li>constant memory<br>片内延迟低，系统RAM延迟高。work group中所有work item的常量数据。</li>\n<li>local memory<br>一个work group内的所有work item共享。<br>Local Memory coalesced access<br><img src=\"https://pic3.zhimg.com/80/v2-4ab02630d7f1fdc1d01ce2973316788e_1440w.jpg\" alt=\"coalesced access\"></li>\n<li>private memory</li>\n</ul>\n<h2 id=\"存储对象类型\"><a href=\"#存储对象类型\" class=\"headerlink\" title=\"存储对象类型\"></a>存储对象类型</h2><ul>\n<li>buffer</li>\n<li>image</li>\n<li>pipe</li>\n</ul>\n<h1 id=\"OpenCL-API\"><a href=\"#OpenCL-API\" class=\"headerlink\" title=\"OpenCL API\"></a>OpenCL API</h1><p>clCreateProgramWithSource()<br>clBuildProgram()<br>clLinkProgram()<br>clUnloadPlatformCompiler()<br>clCreateProgramWithBinary()</p>\n<p>clCreate{Image|Buffer}<br>clEnqueueNDRangeKernel()</p>\n<p>cl_mem clCreateBuffer (<br>    cl_context context,<br>    cl_mem_flags flags,<br>    size_t size,<br>    void *host_ptr,<br>    cl_int *errcode_ret)</p>\n<h1 id=\"OpenCL性能优化\"><a href=\"#OpenCL性能优化\" class=\"headerlink\" title=\"OpenCL性能优化\"></a>OpenCL性能优化</h1><h2 id=\"内存\"><a href=\"#内存\" class=\"headerlink\" title=\"内存\"></a>内存</h2><ul>\n<li>local memory<br>不同work item之间需要barrier进行同步，操作耗时。</li>\n</ul>\n<p>不同work item之间交换数据需要barrier进行同步。</p>\n<p>Barrier 经常会导致同步延迟，从而阻塞ALU，导致更低的ALU的使用效率。</p>\n<p>在某些情况下，将数据缓冲到本地内存中可能会需要同步，同步产生的延迟将会抵消使用本地内存带来的性能提升。在这种情况下，直接使用全局内存，避免使用barrier可能是更好的选择。</p>\n<h1 id=\"OpenCL-资料\"><a href=\"#OpenCL-资料\" class=\"headerlink\" title=\"OpenCL 资料\"></a>OpenCL 资料</h1><p><a href=\"https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf\">https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf</a><br><a href=\"https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html\">https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html</a><br><a href=\"https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization\">https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization</a></p>\n<p><a href=\"https://www.cnblogs.com/xiajingwang/p/11120561.html\">Qualcomm_Mobile_OpenCL 中文翻译</a></p>\n","excerpt":"","more":"<p><img src=\"https://pic2.zhimg.com/80/v2-a7526bdfdb0c6372745f272d6f315291_1440w.jpg\" alt=\"opencl\"><br><img src=\"https://pic2.zhimg.com/80/v2-2ce3ca6ae987befcc7bfb9f3360e0acd_1440w.jpg\" alt=\"opencl\"></p>\n<h1 id=\"OpenCL平台模型\"><a href=\"#OpenCL平台模型\" class=\"headerlink\" title=\"OpenCL平台模型\"></a>OpenCL平台模型</h1><p>fiber(work item) - wave - workgroup</p>\n<p>(thread - warp - block?)</p>\n<h1 id=\"OpenCL执行模型\"><a href=\"#OpenCL执行模型\" class=\"headerlink\" title=\"OpenCL执行模型\"></a>OpenCL执行模型</h1><h2 id=\"上下文\"><a href=\"#上下文\" class=\"headerlink\" title=\"上下文\"></a>上下文</h2><h2 id=\"命令队列\"><a href=\"#命令队列\" class=\"headerlink\" title=\"命令队列\"></a>命令队列</h2><h2 id=\"kernel执行\"><a href=\"#kernel执行\" class=\"headerlink\" title=\"kernel执行\"></a>kernel执行</h2><h1 id=\"OpenCL存储器模型\"><a href=\"#OpenCL存储器模型\" class=\"headerlink\" title=\"OpenCL存储器模型\"></a>OpenCL存储器模型</h1><h2 id=\"存储类型\"><a href=\"#存储类型\" class=\"headerlink\" title=\"存储类型\"></a>存储类型</h2><ul>\n<li>host memory</li>\n<li>global memory</li>\n<li>constant memory<br>片内延迟低，系统RAM延迟高。work group中所有work item的常量数据。</li>\n<li>local memory<br>一个work group内的所有work item共享。<br>Local Memory coalesced access<br><img src=\"https://pic3.zhimg.com/80/v2-4ab02630d7f1fdc1d01ce2973316788e_1440w.jpg\" alt=\"coalesced access\"></li>\n<li>private memory</li>\n</ul>\n<h2 id=\"存储对象类型\"><a href=\"#存储对象类型\" class=\"headerlink\" title=\"存储对象类型\"></a>存储对象类型</h2><ul>\n<li>buffer</li>\n<li>image</li>\n<li>pipe</li>\n</ul>\n<h1 id=\"OpenCL-API\"><a href=\"#OpenCL-API\" class=\"headerlink\" title=\"OpenCL API\"></a>OpenCL API</h1><p>clCreateProgramWithSource()<br>clBuildProgram()<br>clLinkProgram()<br>clUnloadPlatformCompiler()<br>clCreateProgramWithBinary()</p>\n<p>clCreate{Image|Buffer}<br>clEnqueueNDRangeKernel()</p>\n<p>cl_mem clCreateBuffer (<br>    cl_context context,<br>    cl_mem_flags flags,<br>    size_t size,<br>    void *host_ptr,<br>    cl_int *errcode_ret)</p>\n<h1 id=\"OpenCL性能优化\"><a href=\"#OpenCL性能优化\" class=\"headerlink\" title=\"OpenCL性能优化\"></a>OpenCL性能优化</h1><h2 id=\"内存\"><a href=\"#内存\" class=\"headerlink\" title=\"内存\"></a>内存</h2><ul>\n<li>local memory<br>不同work item之间需要barrier进行同步，操作耗时。</li>\n</ul>\n<p>不同work item之间交换数据需要barrier进行同步。</p>\n<p>Barrier 经常会导致同步延迟，从而阻塞ALU，导致更低的ALU的使用效率。</p>\n<p>在某些情况下，将数据缓冲到本地内存中可能会需要同步，同步产生的延迟将会抵消使用本地内存带来的性能提升。在这种情况下，直接使用全局内存，避免使用barrier可能是更好的选择。</p>\n<h1 id=\"OpenCL-资料\"><a href=\"#OpenCL-资料\" class=\"headerlink\" title=\"OpenCL 资料\"></a>OpenCL 资料</h1><p><a href=\"https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf\">https://developer.qualcomm.com/download/adrenosdk/adreno-opencl-programming-guide.pdf</a><br><a href=\"https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html\">https://developer.qualcomm.com/sites/default/files/docs/adreno-gpu/developer-guide/gpu/gpu.html</a><br><a href=\"https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization\">https://developer.qualcomm.com/blog/matrix-multiply-adreno-gpus-part-1-opencl-optimization</a></p>\n<p><a href=\"https://www.cnblogs.com/xiajingwang/p/11120561.html\">Qualcomm_Mobile_OpenCL 中文翻译</a></p>\n"},{"title":"Paddle Lite framework","date":"2021-09-19T06:13:58.000Z","_content":"\n# core\n## context\n```\nclass KernelContext {\n public:\n  template <typename ContextT>\n  ContextT& As() {\n    if (!ctx_.valid()) {\n      ctx_.set<ContextT>();\n    }\n    return *ctx_.get_mutable<ContextT>();\n  }\n\n private:\n  Any ctx_;\n};\n```\n```\nclass ContextScheduler {\n public:\n  static ContextScheduler& Global() {\n    static auto* x = new ContextScheduler;\n    return *x;\n  }\n  std::unique_ptr<KernelContext> NewContext(\n      TargetType target,\n      /*only used for cuda context*/ int exec_stream_id = 0) {\n    std::unique_ptr<KernelContext> ctx(new KernelContext);\n    switch (target) {\n      case TARGET(kHost):\n        kernel_contexts_[TargetType::kHost].As<HostContext>().CopySharedTo(\n            &ctx->As<HostContext>());\n        break;\n    }\n    return ctx;\n  } \nprivate:\n  template <TargetType Type, typename ContextT>\n  void InitContext() {\n    kernel_contexts_[Type].As<ContextT>().InitOnce();\n  }\n  ContextScheduler() {\n    InitContext<TargetType::kHost, HostContext>();\n  }\nprivate:\n  std::map<TargetType, KernelContext> kernel_contexts_;\n};\n```\n## op lite\n```\nclass OpLite : public Registry {\n public:\n  OpLite() = default;\n  explicit OpLite(const std::string &type) : op_type_(type) {}\n  explicit OpLite(const std::vector<Place> &valid_places)\n      : valid_places_(valid_places) {}\n\n  void SetValidPlaces(const std::vector<Place> &places) {\n    VLOG(5) << \"valid places \" << valid_places_.size();\n    valid_places_ = places;\n  }\n  virtual bool Run();\n  // Indicate whether the Op runs only once or not\n  virtual bool run_once() const { return false; }\n  std::string Type() const { return op_type_; }\n\n  // Link the external execution environ to internal context.\n  bool Attach(const cpp::OpDesc &opdesc, lite::Scope *scope);\n\n  template <typename T>\n  inline void AttachParam(T *param) {\n    op_param_ = static_cast<T *>(param);\n  }\n  // Create all the kernels for the valid targets.\n  std::vector<std::unique_ptr<KernelBase>> CreateKernels(\n      const std::vector<Place> &places, const std::string &kernel_type = \"\");\n\n  Scope *scope() { return scope_; }\n\n  // Assign op param to kernel.\n  virtual void AttachKernel(KernelBase *kernel) = 0;\n  void SetKernel(std::vector<std::unique_ptr<KernelBase>> &kernels) {  // NOLINT\n    kernel_ = std::move(kernels.front());\n    kernel_->SetContext(\n        ContextScheduler::Global().NewContext(kernel_->target()));\n  }\n\n  KernelBase *GetKernel() {  // NOLINT\n    return kernel_.get();\n  }\n  virtual ~OpLite() = default;\n protected:\n  friend class mir::Node;\n  friend class mir::SSAGraph;\n protected:\n  Scope *scope_{nullptr};\n  std::unique_ptr<KernelBase> kernel_;\n  std::string op_type_;\n  std::vector<Place> valid_places_;\n  Place kernel_place_{TARGET(kHost), PRECISION(kFloat)};\n  std::unique_ptr<OpInfo> op_info_;\n  // todo: it's prefered to combine last_input_shapes and\n  // last_input_lods into a single hash value to decrease\n  // memory usage.\n  std::vector<DDimLite> last_input_shapes{};\n  std::vector<std::vector<std::vector<uint64_t>>> last_input_lods{};\n  std::vector<DDimLite> last_output_shapes{};\n  std::vector<std::vector<std::vector<uint64_t>>> last_output_lods{};\n  mutable operators::ParamBase *op_param_{nullptr};\n\n private:\n  // Infer Shape according to memory, if current input shapes are consistent\n  // with that of previous inputs, output shapes of last time will be reused.\n  bool InferShapeWithCache();\n};\n```\n```\nstd::vector<std::unique_ptr<KernelBase>> OpLite::CreateKernels(\n    const std::vector<Place> &places, const std::string &kernel_type) {\n  std::vector<std::unique_ptr<KernelBase>> kernels;\n  CHECK(!op_type_.empty()) << \"op_type_ should be set first\";\n\n  auto pick_kernel = [&](const Place &place) {\n    auto ks = KernelRegistry::Global().Create(\n        op_type_, place.target, place.precision, place.layout);\n    VLOG(5) << \"pick kernel for \" << op_info()->Type() << \" \"\n            << place.DebugString() << \" get \" << ks.size() << \" kernels\";\n    for (auto &&it : ks) {\n      AttachKernel(it.get());\n      kernels.emplace_back(std::move(it));\n    }\n  };\n\n  if (!kernel_type.empty()) {\n    Place place;\n    std::string op_type, alias;\n    KernelBase::ParseKernelType(kernel_type, &op_type, &alias, &place);\n    pick_kernel(place);\n    CHECK(!kernels.empty()) << \"no kernel for kernel type \" << kernel_type;\n    return kernels;\n  }\n\n  std::set<Place> expanded_places(places.begin(), places.end());\n  for (auto &place : places) {\n    // Pick kernels those support any Precision and any DataLayout, For example:\n    // kARM,kFloat,kNCHW -> kARM,kFloat,kAny; kARM,kAny,kNCHW; kARM,kAny,kAny\n    expanded_places.insert(\n        Place(place.target, place.precision, DATALAYOUT(kAny)));\n    expanded_places.insert(Place(place.target, PRECISION(kAny), place.layout));\n    expanded_places.insert(\n        Place(place.target, PRECISION(kAny), DATALAYOUT(kAny)));\n  }\n\n  std::set<TargetType> targets;\n  for (auto place : expanded_places) {\n    pick_kernel(place);\n    targets.insert(place.target);\n  }\n\n  VLOG(5) << \"op \" << op_type_ << \" get \" << kernels.size() << \" kernels\";\n  return kernels;\n}\n```\n```\n/*\n * Operator Information, such as some description. It will be shared by all the\n * kernels of the same operator.\n */\nclass OpInfo : public cpp::OpDesc {\n public:\n  OpInfo(const OpInfo &) = default;\n  explicit OpInfo(const cpp::OpDesc &other) : cpp::OpDesc(other) {}\n};\n```\n## op registry\n```\nclass OpKernelInfoCollector {\n public:\n  static OpKernelInfoCollector& Global() {\n    static auto* x = new OpKernelInfoCollector;\n    return *x;\n  }\n  void AddOp2path(const std::string& op_name, const std::string& op_path);\n  void AddKernel2path(const std::string& kernel_name,\n                      const std::string& kernel_path);\n  \n private:\n  std::map<std::string, std::string> op2path_;\n  std::map<std::string, std::string> kernel2path_;\n};\n```\n```\nclass OpLiteFactory {\n public:\n  // Register a function to create an op\n  void RegisterCreator(const std::string& op_type,\n                       std::function<std::shared_ptr<OpLite>()> fun) {\n    op_registry_[op_type] = fun;\n  }\n\n  static OpLiteFactory& Global() {\n    static OpLiteFactory* x = new OpLiteFactory;\n    return *x;\n  }\n\n  std::shared_ptr<OpLite> Create(const std::string& op_type) const {\n    auto it = op_registry_.find(op_type);\n    if (it == op_registry_.end()) return nullptr;\n    return it->second();\n  }\n\n  std::string DebugString();\n\n  std::vector<std::string> GetAllOps() const {\n    std::vector<std::string> res;\n    for (const auto& op : op_registry_) {\n      res.push_back(op.first);\n    }\n    return res;\n  }\n\n protected:\n  std::map<std::string, std::function<std::shared_ptr<OpLite>()>> op_registry_;\n};\n```\n```\nclass OpLiteRegistrar {\n public:\n  OpLiteRegistrar(const std::string& op_type,\n                  std::function<std::shared_ptr<OpLite>()> fun) {\n    OpLiteFactory::Global().RegisterCreator(op_type, fun);\n  }\n  // Touch function is used to guarantee registrar was initialized.\n  void touch() {}\n};\n\nclass KernelFactory {\n public:\n  // Register a function to create kernels\n  void RegisterCreator(const std::string& op_type,\n                       TargetType target,\n                       PrecisionType precision,\n                       DataLayoutType layout,\n                       std::function<std::unique_ptr<KernelBase>()> fun) {\n    op_registry_[op_type][std::make_tuple(target, precision, layout)].push_back(\n        fun);\n  }\n\n  static KernelFactory& Global() {\n    static KernelFactory* x = new KernelFactory;\n    return *x;\n  }\n\n  /**\n   * Create all kernels belongs to an op.\n   */\n  std::list<std::unique_ptr<KernelBase>> Create(const std::string& op_type) {\n    std::list<std::unique_ptr<KernelBase>> res;\n    if (op_registry_.find(op_type) == op_registry_.end()) return res;\n    auto& kernel_registry = op_registry_[op_type];\n    for (auto it = kernel_registry.begin(); it != kernel_registry.end(); ++it) {\n      for (auto& fun : it->second) {\n        res.emplace_back(fun());\n      }\n    }\n    return res;\n  }\n\n  /**\n   * Create a specific kernel. Return a list for API compatible.\n   */\n  std::list<std::unique_ptr<KernelBase>> Create(const std::string& op_type,\n                                                TargetType target,\n                                                PrecisionType precision,\n                                                DataLayoutType layout) {\n    std::list<std::unique_ptr<KernelBase>> res;\n    if (op_registry_.find(op_type) == op_registry_.end()) return res;\n    auto& kernel_registry = op_registry_[op_type];\n    auto it = kernel_registry.find(std::make_tuple(target, precision, layout));\n    if (it == kernel_registry.end()) return res;\n    for (auto& fun : it->second) {\n      res.emplace_back(fun());\n    }\n    return res;\n  }\n\n protected:\n  // Outer map: op -> a map of kernel.\n  // Inner map: kernel -> creator function.\n  // Each kernel was represented by a combination of <TargetType, PrecisionType,\n  // DataLayoutType>\n  std::map<std::string,\n           std::map<std::tuple<TargetType, PrecisionType, DataLayoutType>,\n                    std::list<std::function<std::unique_ptr<KernelBase>()>>>>\n      op_registry_;\n};\n\n// Register Kernel by initializing a static KernelRegistrar instance\nclass KernelRegistrar {\n public:\n  KernelRegistrar(const std::string& op_type,\n                  TargetType target,\n                  PrecisionType precision,\n                  DataLayoutType layout,\n                  std::function<std::unique_ptr<KernelBase>()> fun) {\n    KernelFactory::Global().RegisterCreator(\n        op_type, target, precision, layout, fun);\n  }\n  // Touch function is used to guarantee registrar was initialized.\n  void touch() {}\n};\n\nclass ParamTypeDummyRegistry {\n public:\n  struct NewInstance {\n    NewInstance() {}\n    NewInstance& BindInput(const std::string& arg_name,\n                           const ParamType& ptype) {\n      return *this;\n    }\n    NewInstance& BindOutput(const std::string& arg_name,\n                            const ParamType& ptype) {\n      return *this;\n    }\n    NewInstance& SetVersion(const std::string& version) { return *this; }\n    NewInstance& BindPaddleOpVersion(const std::string& op_type,\n                                     int32_t version_id) {\n      return *this;\n    }\n    bool Finalize() { return true; }\n  };\n\n private:\n  ParamTypeDummyRegistry() = default;\n};\n```\n\n# 注册机制\n## op注册\n```\n#define REGISTER_LITE_OP(op_type__, OpClass)                                   \\\n  static paddle::lite::OpLiteRegistrar op_type__##__registry(                  \\\n      #op_type__, []() {                                                       \\\n        return std::unique_ptr<paddle::lite::OpLite>(new OpClass(#op_type__)); \\\n      });                                                                      \\\n  int touch_op_##op_type__() {                                                 \\\n    op_type__##__registry.touch();                                             \\\n    OpKernelInfoCollector::Global().AddOp2path(#op_type__, __FILE__);          \\\n    return 0;                                                                  \\\n  }\n```\n## kernel 注册\n```\n// Register a kernel.\n#define REGISTER_LITE_KERNEL(                                                 \\\n    op_type__, target__, precision__, layout__, KernelClass, alias__)         \\\n  static paddle::lite::KernelRegistrar                                        \\\n      op_type__##target__##precision__##layout__##alias__##_kernel_registry(  \\\n          #op_type__,                                                         \\\n          TARGET(target__),                                                   \\\n          PRECISION(precision__),                                             \\\n          DATALAYOUT(layout__),                                               \\\n          []() {                                                              \\\n            std::unique_ptr<KernelClass> x(new KernelClass);                  \\\n            x->set_op_type(#op_type__);                                       \\\n            x->set_alias(#alias__);                                           \\\n            return x;                                                         \\\n          });                                                                 \\\n  int touch_##op_type__##target__##precision__##layout__##alias__() {         \\\n    op_type__##target__##precision__##layout__##alias__##_kernel_registry     \\\n        .touch();                                                             \\\n    OpKernelInfoCollector::Global().AddKernel2path(                           \\\n        #op_type__ \",\" #target__ \",\" #precision__ \",\" #layout__ \",\" #alias__, \\\n        __FILE__);                                                            \\\n    return 0;                                                                 \\\n  }                                                                           \\\n  ParamTypeRegistry(                                                          \\\n      op_type__, target__, precision__, layout__, KernelClass, alias__)\n```\n# program\n## scope\n```\nclass Scope final {\n public:\n  Scope()\n      : kids_lock_{new lite::fluid::RWLock},\n        vars_lock_{new lite::fluid::RWLock},\n        rwlock_{new lite::fluid::RWLock} {}\n  // delete below two functions to allow pybind to recognise it cannot make a\n  // copy\n  // link:\n  // https://stackoverflow.com/questions/53807248/pybind11-returning-a-pointer-to-a-container-of-unique-ptr\n  Scope(const Scope&) = delete;\n  Scope& operator=(const Scope&) = delete;\n  ~Scope();\n\n  Scope& NewScope() const;\n\n  Variable* Var(const std::string& name);\n\n  Variable* LocalVar(const std::string& name);\n\n  Variable* FindVar(const std::string& name) const;\n\n  Variable* FindLocalVar(const std::string& name) const;\n\n  const Scope* parent() const { return parent_; }\n\n  // Get attribute params stored in parent scopes.\n  std::vector<std::string> AttributeVarNames() const;\n  // Following the legacy scope interface.\n  std::vector<std::string> LocalVarNames() const;\n\n  /// ------------------------------------- helper functions for Tensor\n  /// ----------------------------------\n  // Create a Tensor variable. This will create a new Variable called `name`.\n  Tensor* NewTensor(const std::string& name) {\n    auto* var = Var(name);\n    return var->GetMutable<Tensor>();\n  }\n\n  const Tensor* FindTensor(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return &var->Get<Tensor>();\n  }\n\n  Tensor* FindMutableTensor(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return var->GetMutable<Tensor>();\n  }\n\n  std::vector<Tensor>* NewTensorList(const std::string& name) {\n    auto* var = Var(name);\n    return var->GetMutable<std::vector<Tensor>>();\n  }\n\n  const std::vector<Tensor>* FindTensorList(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return &var->Get<std::vector<Tensor>>();\n  }\n\n  std::vector<Tensor>* FindMutableTensorList(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return var->GetMutable<std::vector<Tensor>>();\n  }\n\n private:\n  // Scope in `kids_` are owned by this class.\n  mutable std::list<Scope*> kids_;\n  const Scope* parent_{nullptr};\n  std::map<std::string, std::unique_ptr<Variable>> vars_;\n  std::unique_ptr<lite::fluid::RWLock> kids_lock_{nullptr};\n  std::unique_ptr<lite::fluid::RWLock> vars_lock_{nullptr};\n  std::unique_ptr<lite::fluid::RWLock> rwlock_{nullptr};\n};\n```\n# optimize\n## optimizer\n```\n/*\n * lite::Optimizer optimize a program. It utilize the mir passes to analysis the\n * program and export an optimized program.\n * Example :\n *       // (1) Create an optimizer\n *       Optimizer optim(valid_places, kernel_pick_factor);\n *       // (2) add an optimizer method\n *       optim.AddPass(\"post_quant_dynamic_pass\");\n *       // (3) analysis a program to export an optimized program\n *       auto program_ = optim.Run(std::move(program));\n */\nclass Optimizer {\n public:\n  Optimizer(const std::vector<Place>& valid_places,\n            core::KernelPickFactor kernel_pick_factor)\n      : valid_places_(valid_places), kernel_pick_factor_(kernel_pick_factor) {\n    CHECK(!valid_places.empty()) << \"At least one valid_place should be set\";\n  }\n\n  // Append a pass to the optimizer.\n  void AddPass(const std::string& pass_name);\n  // Optimize a program to generate a runtime program.\n  std::unique_ptr<RuntimeProgram> Run(Program&& program);\n\n protected:\n  // Run all the added passes.\n  void ApplyPasses(std::vector<std::unique_ptr<mir::SSAGraph>>* graphes);\n\n  // Generate the optimized runtime program.\n  std::unique_ptr<RuntimeProgram> GenRuntimeProgram(\n      std::vector<std::unique_ptr<mir::SSAGraph>>* graphs);\n\n  void InitTargetTypeTransformPass();\n  void InitControlFlowOpUnusedInputsAndOutputsEliminatePass();\n  void InitControlFlowOpSharedInputsAndOutputsPlaceSyncPass();\n  void SpecifyKernelPickTactic(core::KernelPickFactor factor);\n  Scope* exec_scope() { return exec_scope_; }\n\n private:\n  std::vector<Place> valid_places_;\n  Scope* exec_scope_{};\n  std::vector<mir::Pass*> passes_;\n  std::vector<std::unique_ptr<mir::SSAGraph>> graphs_;\n  core::KernelPickFactor kernel_pick_factor_;\n};\n```","source":"_posts/article/paddle-lite.md","raw":"---\ntitle: Paddle Lite framework\ndate: 2021-09-19 14:13:58\ntags:\n---\n\n# core\n## context\n```\nclass KernelContext {\n public:\n  template <typename ContextT>\n  ContextT& As() {\n    if (!ctx_.valid()) {\n      ctx_.set<ContextT>();\n    }\n    return *ctx_.get_mutable<ContextT>();\n  }\n\n private:\n  Any ctx_;\n};\n```\n```\nclass ContextScheduler {\n public:\n  static ContextScheduler& Global() {\n    static auto* x = new ContextScheduler;\n    return *x;\n  }\n  std::unique_ptr<KernelContext> NewContext(\n      TargetType target,\n      /*only used for cuda context*/ int exec_stream_id = 0) {\n    std::unique_ptr<KernelContext> ctx(new KernelContext);\n    switch (target) {\n      case TARGET(kHost):\n        kernel_contexts_[TargetType::kHost].As<HostContext>().CopySharedTo(\n            &ctx->As<HostContext>());\n        break;\n    }\n    return ctx;\n  } \nprivate:\n  template <TargetType Type, typename ContextT>\n  void InitContext() {\n    kernel_contexts_[Type].As<ContextT>().InitOnce();\n  }\n  ContextScheduler() {\n    InitContext<TargetType::kHost, HostContext>();\n  }\nprivate:\n  std::map<TargetType, KernelContext> kernel_contexts_;\n};\n```\n## op lite\n```\nclass OpLite : public Registry {\n public:\n  OpLite() = default;\n  explicit OpLite(const std::string &type) : op_type_(type) {}\n  explicit OpLite(const std::vector<Place> &valid_places)\n      : valid_places_(valid_places) {}\n\n  void SetValidPlaces(const std::vector<Place> &places) {\n    VLOG(5) << \"valid places \" << valid_places_.size();\n    valid_places_ = places;\n  }\n  virtual bool Run();\n  // Indicate whether the Op runs only once or not\n  virtual bool run_once() const { return false; }\n  std::string Type() const { return op_type_; }\n\n  // Link the external execution environ to internal context.\n  bool Attach(const cpp::OpDesc &opdesc, lite::Scope *scope);\n\n  template <typename T>\n  inline void AttachParam(T *param) {\n    op_param_ = static_cast<T *>(param);\n  }\n  // Create all the kernels for the valid targets.\n  std::vector<std::unique_ptr<KernelBase>> CreateKernels(\n      const std::vector<Place> &places, const std::string &kernel_type = \"\");\n\n  Scope *scope() { return scope_; }\n\n  // Assign op param to kernel.\n  virtual void AttachKernel(KernelBase *kernel) = 0;\n  void SetKernel(std::vector<std::unique_ptr<KernelBase>> &kernels) {  // NOLINT\n    kernel_ = std::move(kernels.front());\n    kernel_->SetContext(\n        ContextScheduler::Global().NewContext(kernel_->target()));\n  }\n\n  KernelBase *GetKernel() {  // NOLINT\n    return kernel_.get();\n  }\n  virtual ~OpLite() = default;\n protected:\n  friend class mir::Node;\n  friend class mir::SSAGraph;\n protected:\n  Scope *scope_{nullptr};\n  std::unique_ptr<KernelBase> kernel_;\n  std::string op_type_;\n  std::vector<Place> valid_places_;\n  Place kernel_place_{TARGET(kHost), PRECISION(kFloat)};\n  std::unique_ptr<OpInfo> op_info_;\n  // todo: it's prefered to combine last_input_shapes and\n  // last_input_lods into a single hash value to decrease\n  // memory usage.\n  std::vector<DDimLite> last_input_shapes{};\n  std::vector<std::vector<std::vector<uint64_t>>> last_input_lods{};\n  std::vector<DDimLite> last_output_shapes{};\n  std::vector<std::vector<std::vector<uint64_t>>> last_output_lods{};\n  mutable operators::ParamBase *op_param_{nullptr};\n\n private:\n  // Infer Shape according to memory, if current input shapes are consistent\n  // with that of previous inputs, output shapes of last time will be reused.\n  bool InferShapeWithCache();\n};\n```\n```\nstd::vector<std::unique_ptr<KernelBase>> OpLite::CreateKernels(\n    const std::vector<Place> &places, const std::string &kernel_type) {\n  std::vector<std::unique_ptr<KernelBase>> kernels;\n  CHECK(!op_type_.empty()) << \"op_type_ should be set first\";\n\n  auto pick_kernel = [&](const Place &place) {\n    auto ks = KernelRegistry::Global().Create(\n        op_type_, place.target, place.precision, place.layout);\n    VLOG(5) << \"pick kernel for \" << op_info()->Type() << \" \"\n            << place.DebugString() << \" get \" << ks.size() << \" kernels\";\n    for (auto &&it : ks) {\n      AttachKernel(it.get());\n      kernels.emplace_back(std::move(it));\n    }\n  };\n\n  if (!kernel_type.empty()) {\n    Place place;\n    std::string op_type, alias;\n    KernelBase::ParseKernelType(kernel_type, &op_type, &alias, &place);\n    pick_kernel(place);\n    CHECK(!kernels.empty()) << \"no kernel for kernel type \" << kernel_type;\n    return kernels;\n  }\n\n  std::set<Place> expanded_places(places.begin(), places.end());\n  for (auto &place : places) {\n    // Pick kernels those support any Precision and any DataLayout, For example:\n    // kARM,kFloat,kNCHW -> kARM,kFloat,kAny; kARM,kAny,kNCHW; kARM,kAny,kAny\n    expanded_places.insert(\n        Place(place.target, place.precision, DATALAYOUT(kAny)));\n    expanded_places.insert(Place(place.target, PRECISION(kAny), place.layout));\n    expanded_places.insert(\n        Place(place.target, PRECISION(kAny), DATALAYOUT(kAny)));\n  }\n\n  std::set<TargetType> targets;\n  for (auto place : expanded_places) {\n    pick_kernel(place);\n    targets.insert(place.target);\n  }\n\n  VLOG(5) << \"op \" << op_type_ << \" get \" << kernels.size() << \" kernels\";\n  return kernels;\n}\n```\n```\n/*\n * Operator Information, such as some description. It will be shared by all the\n * kernels of the same operator.\n */\nclass OpInfo : public cpp::OpDesc {\n public:\n  OpInfo(const OpInfo &) = default;\n  explicit OpInfo(const cpp::OpDesc &other) : cpp::OpDesc(other) {}\n};\n```\n## op registry\n```\nclass OpKernelInfoCollector {\n public:\n  static OpKernelInfoCollector& Global() {\n    static auto* x = new OpKernelInfoCollector;\n    return *x;\n  }\n  void AddOp2path(const std::string& op_name, const std::string& op_path);\n  void AddKernel2path(const std::string& kernel_name,\n                      const std::string& kernel_path);\n  \n private:\n  std::map<std::string, std::string> op2path_;\n  std::map<std::string, std::string> kernel2path_;\n};\n```\n```\nclass OpLiteFactory {\n public:\n  // Register a function to create an op\n  void RegisterCreator(const std::string& op_type,\n                       std::function<std::shared_ptr<OpLite>()> fun) {\n    op_registry_[op_type] = fun;\n  }\n\n  static OpLiteFactory& Global() {\n    static OpLiteFactory* x = new OpLiteFactory;\n    return *x;\n  }\n\n  std::shared_ptr<OpLite> Create(const std::string& op_type) const {\n    auto it = op_registry_.find(op_type);\n    if (it == op_registry_.end()) return nullptr;\n    return it->second();\n  }\n\n  std::string DebugString();\n\n  std::vector<std::string> GetAllOps() const {\n    std::vector<std::string> res;\n    for (const auto& op : op_registry_) {\n      res.push_back(op.first);\n    }\n    return res;\n  }\n\n protected:\n  std::map<std::string, std::function<std::shared_ptr<OpLite>()>> op_registry_;\n};\n```\n```\nclass OpLiteRegistrar {\n public:\n  OpLiteRegistrar(const std::string& op_type,\n                  std::function<std::shared_ptr<OpLite>()> fun) {\n    OpLiteFactory::Global().RegisterCreator(op_type, fun);\n  }\n  // Touch function is used to guarantee registrar was initialized.\n  void touch() {}\n};\n\nclass KernelFactory {\n public:\n  // Register a function to create kernels\n  void RegisterCreator(const std::string& op_type,\n                       TargetType target,\n                       PrecisionType precision,\n                       DataLayoutType layout,\n                       std::function<std::unique_ptr<KernelBase>()> fun) {\n    op_registry_[op_type][std::make_tuple(target, precision, layout)].push_back(\n        fun);\n  }\n\n  static KernelFactory& Global() {\n    static KernelFactory* x = new KernelFactory;\n    return *x;\n  }\n\n  /**\n   * Create all kernels belongs to an op.\n   */\n  std::list<std::unique_ptr<KernelBase>> Create(const std::string& op_type) {\n    std::list<std::unique_ptr<KernelBase>> res;\n    if (op_registry_.find(op_type) == op_registry_.end()) return res;\n    auto& kernel_registry = op_registry_[op_type];\n    for (auto it = kernel_registry.begin(); it != kernel_registry.end(); ++it) {\n      for (auto& fun : it->second) {\n        res.emplace_back(fun());\n      }\n    }\n    return res;\n  }\n\n  /**\n   * Create a specific kernel. Return a list for API compatible.\n   */\n  std::list<std::unique_ptr<KernelBase>> Create(const std::string& op_type,\n                                                TargetType target,\n                                                PrecisionType precision,\n                                                DataLayoutType layout) {\n    std::list<std::unique_ptr<KernelBase>> res;\n    if (op_registry_.find(op_type) == op_registry_.end()) return res;\n    auto& kernel_registry = op_registry_[op_type];\n    auto it = kernel_registry.find(std::make_tuple(target, precision, layout));\n    if (it == kernel_registry.end()) return res;\n    for (auto& fun : it->second) {\n      res.emplace_back(fun());\n    }\n    return res;\n  }\n\n protected:\n  // Outer map: op -> a map of kernel.\n  // Inner map: kernel -> creator function.\n  // Each kernel was represented by a combination of <TargetType, PrecisionType,\n  // DataLayoutType>\n  std::map<std::string,\n           std::map<std::tuple<TargetType, PrecisionType, DataLayoutType>,\n                    std::list<std::function<std::unique_ptr<KernelBase>()>>>>\n      op_registry_;\n};\n\n// Register Kernel by initializing a static KernelRegistrar instance\nclass KernelRegistrar {\n public:\n  KernelRegistrar(const std::string& op_type,\n                  TargetType target,\n                  PrecisionType precision,\n                  DataLayoutType layout,\n                  std::function<std::unique_ptr<KernelBase>()> fun) {\n    KernelFactory::Global().RegisterCreator(\n        op_type, target, precision, layout, fun);\n  }\n  // Touch function is used to guarantee registrar was initialized.\n  void touch() {}\n};\n\nclass ParamTypeDummyRegistry {\n public:\n  struct NewInstance {\n    NewInstance() {}\n    NewInstance& BindInput(const std::string& arg_name,\n                           const ParamType& ptype) {\n      return *this;\n    }\n    NewInstance& BindOutput(const std::string& arg_name,\n                            const ParamType& ptype) {\n      return *this;\n    }\n    NewInstance& SetVersion(const std::string& version) { return *this; }\n    NewInstance& BindPaddleOpVersion(const std::string& op_type,\n                                     int32_t version_id) {\n      return *this;\n    }\n    bool Finalize() { return true; }\n  };\n\n private:\n  ParamTypeDummyRegistry() = default;\n};\n```\n\n# 注册机制\n## op注册\n```\n#define REGISTER_LITE_OP(op_type__, OpClass)                                   \\\n  static paddle::lite::OpLiteRegistrar op_type__##__registry(                  \\\n      #op_type__, []() {                                                       \\\n        return std::unique_ptr<paddle::lite::OpLite>(new OpClass(#op_type__)); \\\n      });                                                                      \\\n  int touch_op_##op_type__() {                                                 \\\n    op_type__##__registry.touch();                                             \\\n    OpKernelInfoCollector::Global().AddOp2path(#op_type__, __FILE__);          \\\n    return 0;                                                                  \\\n  }\n```\n## kernel 注册\n```\n// Register a kernel.\n#define REGISTER_LITE_KERNEL(                                                 \\\n    op_type__, target__, precision__, layout__, KernelClass, alias__)         \\\n  static paddle::lite::KernelRegistrar                                        \\\n      op_type__##target__##precision__##layout__##alias__##_kernel_registry(  \\\n          #op_type__,                                                         \\\n          TARGET(target__),                                                   \\\n          PRECISION(precision__),                                             \\\n          DATALAYOUT(layout__),                                               \\\n          []() {                                                              \\\n            std::unique_ptr<KernelClass> x(new KernelClass);                  \\\n            x->set_op_type(#op_type__);                                       \\\n            x->set_alias(#alias__);                                           \\\n            return x;                                                         \\\n          });                                                                 \\\n  int touch_##op_type__##target__##precision__##layout__##alias__() {         \\\n    op_type__##target__##precision__##layout__##alias__##_kernel_registry     \\\n        .touch();                                                             \\\n    OpKernelInfoCollector::Global().AddKernel2path(                           \\\n        #op_type__ \",\" #target__ \",\" #precision__ \",\" #layout__ \",\" #alias__, \\\n        __FILE__);                                                            \\\n    return 0;                                                                 \\\n  }                                                                           \\\n  ParamTypeRegistry(                                                          \\\n      op_type__, target__, precision__, layout__, KernelClass, alias__)\n```\n# program\n## scope\n```\nclass Scope final {\n public:\n  Scope()\n      : kids_lock_{new lite::fluid::RWLock},\n        vars_lock_{new lite::fluid::RWLock},\n        rwlock_{new lite::fluid::RWLock} {}\n  // delete below two functions to allow pybind to recognise it cannot make a\n  // copy\n  // link:\n  // https://stackoverflow.com/questions/53807248/pybind11-returning-a-pointer-to-a-container-of-unique-ptr\n  Scope(const Scope&) = delete;\n  Scope& operator=(const Scope&) = delete;\n  ~Scope();\n\n  Scope& NewScope() const;\n\n  Variable* Var(const std::string& name);\n\n  Variable* LocalVar(const std::string& name);\n\n  Variable* FindVar(const std::string& name) const;\n\n  Variable* FindLocalVar(const std::string& name) const;\n\n  const Scope* parent() const { return parent_; }\n\n  // Get attribute params stored in parent scopes.\n  std::vector<std::string> AttributeVarNames() const;\n  // Following the legacy scope interface.\n  std::vector<std::string> LocalVarNames() const;\n\n  /// ------------------------------------- helper functions for Tensor\n  /// ----------------------------------\n  // Create a Tensor variable. This will create a new Variable called `name`.\n  Tensor* NewTensor(const std::string& name) {\n    auto* var = Var(name);\n    return var->GetMutable<Tensor>();\n  }\n\n  const Tensor* FindTensor(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return &var->Get<Tensor>();\n  }\n\n  Tensor* FindMutableTensor(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return var->GetMutable<Tensor>();\n  }\n\n  std::vector<Tensor>* NewTensorList(const std::string& name) {\n    auto* var = Var(name);\n    return var->GetMutable<std::vector<Tensor>>();\n  }\n\n  const std::vector<Tensor>* FindTensorList(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return &var->Get<std::vector<Tensor>>();\n  }\n\n  std::vector<Tensor>* FindMutableTensorList(const std::string& name) {\n    auto* var = FindVar(name);\n    if (!var) return nullptr;\n    return var->GetMutable<std::vector<Tensor>>();\n  }\n\n private:\n  // Scope in `kids_` are owned by this class.\n  mutable std::list<Scope*> kids_;\n  const Scope* parent_{nullptr};\n  std::map<std::string, std::unique_ptr<Variable>> vars_;\n  std::unique_ptr<lite::fluid::RWLock> kids_lock_{nullptr};\n  std::unique_ptr<lite::fluid::RWLock> vars_lock_{nullptr};\n  std::unique_ptr<lite::fluid::RWLock> rwlock_{nullptr};\n};\n```\n# optimize\n## optimizer\n```\n/*\n * lite::Optimizer optimize a program. It utilize the mir passes to analysis the\n * program and export an optimized program.\n * Example :\n *       // (1) Create an optimizer\n *       Optimizer optim(valid_places, kernel_pick_factor);\n *       // (2) add an optimizer method\n *       optim.AddPass(\"post_quant_dynamic_pass\");\n *       // (3) analysis a program to export an optimized program\n *       auto program_ = optim.Run(std::move(program));\n */\nclass Optimizer {\n public:\n  Optimizer(const std::vector<Place>& valid_places,\n            core::KernelPickFactor kernel_pick_factor)\n      : valid_places_(valid_places), kernel_pick_factor_(kernel_pick_factor) {\n    CHECK(!valid_places.empty()) << \"At least one valid_place should be set\";\n  }\n\n  // Append a pass to the optimizer.\n  void AddPass(const std::string& pass_name);\n  // Optimize a program to generate a runtime program.\n  std::unique_ptr<RuntimeProgram> Run(Program&& program);\n\n protected:\n  // Run all the added passes.\n  void ApplyPasses(std::vector<std::unique_ptr<mir::SSAGraph>>* graphes);\n\n  // Generate the optimized runtime program.\n  std::unique_ptr<RuntimeProgram> GenRuntimeProgram(\n      std::vector<std::unique_ptr<mir::SSAGraph>>* graphs);\n\n  void InitTargetTypeTransformPass();\n  void InitControlFlowOpUnusedInputsAndOutputsEliminatePass();\n  void InitControlFlowOpSharedInputsAndOutputsPlaceSyncPass();\n  void SpecifyKernelPickTactic(core::KernelPickFactor factor);\n  Scope* exec_scope() { return exec_scope_; }\n\n private:\n  std::vector<Place> valid_places_;\n  Scope* exec_scope_{};\n  std::vector<mir::Pass*> passes_;\n  std::vector<std::unique_ptr<mir::SSAGraph>> graphs_;\n  core::KernelPickFactor kernel_pick_factor_;\n};\n```","slug":"article/paddle-lite","published":1,"updated":"2025-09-29T14:59:49.934Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000iotzka1mu7llp","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class KernelContext &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  template &lt;typename ContextT&gt;</span><br><span class=\"line\">  ContextT&amp; As() &#123;</span><br><span class=\"line\">    if (!ctx_.valid()) &#123;</span><br><span class=\"line\">      ctx_.set&lt;ContextT&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return *ctx_.get_mutable&lt;ContextT&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  Any ctx_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class ContextScheduler &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  static ContextScheduler&amp; Global() &#123;</span><br><span class=\"line\">    static auto* x = new ContextScheduler;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  std::unique_ptr&lt;KernelContext&gt; NewContext(</span><br><span class=\"line\">      TargetType target,</span><br><span class=\"line\">      /*only used for cuda context*/ int exec_stream_id = 0) &#123;</span><br><span class=\"line\">    std::unique_ptr&lt;KernelContext&gt; ctx(new KernelContext);</span><br><span class=\"line\">    switch (target) &#123;</span><br><span class=\"line\">      case TARGET(kHost):</span><br><span class=\"line\">        kernel_contexts_[TargetType::kHost].As&lt;HostContext&gt;().CopySharedTo(</span><br><span class=\"line\">            &amp;ctx-&gt;As&lt;HostContext&gt;());</span><br><span class=\"line\">        break;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return ctx;</span><br><span class=\"line\">  &#125; </span><br><span class=\"line\">private:</span><br><span class=\"line\">  template &lt;TargetType Type, typename ContextT&gt;</span><br><span class=\"line\">  void InitContext() &#123;</span><br><span class=\"line\">    kernel_contexts_[Type].As&lt;ContextT&gt;().InitOnce();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ContextScheduler() &#123;</span><br><span class=\"line\">    InitContext&lt;TargetType::kHost, HostContext&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">private:</span><br><span class=\"line\">  std::map&lt;TargetType, KernelContext&gt; kernel_contexts_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"op-lite\"><a href=\"#op-lite\" class=\"headerlink\" title=\"op lite\"></a>op lite</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLite : public Registry &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpLite() = default;</span><br><span class=\"line\">  explicit OpLite(const std::string &amp;type) : op_type_(type) &#123;&#125;</span><br><span class=\"line\">  explicit OpLite(const std::vector&lt;Place&gt; &amp;valid_places)</span><br><span class=\"line\">      : valid_places_(valid_places) &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  void SetValidPlaces(const std::vector&lt;Place&gt; &amp;places) &#123;</span><br><span class=\"line\">    VLOG(5) &lt;&lt; &quot;valid places &quot; &lt;&lt; valid_places_.size();</span><br><span class=\"line\">    valid_places_ = places;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  virtual bool Run();</span><br><span class=\"line\">  // Indicate whether the Op runs only once or not</span><br><span class=\"line\">  virtual bool run_once() const &#123; return false; &#125;</span><br><span class=\"line\">  std::string Type() const &#123; return op_type_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Link the external execution environ to internal context.</span><br><span class=\"line\">  bool Attach(const cpp::OpDesc &amp;opdesc, lite::Scope *scope);</span><br><span class=\"line\"></span><br><span class=\"line\">  template &lt;typename T&gt;</span><br><span class=\"line\">  inline void AttachParam(T *param) &#123;</span><br><span class=\"line\">    op_param_ = static_cast&lt;T *&gt;(param);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Create all the kernels for the valid targets.</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; CreateKernels(</span><br><span class=\"line\">      const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type = &quot;&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">  Scope *scope() &#123; return scope_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Assign op param to kernel.</span><br><span class=\"line\">  virtual void AttachKernel(KernelBase *kernel) = 0;</span><br><span class=\"line\">  void SetKernel(std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; &amp;kernels) &#123;  // NOLINT</span><br><span class=\"line\">    kernel_ = std::move(kernels.front());</span><br><span class=\"line\">    kernel_-&gt;SetContext(</span><br><span class=\"line\">        ContextScheduler::Global().NewContext(kernel_-&gt;target()));</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  KernelBase *GetKernel() &#123;  // NOLINT</span><br><span class=\"line\">    return kernel_.get();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  virtual ~OpLite() = default;</span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  friend class mir::Node;</span><br><span class=\"line\">  friend class mir::SSAGraph;</span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  Scope *scope_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;KernelBase&gt; kernel_;</span><br><span class=\"line\">  std::string op_type_;</span><br><span class=\"line\">  std::vector&lt;Place&gt; valid_places_;</span><br><span class=\"line\">  Place kernel_place_&#123;TARGET(kHost), PRECISION(kFloat)&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;OpInfo&gt; op_info_;</span><br><span class=\"line\">  // todo: it&#x27;s prefered to combine last_input_shapes and</span><br><span class=\"line\">  // last_input_lods into a single hash value to decrease</span><br><span class=\"line\">  // memory usage.</span><br><span class=\"line\">  std::vector&lt;DDimLite&gt; last_input_shapes&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_input_lods&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;DDimLite&gt; last_output_shapes&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_output_lods&#123;&#125;;</span><br><span class=\"line\">  mutable operators::ParamBase *op_param_&#123;nullptr&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  // Infer Shape according to memory, if current input shapes are consistent</span><br><span class=\"line\">  // with that of previous inputs, output shapes of last time will be reused.</span><br><span class=\"line\">  bool InferShapeWithCache();</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; OpLite::CreateKernels(</span><br><span class=\"line\">    const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type) &#123;</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; kernels;</span><br><span class=\"line\">  CHECK(!op_type_.empty()) &lt;&lt; &quot;op_type_ should be set first&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">  auto pick_kernel = [&amp;](const Place &amp;place) &#123;</span><br><span class=\"line\">    auto ks = KernelRegistry::Global().Create(</span><br><span class=\"line\">        op_type_, place.target, place.precision, place.layout);</span><br><span class=\"line\">    VLOG(5) &lt;&lt; &quot;pick kernel for &quot; &lt;&lt; op_info()-&gt;Type() &lt;&lt; &quot; &quot;</span><br><span class=\"line\">            &lt;&lt; place.DebugString() &lt;&lt; &quot; get &quot; &lt;&lt; ks.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class=\"line\">    for (auto &amp;&amp;it : ks) &#123;</span><br><span class=\"line\">      AttachKernel(it.get());</span><br><span class=\"line\">      kernels.emplace_back(std::move(it));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (!kernel_type.empty()) &#123;</span><br><span class=\"line\">    Place place;</span><br><span class=\"line\">    std::string op_type, alias;</span><br><span class=\"line\">    KernelBase::ParseKernelType(kernel_type, &amp;op_type, &amp;alias, &amp;place);</span><br><span class=\"line\">    pick_kernel(place);</span><br><span class=\"line\">    CHECK(!kernels.empty()) &lt;&lt; &quot;no kernel for kernel type &quot; &lt;&lt; kernel_type;</span><br><span class=\"line\">    return kernels;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::set&lt;Place&gt; expanded_places(places.begin(), places.end());</span><br><span class=\"line\">  for (auto &amp;place : places) &#123;</span><br><span class=\"line\">    // Pick kernels those support any Precision and any DataLayout, For example:</span><br><span class=\"line\">    // kARM,kFloat,kNCHW -&gt; kARM,kFloat,kAny; kARM,kAny,kNCHW; kARM,kAny,kAny</span><br><span class=\"line\">    expanded_places.insert(</span><br><span class=\"line\">        Place(place.target, place.precision, DATALAYOUT(kAny)));</span><br><span class=\"line\">    expanded_places.insert(Place(place.target, PRECISION(kAny), place.layout));</span><br><span class=\"line\">    expanded_places.insert(</span><br><span class=\"line\">        Place(place.target, PRECISION(kAny), DATALAYOUT(kAny)));</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::set&lt;TargetType&gt; targets;</span><br><span class=\"line\">  for (auto place : expanded_places) &#123;</span><br><span class=\"line\">    pick_kernel(place);</span><br><span class=\"line\">    targets.insert(place.target);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  VLOG(5) &lt;&lt; &quot;op &quot; &lt;&lt; op_type_ &lt;&lt; &quot; get &quot; &lt;&lt; kernels.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class=\"line\">  return kernels;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * Operator Information, such as some description. It will be shared by all the</span><br><span class=\"line\"> * kernels of the same operator.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class OpInfo : public cpp::OpDesc &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpInfo(const OpInfo &amp;) = default;</span><br><span class=\"line\">  explicit OpInfo(const cpp::OpDesc &amp;other) : cpp::OpDesc(other) &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"op-registry\"><a href=\"#op-registry\" class=\"headerlink\" title=\"op registry\"></a>op registry</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpKernelInfoCollector &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  static OpKernelInfoCollector&amp; Global() &#123;</span><br><span class=\"line\">    static auto* x = new OpKernelInfoCollector;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  void AddOp2path(const std::string&amp; op_name, const std::string&amp; op_path);</span><br><span class=\"line\">  void AddKernel2path(const std::string&amp; kernel_name,</span><br><span class=\"line\">                      const std::string&amp; kernel_path);</span><br><span class=\"line\">  </span><br><span class=\"line\"> private:</span><br><span class=\"line\">  std::map&lt;std::string, std::string&gt; op2path_;</span><br><span class=\"line\">  std::map&lt;std::string, std::string&gt; kernel2path_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLiteFactory &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  // Register a function to create an op</span><br><span class=\"line\">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class=\"line\">                       std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    op_registry_[op_type] = fun;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  static OpLiteFactory&amp; Global() &#123;</span><br><span class=\"line\">    static OpLiteFactory* x = new OpLiteFactory;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::shared_ptr&lt;OpLite&gt; Create(const std::string&amp; op_type) const &#123;</span><br><span class=\"line\">    auto it = op_registry_.find(op_type);</span><br><span class=\"line\">    if (it == op_registry_.end()) return nullptr;</span><br><span class=\"line\">    return it-&gt;second();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::string DebugString();</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;std::string&gt; GetAllOps() const &#123;</span><br><span class=\"line\">    std::vector&lt;std::string&gt; res;</span><br><span class=\"line\">    for (const auto&amp; op : op_registry_) &#123;</span><br><span class=\"line\">      res.push_back(op.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  std::map&lt;std::string, std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt;&gt; op_registry_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLiteRegistrar &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpLiteRegistrar(const std::string&amp; op_type,</span><br><span class=\"line\">                  std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    OpLiteFactory::Global().RegisterCreator(op_type, fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Touch function is used to guarantee registrar was initialized.</span><br><span class=\"line\">  void touch() &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">class KernelFactory &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  // Register a function to create kernels</span><br><span class=\"line\">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class=\"line\">                       TargetType target,</span><br><span class=\"line\">                       PrecisionType precision,</span><br><span class=\"line\">                       DataLayoutType layout,</span><br><span class=\"line\">                       std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    op_registry_[op_type][std::make_tuple(target, precision, layout)].push_back(</span><br><span class=\"line\">        fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  static KernelFactory&amp; Global() &#123;</span><br><span class=\"line\">    static KernelFactory* x = new KernelFactory;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * Create all kernels belongs to an op.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type) &#123;</span><br><span class=\"line\">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class=\"line\">    if (op_registry_.find(op_type) == op_registry_.end()) return res;</span><br><span class=\"line\">    auto&amp; kernel_registry = op_registry_[op_type];</span><br><span class=\"line\">    for (auto it = kernel_registry.begin(); it != kernel_registry.end(); ++it) &#123;</span><br><span class=\"line\">      for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class=\"line\">        res.emplace_back(fun());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * Create a specific kernel. Return a list for API compatible.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type,</span><br><span class=\"line\">                                                TargetType target,</span><br><span class=\"line\">                                                PrecisionType precision,</span><br><span class=\"line\">                                                DataLayoutType layout) &#123;</span><br><span class=\"line\">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class=\"line\">    if (op_registry_.find(op_type) == op_registry_.end()) return res;</span><br><span class=\"line\">    auto&amp; kernel_registry = op_registry_[op_type];</span><br><span class=\"line\">    auto it = kernel_registry.find(std::make_tuple(target, precision, layout));</span><br><span class=\"line\">    if (it == kernel_registry.end()) return res;</span><br><span class=\"line\">    for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class=\"line\">      res.emplace_back(fun());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  // Outer map: op -&gt; a map of kernel.</span><br><span class=\"line\">  // Inner map: kernel -&gt; creator function.</span><br><span class=\"line\">  // Each kernel was represented by a combination of &lt;TargetType, PrecisionType,</span><br><span class=\"line\">  // DataLayoutType&gt;</span><br><span class=\"line\">  std::map&lt;std::string,</span><br><span class=\"line\">           std::map&lt;std::tuple&lt;TargetType, PrecisionType, DataLayoutType&gt;,</span><br><span class=\"line\">                    std::list&lt;std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt;&gt;&gt;&gt;</span><br><span class=\"line\">      op_registry_;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">// Register Kernel by initializing a static KernelRegistrar instance</span><br><span class=\"line\">class KernelRegistrar &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  KernelRegistrar(const std::string&amp; op_type,</span><br><span class=\"line\">                  TargetType target,</span><br><span class=\"line\">                  PrecisionType precision,</span><br><span class=\"line\">                  DataLayoutType layout,</span><br><span class=\"line\">                  std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    KernelFactory::Global().RegisterCreator(</span><br><span class=\"line\">        op_type, target, precision, layout, fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Touch function is used to guarantee registrar was initialized.</span><br><span class=\"line\">  void touch() &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">class ParamTypeDummyRegistry &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  struct NewInstance &#123;</span><br><span class=\"line\">    NewInstance() &#123;&#125;</span><br><span class=\"line\">    NewInstance&amp; BindInput(const std::string&amp; arg_name,</span><br><span class=\"line\">                           const ParamType&amp; ptype) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    NewInstance&amp; BindOutput(const std::string&amp; arg_name,</span><br><span class=\"line\">                            const ParamType&amp; ptype) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    NewInstance&amp; SetVersion(const std::string&amp; version) &#123; return *this; &#125;</span><br><span class=\"line\">    NewInstance&amp; BindPaddleOpVersion(const std::string&amp; op_type,</span><br><span class=\"line\">                                     int32_t version_id) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    bool Finalize() &#123; return true; &#125;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  ParamTypeDummyRegistry() = default;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"注册机制\"><a href=\"#注册机制\" class=\"headerlink\" title=\"注册机制\"></a>注册机制</h1><h2 id=\"op注册\"><a href=\"#op注册\" class=\"headerlink\" title=\"op注册\"></a>op注册</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define REGISTER_LITE_OP(op_type__, OpClass)                                   \\</span><br><span class=\"line\">  static paddle::lite::OpLiteRegistrar op_type__##__registry(                  \\</span><br><span class=\"line\">      #op_type__, []() &#123;                                                       \\</span><br><span class=\"line\">        return std::unique_ptr&lt;paddle::lite::OpLite&gt;(new OpClass(#op_type__)); \\</span><br><span class=\"line\">      &#125;);                                                                      \\</span><br><span class=\"line\">  int touch_op_##op_type__() &#123;                                                 \\</span><br><span class=\"line\">    op_type__##__registry.touch();                                             \\</span><br><span class=\"line\">    OpKernelInfoCollector::Global().AddOp2path(#op_type__, __FILE__);          \\</span><br><span class=\"line\">    return 0;                                                                  \\</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"kernel-注册\"><a href=\"#kernel-注册\" class=\"headerlink\" title=\"kernel 注册\"></a>kernel 注册</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Register a kernel.</span><br><span class=\"line\">#define REGISTER_LITE_KERNEL(                                                 \\</span><br><span class=\"line\">    op_type__, target__, precision__, layout__, KernelClass, alias__)         \\</span><br><span class=\"line\">  static paddle::lite::KernelRegistrar                                        \\</span><br><span class=\"line\">      op_type__##target__##precision__##layout__##alias__##_kernel_registry(  \\</span><br><span class=\"line\">          #op_type__,                                                         \\</span><br><span class=\"line\">          TARGET(target__),                                                   \\</span><br><span class=\"line\">          PRECISION(precision__),                                             \\</span><br><span class=\"line\">          DATALAYOUT(layout__),                                               \\</span><br><span class=\"line\">          []() &#123;                                                              \\</span><br><span class=\"line\">            std::unique_ptr&lt;KernelClass&gt; x(new KernelClass);                  \\</span><br><span class=\"line\">            x-&gt;set_op_type(#op_type__);                                       \\</span><br><span class=\"line\">            x-&gt;set_alias(#alias__);                                           \\</span><br><span class=\"line\">            return x;                                                         \\</span><br><span class=\"line\">          &#125;);                                                                 \\</span><br><span class=\"line\">  int touch_##op_type__##target__##precision__##layout__##alias__() &#123;         \\</span><br><span class=\"line\">    op_type__##target__##precision__##layout__##alias__##_kernel_registry     \\</span><br><span class=\"line\">        .touch();                                                             \\</span><br><span class=\"line\">    OpKernelInfoCollector::Global().AddKernel2path(                           \\</span><br><span class=\"line\">        #op_type__ &quot;,&quot; #target__ &quot;,&quot; #precision__ &quot;,&quot; #layout__ &quot;,&quot; #alias__, \\</span><br><span class=\"line\">        __FILE__);                                                            \\</span><br><span class=\"line\">    return 0;                                                                 \\</span><br><span class=\"line\">  &#125;                                                                           \\</span><br><span class=\"line\">  ParamTypeRegistry(                                                          \\</span><br><span class=\"line\">      op_type__, target__, precision__, layout__, KernelClass, alias__)</span><br></pre></td></tr></table></figure>\n<h1 id=\"program\"><a href=\"#program\" class=\"headerlink\" title=\"program\"></a>program</h1><h2 id=\"scope\"><a href=\"#scope\" class=\"headerlink\" title=\"scope\"></a>scope</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Scope final &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  Scope()</span><br><span class=\"line\">      : kids_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class=\"line\">        vars_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class=\"line\">        rwlock_&#123;new lite::fluid::RWLock&#125; &#123;&#125;</span><br><span class=\"line\">  // delete below two functions to allow pybind to recognise it cannot make a</span><br><span class=\"line\">  // copy</span><br><span class=\"line\">  // link:</span><br><span class=\"line\">  // https://stackoverflow.com/questions/53807248/pybind11-returning-a-pointer-to-a-container-of-unique-ptr</span><br><span class=\"line\">  Scope(const Scope&amp;) = delete;</span><br><span class=\"line\">  Scope&amp; operator=(const Scope&amp;) = delete;</span><br><span class=\"line\">  ~Scope();</span><br><span class=\"line\"></span><br><span class=\"line\">  Scope&amp; NewScope() const;</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* Var(const std::string&amp; name);</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* LocalVar(const std::string&amp; name);</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* FindVar(const std::string&amp; name) const;</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* FindLocalVar(const std::string&amp; name) const;</span><br><span class=\"line\"></span><br><span class=\"line\">  const Scope* parent() const &#123; return parent_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Get attribute params stored in parent scopes.</span><br><span class=\"line\">  std::vector&lt;std::string&gt; AttributeVarNames() const;</span><br><span class=\"line\">  // Following the legacy scope interface.</span><br><span class=\"line\">  std::vector&lt;std::string&gt; LocalVarNames() const;</span><br><span class=\"line\"></span><br><span class=\"line\">  /// ------------------------------------- helper functions for Tensor</span><br><span class=\"line\">  /// ----------------------------------</span><br><span class=\"line\">  // Create a Tensor variable. This will create a new Variable called `name`.</span><br><span class=\"line\">  Tensor* NewTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = Var(name);</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  const Tensor* FindTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return &amp;var-&gt;Get&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  Tensor* FindMutableTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;Tensor&gt;* NewTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = Var(name);</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  const std::vector&lt;Tensor&gt;* FindTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return &amp;var-&gt;Get&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;Tensor&gt;* FindMutableTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  // Scope in `kids_` are owned by this class.</span><br><span class=\"line\">  mutable std::list&lt;Scope*&gt; kids_;</span><br><span class=\"line\">  const Scope* parent_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::map&lt;std::string, std::unique_ptr&lt;Variable&gt;&gt; vars_;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; kids_lock_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; vars_lock_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; rwlock_&#123;nullptr&#125;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"optimize\"><a href=\"#optimize\" class=\"headerlink\" title=\"optimize\"></a>optimize</h1><h2 id=\"optimizer\"><a href=\"#optimizer\" class=\"headerlink\" title=\"optimizer\"></a>optimizer</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * lite::Optimizer optimize a program. It utilize the mir passes to analysis the</span><br><span class=\"line\"> * program and export an optimized program.</span><br><span class=\"line\"> * Example :</span><br><span class=\"line\"> *       // (1) Create an optimizer</span><br><span class=\"line\"> *       Optimizer optim(valid_places, kernel_pick_factor);</span><br><span class=\"line\"> *       // (2) add an optimizer method</span><br><span class=\"line\"> *       optim.AddPass(&quot;post_quant_dynamic_pass&quot;);</span><br><span class=\"line\"> *       // (3) analysis a program to export an optimized program</span><br><span class=\"line\"> *       auto program_ = optim.Run(std::move(program));</span><br><span class=\"line\"> */</span><br><span class=\"line\">class Optimizer &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  Optimizer(const std::vector&lt;Place&gt;&amp; valid_places,</span><br><span class=\"line\">            core::KernelPickFactor kernel_pick_factor)</span><br><span class=\"line\">      : valid_places_(valid_places), kernel_pick_factor_(kernel_pick_factor) &#123;</span><br><span class=\"line\">    CHECK(!valid_places.empty()) &lt;&lt; &quot;At least one valid_place should be set&quot;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Append a pass to the optimizer.</span><br><span class=\"line\">  void AddPass(const std::string&amp; pass_name);</span><br><span class=\"line\">  // Optimize a program to generate a runtime program.</span><br><span class=\"line\">  std::unique_ptr&lt;RuntimeProgram&gt; Run(Program&amp;&amp; program);</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  // Run all the added passes.</span><br><span class=\"line\">  void ApplyPasses(std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphes);</span><br><span class=\"line\"></span><br><span class=\"line\">  // Generate the optimized runtime program.</span><br><span class=\"line\">  std::unique_ptr&lt;RuntimeProgram&gt; GenRuntimeProgram(</span><br><span class=\"line\">      std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphs);</span><br><span class=\"line\"></span><br><span class=\"line\">  void InitTargetTypeTransformPass();</span><br><span class=\"line\">  void InitControlFlowOpUnusedInputsAndOutputsEliminatePass();</span><br><span class=\"line\">  void InitControlFlowOpSharedInputsAndOutputsPlaceSyncPass();</span><br><span class=\"line\">  void SpecifyKernelPickTactic(core::KernelPickFactor factor);</span><br><span class=\"line\">  Scope* exec_scope() &#123; return exec_scope_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  std::vector&lt;Place&gt; valid_places_;</span><br><span class=\"line\">  Scope* exec_scope_&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;mir::Pass*&gt; passes_;</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt; graphs_;</span><br><span class=\"line\">  core::KernelPickFactor kernel_pick_factor_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><h2 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class KernelContext &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  template &lt;typename ContextT&gt;</span><br><span class=\"line\">  ContextT&amp; As() &#123;</span><br><span class=\"line\">    if (!ctx_.valid()) &#123;</span><br><span class=\"line\">      ctx_.set&lt;ContextT&gt;();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return *ctx_.get_mutable&lt;ContextT&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  Any ctx_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class ContextScheduler &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  static ContextScheduler&amp; Global() &#123;</span><br><span class=\"line\">    static auto* x = new ContextScheduler;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  std::unique_ptr&lt;KernelContext&gt; NewContext(</span><br><span class=\"line\">      TargetType target,</span><br><span class=\"line\">      /*only used for cuda context*/ int exec_stream_id = 0) &#123;</span><br><span class=\"line\">    std::unique_ptr&lt;KernelContext&gt; ctx(new KernelContext);</span><br><span class=\"line\">    switch (target) &#123;</span><br><span class=\"line\">      case TARGET(kHost):</span><br><span class=\"line\">        kernel_contexts_[TargetType::kHost].As&lt;HostContext&gt;().CopySharedTo(</span><br><span class=\"line\">            &amp;ctx-&gt;As&lt;HostContext&gt;());</span><br><span class=\"line\">        break;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return ctx;</span><br><span class=\"line\">  &#125; </span><br><span class=\"line\">private:</span><br><span class=\"line\">  template &lt;TargetType Type, typename ContextT&gt;</span><br><span class=\"line\">  void InitContext() &#123;</span><br><span class=\"line\">    kernel_contexts_[Type].As&lt;ContextT&gt;().InitOnce();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ContextScheduler() &#123;</span><br><span class=\"line\">    InitContext&lt;TargetType::kHost, HostContext&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">private:</span><br><span class=\"line\">  std::map&lt;TargetType, KernelContext&gt; kernel_contexts_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"op-lite\"><a href=\"#op-lite\" class=\"headerlink\" title=\"op lite\"></a>op lite</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLite : public Registry &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpLite() = default;</span><br><span class=\"line\">  explicit OpLite(const std::string &amp;type) : op_type_(type) &#123;&#125;</span><br><span class=\"line\">  explicit OpLite(const std::vector&lt;Place&gt; &amp;valid_places)</span><br><span class=\"line\">      : valid_places_(valid_places) &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  void SetValidPlaces(const std::vector&lt;Place&gt; &amp;places) &#123;</span><br><span class=\"line\">    VLOG(5) &lt;&lt; &quot;valid places &quot; &lt;&lt; valid_places_.size();</span><br><span class=\"line\">    valid_places_ = places;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  virtual bool Run();</span><br><span class=\"line\">  // Indicate whether the Op runs only once or not</span><br><span class=\"line\">  virtual bool run_once() const &#123; return false; &#125;</span><br><span class=\"line\">  std::string Type() const &#123; return op_type_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Link the external execution environ to internal context.</span><br><span class=\"line\">  bool Attach(const cpp::OpDesc &amp;opdesc, lite::Scope *scope);</span><br><span class=\"line\"></span><br><span class=\"line\">  template &lt;typename T&gt;</span><br><span class=\"line\">  inline void AttachParam(T *param) &#123;</span><br><span class=\"line\">    op_param_ = static_cast&lt;T *&gt;(param);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Create all the kernels for the valid targets.</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; CreateKernels(</span><br><span class=\"line\">      const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type = &quot;&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">  Scope *scope() &#123; return scope_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Assign op param to kernel.</span><br><span class=\"line\">  virtual void AttachKernel(KernelBase *kernel) = 0;</span><br><span class=\"line\">  void SetKernel(std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; &amp;kernels) &#123;  // NOLINT</span><br><span class=\"line\">    kernel_ = std::move(kernels.front());</span><br><span class=\"line\">    kernel_-&gt;SetContext(</span><br><span class=\"line\">        ContextScheduler::Global().NewContext(kernel_-&gt;target()));</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  KernelBase *GetKernel() &#123;  // NOLINT</span><br><span class=\"line\">    return kernel_.get();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  virtual ~OpLite() = default;</span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  friend class mir::Node;</span><br><span class=\"line\">  friend class mir::SSAGraph;</span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  Scope *scope_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;KernelBase&gt; kernel_;</span><br><span class=\"line\">  std::string op_type_;</span><br><span class=\"line\">  std::vector&lt;Place&gt; valid_places_;</span><br><span class=\"line\">  Place kernel_place_&#123;TARGET(kHost), PRECISION(kFloat)&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;OpInfo&gt; op_info_;</span><br><span class=\"line\">  // todo: it&#x27;s prefered to combine last_input_shapes and</span><br><span class=\"line\">  // last_input_lods into a single hash value to decrease</span><br><span class=\"line\">  // memory usage.</span><br><span class=\"line\">  std::vector&lt;DDimLite&gt; last_input_shapes&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_input_lods&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;DDimLite&gt; last_output_shapes&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;std::vector&lt;std::vector&lt;uint64_t&gt;&gt;&gt; last_output_lods&#123;&#125;;</span><br><span class=\"line\">  mutable operators::ParamBase *op_param_&#123;nullptr&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  // Infer Shape according to memory, if current input shapes are consistent</span><br><span class=\"line\">  // with that of previous inputs, output shapes of last time will be reused.</span><br><span class=\"line\">  bool InferShapeWithCache();</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; OpLite::CreateKernels(</span><br><span class=\"line\">    const std::vector&lt;Place&gt; &amp;places, const std::string &amp;kernel_type) &#123;</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;KernelBase&gt;&gt; kernels;</span><br><span class=\"line\">  CHECK(!op_type_.empty()) &lt;&lt; &quot;op_type_ should be set first&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">  auto pick_kernel = [&amp;](const Place &amp;place) &#123;</span><br><span class=\"line\">    auto ks = KernelRegistry::Global().Create(</span><br><span class=\"line\">        op_type_, place.target, place.precision, place.layout);</span><br><span class=\"line\">    VLOG(5) &lt;&lt; &quot;pick kernel for &quot; &lt;&lt; op_info()-&gt;Type() &lt;&lt; &quot; &quot;</span><br><span class=\"line\">            &lt;&lt; place.DebugString() &lt;&lt; &quot; get &quot; &lt;&lt; ks.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class=\"line\">    for (auto &amp;&amp;it : ks) &#123;</span><br><span class=\"line\">      AttachKernel(it.get());</span><br><span class=\"line\">      kernels.emplace_back(std::move(it));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  if (!kernel_type.empty()) &#123;</span><br><span class=\"line\">    Place place;</span><br><span class=\"line\">    std::string op_type, alias;</span><br><span class=\"line\">    KernelBase::ParseKernelType(kernel_type, &amp;op_type, &amp;alias, &amp;place);</span><br><span class=\"line\">    pick_kernel(place);</span><br><span class=\"line\">    CHECK(!kernels.empty()) &lt;&lt; &quot;no kernel for kernel type &quot; &lt;&lt; kernel_type;</span><br><span class=\"line\">    return kernels;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::set&lt;Place&gt; expanded_places(places.begin(), places.end());</span><br><span class=\"line\">  for (auto &amp;place : places) &#123;</span><br><span class=\"line\">    // Pick kernels those support any Precision and any DataLayout, For example:</span><br><span class=\"line\">    // kARM,kFloat,kNCHW -&gt; kARM,kFloat,kAny; kARM,kAny,kNCHW; kARM,kAny,kAny</span><br><span class=\"line\">    expanded_places.insert(</span><br><span class=\"line\">        Place(place.target, place.precision, DATALAYOUT(kAny)));</span><br><span class=\"line\">    expanded_places.insert(Place(place.target, PRECISION(kAny), place.layout));</span><br><span class=\"line\">    expanded_places.insert(</span><br><span class=\"line\">        Place(place.target, PRECISION(kAny), DATALAYOUT(kAny)));</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::set&lt;TargetType&gt; targets;</span><br><span class=\"line\">  for (auto place : expanded_places) &#123;</span><br><span class=\"line\">    pick_kernel(place);</span><br><span class=\"line\">    targets.insert(place.target);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  VLOG(5) &lt;&lt; &quot;op &quot; &lt;&lt; op_type_ &lt;&lt; &quot; get &quot; &lt;&lt; kernels.size() &lt;&lt; &quot; kernels&quot;;</span><br><span class=\"line\">  return kernels;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * Operator Information, such as some description. It will be shared by all the</span><br><span class=\"line\"> * kernels of the same operator.</span><br><span class=\"line\"> */</span><br><span class=\"line\">class OpInfo : public cpp::OpDesc &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpInfo(const OpInfo &amp;) = default;</span><br><span class=\"line\">  explicit OpInfo(const cpp::OpDesc &amp;other) : cpp::OpDesc(other) &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h2 id=\"op-registry\"><a href=\"#op-registry\" class=\"headerlink\" title=\"op registry\"></a>op registry</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpKernelInfoCollector &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  static OpKernelInfoCollector&amp; Global() &#123;</span><br><span class=\"line\">    static auto* x = new OpKernelInfoCollector;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  void AddOp2path(const std::string&amp; op_name, const std::string&amp; op_path);</span><br><span class=\"line\">  void AddKernel2path(const std::string&amp; kernel_name,</span><br><span class=\"line\">                      const std::string&amp; kernel_path);</span><br><span class=\"line\">  </span><br><span class=\"line\"> private:</span><br><span class=\"line\">  std::map&lt;std::string, std::string&gt; op2path_;</span><br><span class=\"line\">  std::map&lt;std::string, std::string&gt; kernel2path_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLiteFactory &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  // Register a function to create an op</span><br><span class=\"line\">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class=\"line\">                       std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    op_registry_[op_type] = fun;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  static OpLiteFactory&amp; Global() &#123;</span><br><span class=\"line\">    static OpLiteFactory* x = new OpLiteFactory;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::shared_ptr&lt;OpLite&gt; Create(const std::string&amp; op_type) const &#123;</span><br><span class=\"line\">    auto it = op_registry_.find(op_type);</span><br><span class=\"line\">    if (it == op_registry_.end()) return nullptr;</span><br><span class=\"line\">    return it-&gt;second();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::string DebugString();</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;std::string&gt; GetAllOps() const &#123;</span><br><span class=\"line\">    std::vector&lt;std::string&gt; res;</span><br><span class=\"line\">    for (const auto&amp; op : op_registry_) &#123;</span><br><span class=\"line\">      res.push_back(op.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  std::map&lt;std::string, std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt;&gt; op_registry_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class OpLiteRegistrar &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  OpLiteRegistrar(const std::string&amp; op_type,</span><br><span class=\"line\">                  std::function&lt;std::shared_ptr&lt;OpLite&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    OpLiteFactory::Global().RegisterCreator(op_type, fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Touch function is used to guarantee registrar was initialized.</span><br><span class=\"line\">  void touch() &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">class KernelFactory &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  // Register a function to create kernels</span><br><span class=\"line\">  void RegisterCreator(const std::string&amp; op_type,</span><br><span class=\"line\">                       TargetType target,</span><br><span class=\"line\">                       PrecisionType precision,</span><br><span class=\"line\">                       DataLayoutType layout,</span><br><span class=\"line\">                       std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    op_registry_[op_type][std::make_tuple(target, precision, layout)].push_back(</span><br><span class=\"line\">        fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  static KernelFactory&amp; Global() &#123;</span><br><span class=\"line\">    static KernelFactory* x = new KernelFactory;</span><br><span class=\"line\">    return *x;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * Create all kernels belongs to an op.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type) &#123;</span><br><span class=\"line\">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class=\"line\">    if (op_registry_.find(op_type) == op_registry_.end()) return res;</span><br><span class=\"line\">    auto&amp; kernel_registry = op_registry_[op_type];</span><br><span class=\"line\">    for (auto it = kernel_registry.begin(); it != kernel_registry.end(); ++it) &#123;</span><br><span class=\"line\">      for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class=\"line\">        res.emplace_back(fun());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * Create a specific kernel. Return a list for API compatible.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; Create(const std::string&amp; op_type,</span><br><span class=\"line\">                                                TargetType target,</span><br><span class=\"line\">                                                PrecisionType precision,</span><br><span class=\"line\">                                                DataLayoutType layout) &#123;</span><br><span class=\"line\">    std::list&lt;std::unique_ptr&lt;KernelBase&gt;&gt; res;</span><br><span class=\"line\">    if (op_registry_.find(op_type) == op_registry_.end()) return res;</span><br><span class=\"line\">    auto&amp; kernel_registry = op_registry_[op_type];</span><br><span class=\"line\">    auto it = kernel_registry.find(std::make_tuple(target, precision, layout));</span><br><span class=\"line\">    if (it == kernel_registry.end()) return res;</span><br><span class=\"line\">    for (auto&amp; fun : it-&gt;second) &#123;</span><br><span class=\"line\">      res.emplace_back(fun());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  // Outer map: op -&gt; a map of kernel.</span><br><span class=\"line\">  // Inner map: kernel -&gt; creator function.</span><br><span class=\"line\">  // Each kernel was represented by a combination of &lt;TargetType, PrecisionType,</span><br><span class=\"line\">  // DataLayoutType&gt;</span><br><span class=\"line\">  std::map&lt;std::string,</span><br><span class=\"line\">           std::map&lt;std::tuple&lt;TargetType, PrecisionType, DataLayoutType&gt;,</span><br><span class=\"line\">                    std::list&lt;std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt;&gt;&gt;&gt;</span><br><span class=\"line\">      op_registry_;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">// Register Kernel by initializing a static KernelRegistrar instance</span><br><span class=\"line\">class KernelRegistrar &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  KernelRegistrar(const std::string&amp; op_type,</span><br><span class=\"line\">                  TargetType target,</span><br><span class=\"line\">                  PrecisionType precision,</span><br><span class=\"line\">                  DataLayoutType layout,</span><br><span class=\"line\">                  std::function&lt;std::unique_ptr&lt;KernelBase&gt;()&gt; fun) &#123;</span><br><span class=\"line\">    KernelFactory::Global().RegisterCreator(</span><br><span class=\"line\">        op_type, target, precision, layout, fun);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // Touch function is used to guarantee registrar was initialized.</span><br><span class=\"line\">  void touch() &#123;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">class ParamTypeDummyRegistry &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  struct NewInstance &#123;</span><br><span class=\"line\">    NewInstance() &#123;&#125;</span><br><span class=\"line\">    NewInstance&amp; BindInput(const std::string&amp; arg_name,</span><br><span class=\"line\">                           const ParamType&amp; ptype) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    NewInstance&amp; BindOutput(const std::string&amp; arg_name,</span><br><span class=\"line\">                            const ParamType&amp; ptype) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    NewInstance&amp; SetVersion(const std::string&amp; version) &#123; return *this; &#125;</span><br><span class=\"line\">    NewInstance&amp; BindPaddleOpVersion(const std::string&amp; op_type,</span><br><span class=\"line\">                                     int32_t version_id) &#123;</span><br><span class=\"line\">      return *this;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    bool Finalize() &#123; return true; &#125;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  ParamTypeDummyRegistry() = default;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"注册机制\"><a href=\"#注册机制\" class=\"headerlink\" title=\"注册机制\"></a>注册机制</h1><h2 id=\"op注册\"><a href=\"#op注册\" class=\"headerlink\" title=\"op注册\"></a>op注册</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define REGISTER_LITE_OP(op_type__, OpClass)                                   \\</span><br><span class=\"line\">  static paddle::lite::OpLiteRegistrar op_type__##__registry(                  \\</span><br><span class=\"line\">      #op_type__, []() &#123;                                                       \\</span><br><span class=\"line\">        return std::unique_ptr&lt;paddle::lite::OpLite&gt;(new OpClass(#op_type__)); \\</span><br><span class=\"line\">      &#125;);                                                                      \\</span><br><span class=\"line\">  int touch_op_##op_type__() &#123;                                                 \\</span><br><span class=\"line\">    op_type__##__registry.touch();                                             \\</span><br><span class=\"line\">    OpKernelInfoCollector::Global().AddOp2path(#op_type__, __FILE__);          \\</span><br><span class=\"line\">    return 0;                                                                  \\</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"kernel-注册\"><a href=\"#kernel-注册\" class=\"headerlink\" title=\"kernel 注册\"></a>kernel 注册</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Register a kernel.</span><br><span class=\"line\">#define REGISTER_LITE_KERNEL(                                                 \\</span><br><span class=\"line\">    op_type__, target__, precision__, layout__, KernelClass, alias__)         \\</span><br><span class=\"line\">  static paddle::lite::KernelRegistrar                                        \\</span><br><span class=\"line\">      op_type__##target__##precision__##layout__##alias__##_kernel_registry(  \\</span><br><span class=\"line\">          #op_type__,                                                         \\</span><br><span class=\"line\">          TARGET(target__),                                                   \\</span><br><span class=\"line\">          PRECISION(precision__),                                             \\</span><br><span class=\"line\">          DATALAYOUT(layout__),                                               \\</span><br><span class=\"line\">          []() &#123;                                                              \\</span><br><span class=\"line\">            std::unique_ptr&lt;KernelClass&gt; x(new KernelClass);                  \\</span><br><span class=\"line\">            x-&gt;set_op_type(#op_type__);                                       \\</span><br><span class=\"line\">            x-&gt;set_alias(#alias__);                                           \\</span><br><span class=\"line\">            return x;                                                         \\</span><br><span class=\"line\">          &#125;);                                                                 \\</span><br><span class=\"line\">  int touch_##op_type__##target__##precision__##layout__##alias__() &#123;         \\</span><br><span class=\"line\">    op_type__##target__##precision__##layout__##alias__##_kernel_registry     \\</span><br><span class=\"line\">        .touch();                                                             \\</span><br><span class=\"line\">    OpKernelInfoCollector::Global().AddKernel2path(                           \\</span><br><span class=\"line\">        #op_type__ &quot;,&quot; #target__ &quot;,&quot; #precision__ &quot;,&quot; #layout__ &quot;,&quot; #alias__, \\</span><br><span class=\"line\">        __FILE__);                                                            \\</span><br><span class=\"line\">    return 0;                                                                 \\</span><br><span class=\"line\">  &#125;                                                                           \\</span><br><span class=\"line\">  ParamTypeRegistry(                                                          \\</span><br><span class=\"line\">      op_type__, target__, precision__, layout__, KernelClass, alias__)</span><br></pre></td></tr></table></figure>\n<h1 id=\"program\"><a href=\"#program\" class=\"headerlink\" title=\"program\"></a>program</h1><h2 id=\"scope\"><a href=\"#scope\" class=\"headerlink\" title=\"scope\"></a>scope</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Scope final &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  Scope()</span><br><span class=\"line\">      : kids_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class=\"line\">        vars_lock_&#123;new lite::fluid::RWLock&#125;,</span><br><span class=\"line\">        rwlock_&#123;new lite::fluid::RWLock&#125; &#123;&#125;</span><br><span class=\"line\">  // delete below two functions to allow pybind to recognise it cannot make a</span><br><span class=\"line\">  // copy</span><br><span class=\"line\">  // link:</span><br><span class=\"line\">  // https://stackoverflow.com/questions/53807248/pybind11-returning-a-pointer-to-a-container-of-unique-ptr</span><br><span class=\"line\">  Scope(const Scope&amp;) = delete;</span><br><span class=\"line\">  Scope&amp; operator=(const Scope&amp;) = delete;</span><br><span class=\"line\">  ~Scope();</span><br><span class=\"line\"></span><br><span class=\"line\">  Scope&amp; NewScope() const;</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* Var(const std::string&amp; name);</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* LocalVar(const std::string&amp; name);</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* FindVar(const std::string&amp; name) const;</span><br><span class=\"line\"></span><br><span class=\"line\">  Variable* FindLocalVar(const std::string&amp; name) const;</span><br><span class=\"line\"></span><br><span class=\"line\">  const Scope* parent() const &#123; return parent_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Get attribute params stored in parent scopes.</span><br><span class=\"line\">  std::vector&lt;std::string&gt; AttributeVarNames() const;</span><br><span class=\"line\">  // Following the legacy scope interface.</span><br><span class=\"line\">  std::vector&lt;std::string&gt; LocalVarNames() const;</span><br><span class=\"line\"></span><br><span class=\"line\">  /// ------------------------------------- helper functions for Tensor</span><br><span class=\"line\">  /// ----------------------------------</span><br><span class=\"line\">  // Create a Tensor variable. This will create a new Variable called `name`.</span><br><span class=\"line\">  Tensor* NewTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = Var(name);</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  const Tensor* FindTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return &amp;var-&gt;Get&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  Tensor* FindMutableTensor(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;Tensor&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;Tensor&gt;* NewTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = Var(name);</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  const std::vector&lt;Tensor&gt;* FindTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return &amp;var-&gt;Get&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;Tensor&gt;* FindMutableTensorList(const std::string&amp; name) &#123;</span><br><span class=\"line\">    auto* var = FindVar(name);</span><br><span class=\"line\">    if (!var) return nullptr;</span><br><span class=\"line\">    return var-&gt;GetMutable&lt;std::vector&lt;Tensor&gt;&gt;();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  // Scope in `kids_` are owned by this class.</span><br><span class=\"line\">  mutable std::list&lt;Scope*&gt; kids_;</span><br><span class=\"line\">  const Scope* parent_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::map&lt;std::string, std::unique_ptr&lt;Variable&gt;&gt; vars_;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; kids_lock_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; vars_lock_&#123;nullptr&#125;;</span><br><span class=\"line\">  std::unique_ptr&lt;lite::fluid::RWLock&gt; rwlock_&#123;nullptr&#125;;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"optimize\"><a href=\"#optimize\" class=\"headerlink\" title=\"optimize\"></a>optimize</h1><h2 id=\"optimizer\"><a href=\"#optimizer\" class=\"headerlink\" title=\"optimizer\"></a>optimizer</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/*</span><br><span class=\"line\"> * lite::Optimizer optimize a program. It utilize the mir passes to analysis the</span><br><span class=\"line\"> * program and export an optimized program.</span><br><span class=\"line\"> * Example :</span><br><span class=\"line\"> *       // (1) Create an optimizer</span><br><span class=\"line\"> *       Optimizer optim(valid_places, kernel_pick_factor);</span><br><span class=\"line\"> *       // (2) add an optimizer method</span><br><span class=\"line\"> *       optim.AddPass(&quot;post_quant_dynamic_pass&quot;);</span><br><span class=\"line\"> *       // (3) analysis a program to export an optimized program</span><br><span class=\"line\"> *       auto program_ = optim.Run(std::move(program));</span><br><span class=\"line\"> */</span><br><span class=\"line\">class Optimizer &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  Optimizer(const std::vector&lt;Place&gt;&amp; valid_places,</span><br><span class=\"line\">            core::KernelPickFactor kernel_pick_factor)</span><br><span class=\"line\">      : valid_places_(valid_places), kernel_pick_factor_(kernel_pick_factor) &#123;</span><br><span class=\"line\">    CHECK(!valid_places.empty()) &lt;&lt; &quot;At least one valid_place should be set&quot;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  // Append a pass to the optimizer.</span><br><span class=\"line\">  void AddPass(const std::string&amp; pass_name);</span><br><span class=\"line\">  // Optimize a program to generate a runtime program.</span><br><span class=\"line\">  std::unique_ptr&lt;RuntimeProgram&gt; Run(Program&amp;&amp; program);</span><br><span class=\"line\"></span><br><span class=\"line\"> protected:</span><br><span class=\"line\">  // Run all the added passes.</span><br><span class=\"line\">  void ApplyPasses(std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphes);</span><br><span class=\"line\"></span><br><span class=\"line\">  // Generate the optimized runtime program.</span><br><span class=\"line\">  std::unique_ptr&lt;RuntimeProgram&gt; GenRuntimeProgram(</span><br><span class=\"line\">      std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt;* graphs);</span><br><span class=\"line\"></span><br><span class=\"line\">  void InitTargetTypeTransformPass();</span><br><span class=\"line\">  void InitControlFlowOpUnusedInputsAndOutputsEliminatePass();</span><br><span class=\"line\">  void InitControlFlowOpSharedInputsAndOutputsPlaceSyncPass();</span><br><span class=\"line\">  void SpecifyKernelPickTactic(core::KernelPickFactor factor);</span><br><span class=\"line\">  Scope* exec_scope() &#123; return exec_scope_; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  std::vector&lt;Place&gt; valid_places_;</span><br><span class=\"line\">  Scope* exec_scope_&#123;&#125;;</span><br><span class=\"line\">  std::vector&lt;mir::Pass*&gt; passes_;</span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;mir::SSAGraph&gt;&gt; graphs_;</span><br><span class=\"line\">  core::KernelPickFactor kernel_pick_factor_;</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>"},{"title":"paddle inference","date":"2021-03-15T19:32:13.000Z","_content":"paddle inference学习记录\n\n# 代码解析\npaddle inference代码位于paddle/fluid/inference下面。\n\n## engine基类\n```\nclass EngineBase {\n public:\n  using DescType = ::paddle::framework::proto::BlockDesc;\n  // Build the model and do some preparation, for example, in TensorRT, run\n  // createInferBuilder, buildCudaEngine.\n  virtual void Build(const DescType& paddle_model) = 0;\n  // Execute the engine, that will run the inference network.\n  virtual void Execute(int batch_size) = 0;\n  virtual ~EngineBase() {}\n}; \n```\n\n## 待添加\nframework::ProgramDesc\nframework::Executor* executor\nframework::Scope* scope\n\n# paddle inference 执行逻辑\n```\npaddle/fluid/framework/naive_executor.cc#L41\n```\n```\nvoid NaiveExecutor::Run() {\n#ifdef PADDLE_WITH_MKLDNN\n  platform::AttachPointerHashToMKLDNNKey(this, place_);\n  platform::RegisterModelLayout(ops_, place_);\n#endif\n  platform::ScopedFlushDenormal flush;\n  for (auto &op : ops_) {\n    VLOG(4) << std::this_thread::get_id() << \" run \"\n            << op->DebugStringEx(scope_) << \" on scope \" << scope_;\n    op->SetIsCalledByExecutor(false);\n    op->Run(*scope_, place_);\n  }\n}\n```\n\n```\npaddle/fluid/framework/operator.cc#L204\n```\n```\nvoid OperatorBase::Run(const Scope& scope, const platform::Place& place) {\n  auto dev_id = place.device;\n  platform::SetDeviceId(dev_id);\n  auto op_name = platform::OpName(outputs_, Type());\n  RunImpl(scope, place);\n}\n```\n```\nvoid OperatorWithKernel::RunImpl(const Scope& scope,\n                                 const platform::Place& place,\n                                 RuntimeContext* runtime_ctx) const {\n  platform::DeviceContextPool& pool = platform::DeviceContextPool::Instance();\n  auto* dev_ctx = pool.Get(place);\n  auto exe_ctx = ExecutionContext(*this, scope, *dev_ctx, *runtime_ctx);\n  // using cache\n  if (kernel_type_.get()) {\n    dev_ctx = pool.Get(kernel_type_->place_);\n  }\n  {\n    impl_ =\n        new CacheImpl(new phi::KernelContext(),\n                          new RuntimeInferShapeContext(*this, *runtime_ctx));\n    BuildPhiKernelContext(*runtime_ctx, dev_ctx, impl_->getKernelContext());\n\n    (*pt_kernel_)(impl_->getKernelContext());\n  }\n}\n```","source":"_posts/article/paddle_inference.md","raw":"---\ntitle: paddle inference\ndate: 2021-03-16 03:32:13\ntags:\n---\npaddle inference学习记录\n\n# 代码解析\npaddle inference代码位于paddle/fluid/inference下面。\n\n## engine基类\n```\nclass EngineBase {\n public:\n  using DescType = ::paddle::framework::proto::BlockDesc;\n  // Build the model and do some preparation, for example, in TensorRT, run\n  // createInferBuilder, buildCudaEngine.\n  virtual void Build(const DescType& paddle_model) = 0;\n  // Execute the engine, that will run the inference network.\n  virtual void Execute(int batch_size) = 0;\n  virtual ~EngineBase() {}\n}; \n```\n\n## 待添加\nframework::ProgramDesc\nframework::Executor* executor\nframework::Scope* scope\n\n# paddle inference 执行逻辑\n```\npaddle/fluid/framework/naive_executor.cc#L41\n```\n```\nvoid NaiveExecutor::Run() {\n#ifdef PADDLE_WITH_MKLDNN\n  platform::AttachPointerHashToMKLDNNKey(this, place_);\n  platform::RegisterModelLayout(ops_, place_);\n#endif\n  platform::ScopedFlushDenormal flush;\n  for (auto &op : ops_) {\n    VLOG(4) << std::this_thread::get_id() << \" run \"\n            << op->DebugStringEx(scope_) << \" on scope \" << scope_;\n    op->SetIsCalledByExecutor(false);\n    op->Run(*scope_, place_);\n  }\n}\n```\n\n```\npaddle/fluid/framework/operator.cc#L204\n```\n```\nvoid OperatorBase::Run(const Scope& scope, const platform::Place& place) {\n  auto dev_id = place.device;\n  platform::SetDeviceId(dev_id);\n  auto op_name = platform::OpName(outputs_, Type());\n  RunImpl(scope, place);\n}\n```\n```\nvoid OperatorWithKernel::RunImpl(const Scope& scope,\n                                 const platform::Place& place,\n                                 RuntimeContext* runtime_ctx) const {\n  platform::DeviceContextPool& pool = platform::DeviceContextPool::Instance();\n  auto* dev_ctx = pool.Get(place);\n  auto exe_ctx = ExecutionContext(*this, scope, *dev_ctx, *runtime_ctx);\n  // using cache\n  if (kernel_type_.get()) {\n    dev_ctx = pool.Get(kernel_type_->place_);\n  }\n  {\n    impl_ =\n        new CacheImpl(new phi::KernelContext(),\n                          new RuntimeInferShapeContext(*this, *runtime_ctx));\n    BuildPhiKernelContext(*runtime_ctx, dev_ctx, impl_->getKernelContext());\n\n    (*pt_kernel_)(impl_->getKernelContext());\n  }\n}\n```","slug":"article/paddle_inference","published":1,"updated":"2025-09-29T14:59:49.935Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000jotzk2qnvg5ps","content":"<p>paddle inference学习记录</p>\n<h1 id=\"代码解析\"><a href=\"#代码解析\" class=\"headerlink\" title=\"代码解析\"></a>代码解析</h1><p>paddle inference代码位于paddle&#x2F;fluid&#x2F;inference下面。</p>\n<h2 id=\"engine基类\"><a href=\"#engine基类\" class=\"headerlink\" title=\"engine基类\"></a>engine基类</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class EngineBase &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  using DescType = ::paddle::framework::proto::BlockDesc;</span><br><span class=\"line\">  // Build the model and do some preparation, for example, in TensorRT, run</span><br><span class=\"line\">  // createInferBuilder, buildCudaEngine.</span><br><span class=\"line\">  virtual void Build(const DescType&amp; paddle_model) = 0;</span><br><span class=\"line\">  // Execute the engine, that will run the inference network.</span><br><span class=\"line\">  virtual void Execute(int batch_size) = 0;</span><br><span class=\"line\">  virtual ~EngineBase() &#123;&#125;</span><br><span class=\"line\">&#125;; </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"待添加\"><a href=\"#待添加\" class=\"headerlink\" title=\"待添加\"></a>待添加</h2><p>framework::ProgramDesc<br>framework::Executor* executor<br>framework::Scope* scope</p>\n<h1 id=\"paddle-inference-执行逻辑\"><a href=\"#paddle-inference-执行逻辑\" class=\"headerlink\" title=\"paddle inference 执行逻辑\"></a>paddle inference 执行逻辑</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">paddle/fluid/framework/naive_executor.cc#L41</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void NaiveExecutor::Run() &#123;</span><br><span class=\"line\">#ifdef PADDLE_WITH_MKLDNN</span><br><span class=\"line\">  platform::AttachPointerHashToMKLDNNKey(this, place_);</span><br><span class=\"line\">  platform::RegisterModelLayout(ops_, place_);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  platform::ScopedFlushDenormal flush;</span><br><span class=\"line\">  for (auto &amp;op : ops_) &#123;</span><br><span class=\"line\">    VLOG(4) &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot; run &quot;</span><br><span class=\"line\">            &lt;&lt; op-&gt;DebugStringEx(scope_) &lt;&lt; &quot; on scope &quot; &lt;&lt; scope_;</span><br><span class=\"line\">    op-&gt;SetIsCalledByExecutor(false);</span><br><span class=\"line\">    op-&gt;Run(*scope_, place_);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">paddle/fluid/framework/operator.cc#L204</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void OperatorBase::Run(const Scope&amp; scope, const platform::Place&amp; place) &#123;</span><br><span class=\"line\">  auto dev_id = place.device;</span><br><span class=\"line\">  platform::SetDeviceId(dev_id);</span><br><span class=\"line\">  auto op_name = platform::OpName(outputs_, Type());</span><br><span class=\"line\">  RunImpl(scope, place);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void OperatorWithKernel::RunImpl(const Scope&amp; scope,</span><br><span class=\"line\">                                 const platform::Place&amp; place,</span><br><span class=\"line\">                                 RuntimeContext* runtime_ctx) const &#123;</span><br><span class=\"line\">  platform::DeviceContextPool&amp; pool = platform::DeviceContextPool::Instance();</span><br><span class=\"line\">  auto* dev_ctx = pool.Get(place);</span><br><span class=\"line\">  auto exe_ctx = ExecutionContext(*this, scope, *dev_ctx, *runtime_ctx);</span><br><span class=\"line\">  // using cache</span><br><span class=\"line\">  if (kernel_type_.get()) &#123;</span><br><span class=\"line\">    dev_ctx = pool.Get(kernel_type_-&gt;place_);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    impl_ =</span><br><span class=\"line\">        new CacheImpl(new phi::KernelContext(),</span><br><span class=\"line\">                          new RuntimeInferShapeContext(*this, *runtime_ctx));</span><br><span class=\"line\">    BuildPhiKernelContext(*runtime_ctx, dev_ctx, impl_-&gt;getKernelContext());</span><br><span class=\"line\"></span><br><span class=\"line\">    (*pt_kernel_)(impl_-&gt;getKernelContext());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<p>paddle inference学习记录</p>\n<h1 id=\"代码解析\"><a href=\"#代码解析\" class=\"headerlink\" title=\"代码解析\"></a>代码解析</h1><p>paddle inference代码位于paddle&#x2F;fluid&#x2F;inference下面。</p>\n<h2 id=\"engine基类\"><a href=\"#engine基类\" class=\"headerlink\" title=\"engine基类\"></a>engine基类</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class EngineBase &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  using DescType = ::paddle::framework::proto::BlockDesc;</span><br><span class=\"line\">  // Build the model and do some preparation, for example, in TensorRT, run</span><br><span class=\"line\">  // createInferBuilder, buildCudaEngine.</span><br><span class=\"line\">  virtual void Build(const DescType&amp; paddle_model) = 0;</span><br><span class=\"line\">  // Execute the engine, that will run the inference network.</span><br><span class=\"line\">  virtual void Execute(int batch_size) = 0;</span><br><span class=\"line\">  virtual ~EngineBase() &#123;&#125;</span><br><span class=\"line\">&#125;; </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"待添加\"><a href=\"#待添加\" class=\"headerlink\" title=\"待添加\"></a>待添加</h2><p>framework::ProgramDesc<br>framework::Executor* executor<br>framework::Scope* scope</p>\n<h1 id=\"paddle-inference-执行逻辑\"><a href=\"#paddle-inference-执行逻辑\" class=\"headerlink\" title=\"paddle inference 执行逻辑\"></a>paddle inference 执行逻辑</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">paddle/fluid/framework/naive_executor.cc#L41</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void NaiveExecutor::Run() &#123;</span><br><span class=\"line\">#ifdef PADDLE_WITH_MKLDNN</span><br><span class=\"line\">  platform::AttachPointerHashToMKLDNNKey(this, place_);</span><br><span class=\"line\">  platform::RegisterModelLayout(ops_, place_);</span><br><span class=\"line\">#endif</span><br><span class=\"line\">  platform::ScopedFlushDenormal flush;</span><br><span class=\"line\">  for (auto &amp;op : ops_) &#123;</span><br><span class=\"line\">    VLOG(4) &lt;&lt; std::this_thread::get_id() &lt;&lt; &quot; run &quot;</span><br><span class=\"line\">            &lt;&lt; op-&gt;DebugStringEx(scope_) &lt;&lt; &quot; on scope &quot; &lt;&lt; scope_;</span><br><span class=\"line\">    op-&gt;SetIsCalledByExecutor(false);</span><br><span class=\"line\">    op-&gt;Run(*scope_, place_);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">paddle/fluid/framework/operator.cc#L204</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void OperatorBase::Run(const Scope&amp; scope, const platform::Place&amp; place) &#123;</span><br><span class=\"line\">  auto dev_id = place.device;</span><br><span class=\"line\">  platform::SetDeviceId(dev_id);</span><br><span class=\"line\">  auto op_name = platform::OpName(outputs_, Type());</span><br><span class=\"line\">  RunImpl(scope, place);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void OperatorWithKernel::RunImpl(const Scope&amp; scope,</span><br><span class=\"line\">                                 const platform::Place&amp; place,</span><br><span class=\"line\">                                 RuntimeContext* runtime_ctx) const &#123;</span><br><span class=\"line\">  platform::DeviceContextPool&amp; pool = platform::DeviceContextPool::Instance();</span><br><span class=\"line\">  auto* dev_ctx = pool.Get(place);</span><br><span class=\"line\">  auto exe_ctx = ExecutionContext(*this, scope, *dev_ctx, *runtime_ctx);</span><br><span class=\"line\">  // using cache</span><br><span class=\"line\">  if (kernel_type_.get()) &#123;</span><br><span class=\"line\">    dev_ctx = pool.Get(kernel_type_-&gt;place_);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    impl_ =</span><br><span class=\"line\">        new CacheImpl(new phi::KernelContext(),</span><br><span class=\"line\">                          new RuntimeInferShapeContext(*this, *runtime_ctx));</span><br><span class=\"line\">    BuildPhiKernelContext(*runtime_ctx, dev_ctx, impl_-&gt;getKernelContext());</span><br><span class=\"line\"></span><br><span class=\"line\">    (*pt_kernel_)(impl_-&gt;getKernelContext());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Metal for Paddle Lite","date":"2022-01-05T05:32:39.000Z","_content":"# Metal for Paddle Lite\n![image](https://user-images.githubusercontent.com/1312389/161764131-2e745999-bc96-4c92-9bb0-1e459bc46c95.png)\n\n* Metal kernel and context\n![image](https://user-images.githubusercontent.com/1312389/161764543-9e53e60a-6dbb-4c28-9d32-b23b41857b09.png)\n* Metal OP executation\n![image](https://user-images.githubusercontent.com/1312389/161764633-cfc0e0ca-1786-48ad-97b5-4e8410dc8a05.png)","source":"_posts/article/paddlelite-metal.md","raw":"---\ntitle: Metal for Paddle Lite\ndate: 2022-01-05 13:32:39\ntags:\n---\n# Metal for Paddle Lite\n![image](https://user-images.githubusercontent.com/1312389/161764131-2e745999-bc96-4c92-9bb0-1e459bc46c95.png)\n\n* Metal kernel and context\n![image](https://user-images.githubusercontent.com/1312389/161764543-9e53e60a-6dbb-4c28-9d32-b23b41857b09.png)\n* Metal OP executation\n![image](https://user-images.githubusercontent.com/1312389/161764633-cfc0e0ca-1786-48ad-97b5-4e8410dc8a05.png)","slug":"article/paddlelite-metal","published":1,"updated":"2025-09-29T14:59:49.936Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000kotzkablagvhj","content":"<h1 id=\"Metal-for-Paddle-Lite\"><a href=\"#Metal-for-Paddle-Lite\" class=\"headerlink\" title=\"Metal for Paddle Lite\"></a>Metal for Paddle Lite</h1><p><img src=\"https://user-images.githubusercontent.com/1312389/161764131-2e745999-bc96-4c92-9bb0-1e459bc46c95.png\" alt=\"image\"></p>\n<ul>\n<li>Metal kernel and context<br><img src=\"https://user-images.githubusercontent.com/1312389/161764543-9e53e60a-6dbb-4c28-9d32-b23b41857b09.png\" alt=\"image\"></li>\n<li>Metal OP executation<br><img src=\"https://user-images.githubusercontent.com/1312389/161764633-cfc0e0ca-1786-48ad-97b5-4e8410dc8a05.png\" alt=\"image\"></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"Metal-for-Paddle-Lite\"><a href=\"#Metal-for-Paddle-Lite\" class=\"headerlink\" title=\"Metal for Paddle Lite\"></a>Metal for Paddle Lite</h1><p><img src=\"https://user-images.githubusercontent.com/1312389/161764131-2e745999-bc96-4c92-9bb0-1e459bc46c95.png\" alt=\"image\"></p>\n<ul>\n<li>Metal kernel and context<br><img src=\"https://user-images.githubusercontent.com/1312389/161764543-9e53e60a-6dbb-4c28-9d32-b23b41857b09.png\" alt=\"image\"></li>\n<li>Metal OP executation<br><img src=\"https://user-images.githubusercontent.com/1312389/161764633-cfc0e0ca-1786-48ad-97b5-4e8410dc8a05.png\" alt=\"image\"></li>\n</ul>\n"},{"title":"常用命令","date":"2021-03-11T00:50:47.000Z","_content":"记录linux常用命令\n# shell\n* 目录下文件中指定字符串替换\n  将当前目录包括子目录的文件中`printf`字符串替换为`print`\n  ```\n  sed -i \"s/printf/print/g\" `grep printf -rl ./`\n  ```","source":"_posts/article/shell.md","raw":"---\ntitle: 常用命令\ndate: 2021-03-11 08:50:47\ntags:\n---\n记录linux常用命令\n# shell\n* 目录下文件中指定字符串替换\n  将当前目录包括子目录的文件中`printf`字符串替换为`print`\n  ```\n  sed -i \"s/printf/print/g\" `grep printf -rl ./`\n  ```","slug":"article/shell","published":1,"updated":"2025-09-29T14:59:49.939Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000lotzk7brug7gt","content":"<p>记录linux常用命令</p>\n<h1 id=\"shell\"><a href=\"#shell\" class=\"headerlink\" title=\"shell\"></a>shell</h1><ul>\n<li>目录下文件中指定字符串替换<br>将当前目录包括子目录的文件中<code>printf</code>字符串替换为<code>print</code><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed -i &quot;s/printf/print/g&quot; `grep printf -rl ./`</span><br></pre></td></tr></table></figure></li>\n</ul>\n","excerpt":"","more":"<p>记录linux常用命令</p>\n<h1 id=\"shell\"><a href=\"#shell\" class=\"headerlink\" title=\"shell\"></a>shell</h1><ul>\n<li>目录下文件中指定字符串替换<br>将当前目录包括子目录的文件中<code>printf</code>字符串替换为<code>print</code><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sed -i &quot;s/printf/print/g&quot; `grep printf -rl ./`</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"std::async、std::future","date":"2021-04-05T06:22:20.000Z","_content":"\n## std::async 用法\n```\ntemplate<class Fn, class... Args>\nfuture<typename result_of<Fn(Args...)>::type> async(launch policy, Fn&& fn, Args&&...args);\n```\n* std::launch::async\n  系统默认，调用时创建新线程, \n* std::launch::deferred\n  延迟到std::future调用wait()或者get()时才执行，主线程调用，不创建新线程\n\nstd::async 封装\n```\ntemplate <typename F, typename... Args>\nauto really_async(F&& f, Args&&... args)\n-> std::future<typename std::result_of<F(Args...)>::type>\n{\n    using RetType = typename std::result_of<F(Args...)>::type;\n    auto func = std::bind(std::forward<F>(f), std::forward<Args>(args)...);\n    std::packaged_task<RetType()> task(std::move(func));\n    auto fut = task.get_future();\n    std::thread trd(std::move(task));\n    trd.detach();\n    return fut;\n}\n```\n\n## std::future 用法\n\n### std::future_status 三种状态\n* deferred\n  异步操作待开始\n* ready\n  异步操作完成\n* timeout\n  异步操作超时\n\n## std::promise\n## std::packaged_task","source":"_posts/article/std-async.md","raw":"---\ntitle: 'std::async、std::future'\ndate: 2021-04-05 14:22:20\ntags:\n---\n\n## std::async 用法\n```\ntemplate<class Fn, class... Args>\nfuture<typename result_of<Fn(Args...)>::type> async(launch policy, Fn&& fn, Args&&...args);\n```\n* std::launch::async\n  系统默认，调用时创建新线程, \n* std::launch::deferred\n  延迟到std::future调用wait()或者get()时才执行，主线程调用，不创建新线程\n\nstd::async 封装\n```\ntemplate <typename F, typename... Args>\nauto really_async(F&& f, Args&&... args)\n-> std::future<typename std::result_of<F(Args...)>::type>\n{\n    using RetType = typename std::result_of<F(Args...)>::type;\n    auto func = std::bind(std::forward<F>(f), std::forward<Args>(args)...);\n    std::packaged_task<RetType()> task(std::move(func));\n    auto fut = task.get_future();\n    std::thread trd(std::move(task));\n    trd.detach();\n    return fut;\n}\n```\n\n## std::future 用法\n\n### std::future_status 三种状态\n* deferred\n  异步操作待开始\n* ready\n  异步操作完成\n* timeout\n  异步操作超时\n\n## std::promise\n## std::packaged_task","slug":"article/std-async","published":1,"updated":"2025-09-29T14:59:49.939Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905y000motzk6kun0x1k","content":"<h2 id=\"std-async-用法\"><a href=\"#std-async-用法\" class=\"headerlink\" title=\"std::async 用法\"></a>std::async 用法</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;class Fn, class... Args&gt;</span><br><span class=\"line\">future&lt;typename result_of&lt;Fn(Args...)&gt;::type&gt; async(launch policy, Fn&amp;&amp; fn, Args&amp;&amp;...args);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>std::launch::async<br>系统默认，调用时创建新线程, </li>\n<li>std::launch::deferred<br>延迟到std::future调用wait()或者get()时才执行，主线程调用，不创建新线程</li>\n</ul>\n<p>std::async 封装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;typename F, typename... Args&gt;</span><br><span class=\"line\">auto really_async(F&amp;&amp; f, Args&amp;&amp;... args)</span><br><span class=\"line\">-&gt; std::future&lt;typename std::result_of&lt;F(Args...)&gt;::type&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    using RetType = typename std::result_of&lt;F(Args...)&gt;::type;</span><br><span class=\"line\">    auto func = std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...);</span><br><span class=\"line\">    std::packaged_task&lt;RetType()&gt; task(std::move(func));</span><br><span class=\"line\">    auto fut = task.get_future();</span><br><span class=\"line\">    std::thread trd(std::move(task));</span><br><span class=\"line\">    trd.detach();</span><br><span class=\"line\">    return fut;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"std-future-用法\"><a href=\"#std-future-用法\" class=\"headerlink\" title=\"std::future 用法\"></a>std::future 用法</h2><h3 id=\"std-future-status-三种状态\"><a href=\"#std-future-status-三种状态\" class=\"headerlink\" title=\"std::future_status 三种状态\"></a>std::future_status 三种状态</h3><ul>\n<li>deferred<br>异步操作待开始</li>\n<li>ready<br>异步操作完成</li>\n<li>timeout<br>异步操作超时</li>\n</ul>\n<h2 id=\"std-promise\"><a href=\"#std-promise\" class=\"headerlink\" title=\"std::promise\"></a>std::promise</h2><h2 id=\"std-packaged-task\"><a href=\"#std-packaged-task\" class=\"headerlink\" title=\"std::packaged_task\"></a>std::packaged_task</h2>","excerpt":"","more":"<h2 id=\"std-async-用法\"><a href=\"#std-async-用法\" class=\"headerlink\" title=\"std::async 用法\"></a>std::async 用法</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;class Fn, class... Args&gt;</span><br><span class=\"line\">future&lt;typename result_of&lt;Fn(Args...)&gt;::type&gt; async(launch policy, Fn&amp;&amp; fn, Args&amp;&amp;...args);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>std::launch::async<br>系统默认，调用时创建新线程, </li>\n<li>std::launch::deferred<br>延迟到std::future调用wait()或者get()时才执行，主线程调用，不创建新线程</li>\n</ul>\n<p>std::async 封装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;typename F, typename... Args&gt;</span><br><span class=\"line\">auto really_async(F&amp;&amp; f, Args&amp;&amp;... args)</span><br><span class=\"line\">-&gt; std::future&lt;typename std::result_of&lt;F(Args...)&gt;::type&gt;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    using RetType = typename std::result_of&lt;F(Args...)&gt;::type;</span><br><span class=\"line\">    auto func = std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...);</span><br><span class=\"line\">    std::packaged_task&lt;RetType()&gt; task(std::move(func));</span><br><span class=\"line\">    auto fut = task.get_future();</span><br><span class=\"line\">    std::thread trd(std::move(task));</span><br><span class=\"line\">    trd.detach();</span><br><span class=\"line\">    return fut;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"std-future-用法\"><a href=\"#std-future-用法\" class=\"headerlink\" title=\"std::future 用法\"></a>std::future 用法</h2><h3 id=\"std-future-status-三种状态\"><a href=\"#std-future-status-三种状态\" class=\"headerlink\" title=\"std::future_status 三种状态\"></a>std::future_status 三种状态</h3><ul>\n<li>deferred<br>异步操作待开始</li>\n<li>ready<br>异步操作完成</li>\n<li>timeout<br>异步操作超时</li>\n</ul>\n<h2 id=\"std-promise\"><a href=\"#std-promise\" class=\"headerlink\" title=\"std::promise\"></a>std::promise</h2><h2 id=\"std-packaged-task\"><a href=\"#std-packaged-task\" class=\"headerlink\" title=\"std::packaged_task\"></a>std::packaged_task</h2>"},{"title":"TensorRT","date":"2022-01-05T23:47:06.000Z","_content":"[TOC]\n# introduction\n![TensorRT](https://user-images.githubusercontent.com/1312389/150677468-932f4721-78ee-4789-936d-2e0dd4c13f4c.png)\n\n# install \n可以使用三种方式进行安装，包括\n* container 形式进行安装，下载[NGC container](http://ngc.nvidia.com/); \n* debian 形式安装\n* pip 形式进行安装\n\n## container 形式安装\n下载https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile\n```docker build -f ubuntu-18.04.Dockerfile --build-arg CUDA_VERSION=11.4.3 --tag=tensorrt-ubuntu .```\n\n## debian 形式安装\n## pip形式进行安装\n与TensorRT包里面wheel包安装形式不同，这种方式是自己管理TensorRT安装，不需要提前安装TensorRT包。目前只支持Python 3.6～3.9和CUDA 11.4。\n\n安装前的准备\n```\npython3 -m pip install nvidia-pyindex\n```\npip install时需要额外指定```--extra-index-url https://pypi.ngc.nvidia.com```\n\n安装TensorRT wheel包\n```\npython3 -m pip install --upgrade nvidia-tensorrt\n```\n进行验证\n```\npython3\n>>> import tensorrt\n>>> print(tensorrt.__version__)\n>>> assert tensorrt.Builder(tensorrt.Logger())\n```\n\n# TensorRT生态\n## basic workflow\n![workflow](https://user-images.githubusercontent.com/1312389/150677789-25ed7568-b01d-4ccc-9e15-0266a61ae2c2.png)\n\n## convert\n* 使用[TF-TRT](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt)\n* 使用[Torch-TensorRT](https://github.com/NVIDIA/Torch-TensorRT)\n* onnx转换器转换.onnx模型\n* 使用[TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/index.html)进行组网\n\n## deploy\n* 使用 TensorFlow\n\n  使用 TensorFflow 模型部署即可，TensorRT不支持的OP，会fall back到TensorFlow实现。\n* 使用 TRT Runtime API\n\n  开销最小，能实现细粒度控制。对于不是原生支持的OP，需要使用plugin进行实现\n* 使用 [Nvidia Triton Inference Server](https://github.com/triton-inference-server/server)\n  \n  能支持多种框架，包括 TensorFlow, TensorRT, PyTorch, ONNX Runtime, 或者自定义框架。\n\n# TensorRT 基础介绍\n## 创建引擎\n```\nLogger gLogger;\nIBuilder* builder = createInferBuilder(gLogger);\nnvinfer1::INetworkDefinition* network = builder->createNetworkV2(1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));\n```\n## 构建推理\n```\nIBuilderConfig* config = builder->createBuilderConfig();\nconfig->setMemoryPoolLimit(1 << 20);\n//设置推理精度\nconfig->setFlag(nvinfer1::BuilderFlag::kFP16);\n\nengine = builder->buildSerializedNetwork(*network, *config);\ncontext = engine->createExecutionContext();\n```\n\n```\nvoid* buffers[n];\nengine->getBindingIndex(\ncontext->enqueueV2(buffers, stream, nullptr);\n```\n## dynamic shape\n createNetwork()与createNetworkV2()的区别有两处，一是前者处理的维度为(C,H,W), 后者为(B,C,H,W)；二是后者支持dynamic shapes。\n## plugin\n\n * createNetwork()\n * createNetworkV2()\n## TensorRT 优化\n https://blog.csdn.net/qq_33287871/article/details/117201271\n * Weight &Activation Precision Calibration\n * Layer & Tensor Fusion\n * Kernel Auto-Tuning\n * Dynamic Tensor Memory\n * Multi-Stream Execution\n\n# TensorRT API\n ```\n bool reshapeWeights(\n     const Weights& input, int32_t const* shape, int32_t const* shapeOrder, void* data, int32_t nbDims) noexcept;\n bool reorderSubBuffers(\n     void* input, int32_t const* order, int32_t num, int32_t size) noexcept;\n bool transposeSubBuffers(\n     void* input, DataType type, int32_t num, int32_t height, int32_t width) noexcept;\n ```\n# TensorRT 常见问题\n\n# DLA\n## DLA Supported Layers\n# reference\n[TensorRT](https://github.com/NVIDIA/TensorRT)\n\n[TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)\n\n[TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/index.html)\n\n[ONNX-TensorRT](https://github.com/onnx/onnx-tensorrt)\n\n[Torch-TensorRT](https://github.com/NVIDIA/Torch-TensorRT)","source":"_posts/article/tensorrt.md","raw":"---\ntitle: TensorRT\ndate: 2022-01-06 07:47:06\ntags: GPU, TensorRT\n---\n[TOC]\n# introduction\n![TensorRT](https://user-images.githubusercontent.com/1312389/150677468-932f4721-78ee-4789-936d-2e0dd4c13f4c.png)\n\n# install \n可以使用三种方式进行安装，包括\n* container 形式进行安装，下载[NGC container](http://ngc.nvidia.com/); \n* debian 形式安装\n* pip 形式进行安装\n\n## container 形式安装\n下载https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile\n```docker build -f ubuntu-18.04.Dockerfile --build-arg CUDA_VERSION=11.4.3 --tag=tensorrt-ubuntu .```\n\n## debian 形式安装\n## pip形式进行安装\n与TensorRT包里面wheel包安装形式不同，这种方式是自己管理TensorRT安装，不需要提前安装TensorRT包。目前只支持Python 3.6～3.9和CUDA 11.4。\n\n安装前的准备\n```\npython3 -m pip install nvidia-pyindex\n```\npip install时需要额外指定```--extra-index-url https://pypi.ngc.nvidia.com```\n\n安装TensorRT wheel包\n```\npython3 -m pip install --upgrade nvidia-tensorrt\n```\n进行验证\n```\npython3\n>>> import tensorrt\n>>> print(tensorrt.__version__)\n>>> assert tensorrt.Builder(tensorrt.Logger())\n```\n\n# TensorRT生态\n## basic workflow\n![workflow](https://user-images.githubusercontent.com/1312389/150677789-25ed7568-b01d-4ccc-9e15-0266a61ae2c2.png)\n\n## convert\n* 使用[TF-TRT](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt)\n* 使用[Torch-TensorRT](https://github.com/NVIDIA/Torch-TensorRT)\n* onnx转换器转换.onnx模型\n* 使用[TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/index.html)进行组网\n\n## deploy\n* 使用 TensorFlow\n\n  使用 TensorFflow 模型部署即可，TensorRT不支持的OP，会fall back到TensorFlow实现。\n* 使用 TRT Runtime API\n\n  开销最小，能实现细粒度控制。对于不是原生支持的OP，需要使用plugin进行实现\n* 使用 [Nvidia Triton Inference Server](https://github.com/triton-inference-server/server)\n  \n  能支持多种框架，包括 TensorFlow, TensorRT, PyTorch, ONNX Runtime, 或者自定义框架。\n\n# TensorRT 基础介绍\n## 创建引擎\n```\nLogger gLogger;\nIBuilder* builder = createInferBuilder(gLogger);\nnvinfer1::INetworkDefinition* network = builder->createNetworkV2(1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));\n```\n## 构建推理\n```\nIBuilderConfig* config = builder->createBuilderConfig();\nconfig->setMemoryPoolLimit(1 << 20);\n//设置推理精度\nconfig->setFlag(nvinfer1::BuilderFlag::kFP16);\n\nengine = builder->buildSerializedNetwork(*network, *config);\ncontext = engine->createExecutionContext();\n```\n\n```\nvoid* buffers[n];\nengine->getBindingIndex(\ncontext->enqueueV2(buffers, stream, nullptr);\n```\n## dynamic shape\n createNetwork()与createNetworkV2()的区别有两处，一是前者处理的维度为(C,H,W), 后者为(B,C,H,W)；二是后者支持dynamic shapes。\n## plugin\n\n * createNetwork()\n * createNetworkV2()\n## TensorRT 优化\n https://blog.csdn.net/qq_33287871/article/details/117201271\n * Weight &Activation Precision Calibration\n * Layer & Tensor Fusion\n * Kernel Auto-Tuning\n * Dynamic Tensor Memory\n * Multi-Stream Execution\n\n# TensorRT API\n ```\n bool reshapeWeights(\n     const Weights& input, int32_t const* shape, int32_t const* shapeOrder, void* data, int32_t nbDims) noexcept;\n bool reorderSubBuffers(\n     void* input, int32_t const* order, int32_t num, int32_t size) noexcept;\n bool transposeSubBuffers(\n     void* input, DataType type, int32_t num, int32_t height, int32_t width) noexcept;\n ```\n# TensorRT 常见问题\n\n# DLA\n## DLA Supported Layers\n# reference\n[TensorRT](https://github.com/NVIDIA/TensorRT)\n\n[TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)\n\n[TensorRT API](https://docs.nvidia.com/deeplearning/tensorrt/api/index.html)\n\n[ONNX-TensorRT](https://github.com/onnx/onnx-tensorrt)\n\n[Torch-TensorRT](https://github.com/NVIDIA/Torch-TensorRT)","slug":"article/tensorrt","published":1,"updated":"2025-09-29T14:59:49.935Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905z000notzkbo73dhat","content":"<p>[TOC]</p>\n<h1 id=\"introduction\"><a href=\"#introduction\" class=\"headerlink\" title=\"introduction\"></a>introduction</h1><p><img src=\"https://user-images.githubusercontent.com/1312389/150677468-932f4721-78ee-4789-936d-2e0dd4c13f4c.png\" alt=\"TensorRT\"></p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p>可以使用三种方式进行安装，包括</p>\n<ul>\n<li>container 形式进行安装，下载<a href=\"http://ngc.nvidia.com/\">NGC container</a>; </li>\n<li>debian 形式安装</li>\n<li>pip 形式进行安装</li>\n</ul>\n<h2 id=\"container-形式安装\"><a href=\"#container-形式安装\" class=\"headerlink\" title=\"container 形式安装\"></a>container 形式安装</h2><p>下载<a href=\"https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile\">https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile</a><br><code>docker build -f ubuntu-18.04.Dockerfile --build-arg CUDA_VERSION=11.4.3 --tag=tensorrt-ubuntu .</code></p>\n<h2 id=\"debian-形式安装\"><a href=\"#debian-形式安装\" class=\"headerlink\" title=\"debian 形式安装\"></a>debian 形式安装</h2><h2 id=\"pip形式进行安装\"><a href=\"#pip形式进行安装\" class=\"headerlink\" title=\"pip形式进行安装\"></a>pip形式进行安装</h2><p>与TensorRT包里面wheel包安装形式不同，这种方式是自己管理TensorRT安装，不需要提前安装TensorRT包。目前只支持Python 3.6～3.9和CUDA 11.4。</p>\n<p>安装前的准备</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install nvidia-pyindex</span><br></pre></td></tr></table></figure>\n<p>pip install时需要额外指定<code>--extra-index-url https://pypi.ngc.nvidia.com</code></p>\n<p>安装TensorRT wheel包</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade nvidia-tensorrt</span><br></pre></td></tr></table></figure>\n<p>进行验证</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3</span><br><span class=\"line\">&gt;&gt;&gt; import tensorrt</span><br><span class=\"line\">&gt;&gt;&gt; print(tensorrt.__version__)</span><br><span class=\"line\">&gt;&gt;&gt; assert tensorrt.Builder(tensorrt.Logger())</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"TensorRT生态\"><a href=\"#TensorRT生态\" class=\"headerlink\" title=\"TensorRT生态\"></a>TensorRT生态</h1><h2 id=\"basic-workflow\"><a href=\"#basic-workflow\" class=\"headerlink\" title=\"basic workflow\"></a>basic workflow</h2><p><img src=\"https://user-images.githubusercontent.com/1312389/150677789-25ed7568-b01d-4ccc-9e15-0266a61ae2c2.png\" alt=\"workflow\"></p>\n<h2 id=\"convert\"><a href=\"#convert\" class=\"headerlink\" title=\"convert\"></a>convert</h2><ul>\n<li>使用<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt\">TF-TRT</a></li>\n<li>使用<a href=\"https://github.com/NVIDIA/Torch-TensorRT\">Torch-TensorRT</a></li>\n<li>onnx转换器转换.onnx模型</li>\n<li>使用<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/api/index.html\">TensorRT API</a>进行组网</li>\n</ul>\n<h2 id=\"deploy\"><a href=\"#deploy\" class=\"headerlink\" title=\"deploy\"></a>deploy</h2><ul>\n<li><p>使用 TensorFlow</p>\n<p>使用 TensorFflow 模型部署即可，TensorRT不支持的OP，会fall back到TensorFlow实现。</p>\n</li>\n<li><p>使用 TRT Runtime API</p>\n<p>开销最小，能实现细粒度控制。对于不是原生支持的OP，需要使用plugin进行实现</p>\n</li>\n<li><p>使用 <a href=\"https://github.com/triton-inference-server/server\">Nvidia Triton Inference Server</a></p>\n<p>能支持多种框架，包括 TensorFlow, TensorRT, PyTorch, ONNX Runtime, 或者自定义框架。</p>\n</li>\n</ul>\n<h1 id=\"TensorRT-基础介绍\"><a href=\"#TensorRT-基础介绍\" class=\"headerlink\" title=\"TensorRT 基础介绍\"></a>TensorRT 基础介绍</h1><h2 id=\"创建引擎\"><a href=\"#创建引擎\" class=\"headerlink\" title=\"创建引擎\"></a>创建引擎</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Logger gLogger;</span><br><span class=\"line\">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class=\"line\">nvinfer1::INetworkDefinition* network = builder-&gt;createNetworkV2(1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure>\n<h2 id=\"构建推理\"><a href=\"#构建推理\" class=\"headerlink\" title=\"构建推理\"></a>构建推理</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br><span class=\"line\">config-&gt;setMemoryPoolLimit(1 &lt;&lt; 20);</span><br><span class=\"line\">//设置推理精度</span><br><span class=\"line\">config-&gt;setFlag(nvinfer1::BuilderFlag::kFP16);</span><br><span class=\"line\"></span><br><span class=\"line\">engine = builder-&gt;buildSerializedNetwork(*network, *config);</span><br><span class=\"line\">context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void* buffers[n];</span><br><span class=\"line\">engine-&gt;getBindingIndex(</span><br><span class=\"line\">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br></pre></td></tr></table></figure>\n<h2 id=\"dynamic-shape\"><a href=\"#dynamic-shape\" class=\"headerlink\" title=\"dynamic shape\"></a>dynamic shape</h2><p> createNetwork()与createNetworkV2()的区别有两处，一是前者处理的维度为(C,H,W), 后者为(B,C,H,W)；二是后者支持dynamic shapes。</p>\n<h2 id=\"plugin\"><a href=\"#plugin\" class=\"headerlink\" title=\"plugin\"></a>plugin</h2><ul>\n<li>createNetwork()</li>\n<li>createNetworkV2()</li>\n</ul>\n<h2 id=\"TensorRT-优化\"><a href=\"#TensorRT-优化\" class=\"headerlink\" title=\"TensorRT 优化\"></a>TensorRT 优化</h2><p> <a href=\"https://blog.csdn.net/qq_33287871/article/details/117201271\">https://blog.csdn.net/qq_33287871/article/details/117201271</a></p>\n<ul>\n<li>Weight &amp;Activation Precision Calibration</li>\n<li>Layer &amp; Tensor Fusion</li>\n<li>Kernel Auto-Tuning</li>\n<li>Dynamic Tensor Memory</li>\n<li>Multi-Stream Execution</li>\n</ul>\n<h1 id=\"TensorRT-API\"><a href=\"#TensorRT-API\" class=\"headerlink\" title=\"TensorRT API\"></a>TensorRT API</h1> <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool reshapeWeights(</span><br><span class=\"line\">    const Weights&amp; input, int32_t const* shape, int32_t const* shapeOrder, void* data, int32_t nbDims) noexcept;</span><br><span class=\"line\">bool reorderSubBuffers(</span><br><span class=\"line\">    void* input, int32_t const* order, int32_t num, int32_t size) noexcept;</span><br><span class=\"line\">bool transposeSubBuffers(</span><br><span class=\"line\">    void* input, DataType type, int32_t num, int32_t height, int32_t width) noexcept;</span><br></pre></td></tr></table></figure>\n<h1 id=\"TensorRT-常见问题\"><a href=\"#TensorRT-常见问题\" class=\"headerlink\" title=\"TensorRT 常见问题\"></a>TensorRT 常见问题</h1><h1 id=\"DLA\"><a href=\"#DLA\" class=\"headerlink\" title=\"DLA\"></a>DLA</h1><h2 id=\"DLA-Supported-Layers\"><a href=\"#DLA-Supported-Layers\" class=\"headerlink\" title=\"DLA Supported Layers\"></a>DLA Supported Layers</h2><h1 id=\"reference\"><a href=\"#reference\" class=\"headerlink\" title=\"reference\"></a>reference</h1><p><a href=\"https://github.com/NVIDIA/TensorRT\">TensorRT</a></p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/\">TensorRT Developer Guide</a></p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/api/index.html\">TensorRT API</a></p>\n<p><a href=\"https://github.com/onnx/onnx-tensorrt\">ONNX-TensorRT</a></p>\n<p><a href=\"https://github.com/NVIDIA/Torch-TensorRT\">Torch-TensorRT</a></p>\n","excerpt":"","more":"<p>[TOC]</p>\n<h1 id=\"introduction\"><a href=\"#introduction\" class=\"headerlink\" title=\"introduction\"></a>introduction</h1><p><img src=\"https://user-images.githubusercontent.com/1312389/150677468-932f4721-78ee-4789-936d-2e0dd4c13f4c.png\" alt=\"TensorRT\"></p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p>可以使用三种方式进行安装，包括</p>\n<ul>\n<li>container 形式进行安装，下载<a href=\"http://ngc.nvidia.com/\">NGC container</a>; </li>\n<li>debian 形式安装</li>\n<li>pip 形式进行安装</li>\n</ul>\n<h2 id=\"container-形式安装\"><a href=\"#container-形式安装\" class=\"headerlink\" title=\"container 形式安装\"></a>container 形式安装</h2><p>下载<a href=\"https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile\">https://github.com/NVIDIA/TensorRT/blob/main/docker/ubuntu-18.04.Dockerfile</a><br><code>docker build -f ubuntu-18.04.Dockerfile --build-arg CUDA_VERSION=11.4.3 --tag=tensorrt-ubuntu .</code></p>\n<h2 id=\"debian-形式安装\"><a href=\"#debian-形式安装\" class=\"headerlink\" title=\"debian 形式安装\"></a>debian 形式安装</h2><h2 id=\"pip形式进行安装\"><a href=\"#pip形式进行安装\" class=\"headerlink\" title=\"pip形式进行安装\"></a>pip形式进行安装</h2><p>与TensorRT包里面wheel包安装形式不同，这种方式是自己管理TensorRT安装，不需要提前安装TensorRT包。目前只支持Python 3.6～3.9和CUDA 11.4。</p>\n<p>安装前的准备</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install nvidia-pyindex</span><br></pre></td></tr></table></figure>\n<p>pip install时需要额外指定<code>--extra-index-url https://pypi.ngc.nvidia.com</code></p>\n<p>安装TensorRT wheel包</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m pip install --upgrade nvidia-tensorrt</span><br></pre></td></tr></table></figure>\n<p>进行验证</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3</span><br><span class=\"line\">&gt;&gt;&gt; import tensorrt</span><br><span class=\"line\">&gt;&gt;&gt; print(tensorrt.__version__)</span><br><span class=\"line\">&gt;&gt;&gt; assert tensorrt.Builder(tensorrt.Logger())</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"TensorRT生态\"><a href=\"#TensorRT生态\" class=\"headerlink\" title=\"TensorRT生态\"></a>TensorRT生态</h1><h2 id=\"basic-workflow\"><a href=\"#basic-workflow\" class=\"headerlink\" title=\"basic workflow\"></a>basic workflow</h2><p><img src=\"https://user-images.githubusercontent.com/1312389/150677789-25ed7568-b01d-4ccc-9e15-0266a61ae2c2.png\" alt=\"workflow\"></p>\n<h2 id=\"convert\"><a href=\"#convert\" class=\"headerlink\" title=\"convert\"></a>convert</h2><ul>\n<li>使用<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2tensorrt\">TF-TRT</a></li>\n<li>使用<a href=\"https://github.com/NVIDIA/Torch-TensorRT\">Torch-TensorRT</a></li>\n<li>onnx转换器转换.onnx模型</li>\n<li>使用<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/api/index.html\">TensorRT API</a>进行组网</li>\n</ul>\n<h2 id=\"deploy\"><a href=\"#deploy\" class=\"headerlink\" title=\"deploy\"></a>deploy</h2><ul>\n<li><p>使用 TensorFlow</p>\n<p>使用 TensorFflow 模型部署即可，TensorRT不支持的OP，会fall back到TensorFlow实现。</p>\n</li>\n<li><p>使用 TRT Runtime API</p>\n<p>开销最小，能实现细粒度控制。对于不是原生支持的OP，需要使用plugin进行实现</p>\n</li>\n<li><p>使用 <a href=\"https://github.com/triton-inference-server/server\">Nvidia Triton Inference Server</a></p>\n<p>能支持多种框架，包括 TensorFlow, TensorRT, PyTorch, ONNX Runtime, 或者自定义框架。</p>\n</li>\n</ul>\n<h1 id=\"TensorRT-基础介绍\"><a href=\"#TensorRT-基础介绍\" class=\"headerlink\" title=\"TensorRT 基础介绍\"></a>TensorRT 基础介绍</h1><h2 id=\"创建引擎\"><a href=\"#创建引擎\" class=\"headerlink\" title=\"创建引擎\"></a>创建引擎</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Logger gLogger;</span><br><span class=\"line\">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class=\"line\">nvinfer1::INetworkDefinition* network = builder-&gt;createNetworkV2(1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure>\n<h2 id=\"构建推理\"><a href=\"#构建推理\" class=\"headerlink\" title=\"构建推理\"></a>构建推理</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br><span class=\"line\">config-&gt;setMemoryPoolLimit(1 &lt;&lt; 20);</span><br><span class=\"line\">//设置推理精度</span><br><span class=\"line\">config-&gt;setFlag(nvinfer1::BuilderFlag::kFP16);</span><br><span class=\"line\"></span><br><span class=\"line\">engine = builder-&gt;buildSerializedNetwork(*network, *config);</span><br><span class=\"line\">context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void* buffers[n];</span><br><span class=\"line\">engine-&gt;getBindingIndex(</span><br><span class=\"line\">context-&gt;enqueueV2(buffers, stream, nullptr);</span><br></pre></td></tr></table></figure>\n<h2 id=\"dynamic-shape\"><a href=\"#dynamic-shape\" class=\"headerlink\" title=\"dynamic shape\"></a>dynamic shape</h2><p> createNetwork()与createNetworkV2()的区别有两处，一是前者处理的维度为(C,H,W), 后者为(B,C,H,W)；二是后者支持dynamic shapes。</p>\n<h2 id=\"plugin\"><a href=\"#plugin\" class=\"headerlink\" title=\"plugin\"></a>plugin</h2><ul>\n<li>createNetwork()</li>\n<li>createNetworkV2()</li>\n</ul>\n<h2 id=\"TensorRT-优化\"><a href=\"#TensorRT-优化\" class=\"headerlink\" title=\"TensorRT 优化\"></a>TensorRT 优化</h2><p> <a href=\"https://blog.csdn.net/qq_33287871/article/details/117201271\">https://blog.csdn.net/qq_33287871/article/details/117201271</a></p>\n<ul>\n<li>Weight &amp;Activation Precision Calibration</li>\n<li>Layer &amp; Tensor Fusion</li>\n<li>Kernel Auto-Tuning</li>\n<li>Dynamic Tensor Memory</li>\n<li>Multi-Stream Execution</li>\n</ul>\n<h1 id=\"TensorRT-API\"><a href=\"#TensorRT-API\" class=\"headerlink\" title=\"TensorRT API\"></a>TensorRT API</h1> <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bool reshapeWeights(</span><br><span class=\"line\">    const Weights&amp; input, int32_t const* shape, int32_t const* shapeOrder, void* data, int32_t nbDims) noexcept;</span><br><span class=\"line\">bool reorderSubBuffers(</span><br><span class=\"line\">    void* input, int32_t const* order, int32_t num, int32_t size) noexcept;</span><br><span class=\"line\">bool transposeSubBuffers(</span><br><span class=\"line\">    void* input, DataType type, int32_t num, int32_t height, int32_t width) noexcept;</span><br></pre></td></tr></table></figure>\n<h1 id=\"TensorRT-常见问题\"><a href=\"#TensorRT-常见问题\" class=\"headerlink\" title=\"TensorRT 常见问题\"></a>TensorRT 常见问题</h1><h1 id=\"DLA\"><a href=\"#DLA\" class=\"headerlink\" title=\"DLA\"></a>DLA</h1><h2 id=\"DLA-Supported-Layers\"><a href=\"#DLA-Supported-Layers\" class=\"headerlink\" title=\"DLA Supported Layers\"></a>DLA Supported Layers</h2><h1 id=\"reference\"><a href=\"#reference\" class=\"headerlink\" title=\"reference\"></a>reference</h1><p><a href=\"https://github.com/NVIDIA/TensorRT\">TensorRT</a></p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/\">TensorRT Developer Guide</a></p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/api/index.html\">TensorRT API</a></p>\n<p><a href=\"https://github.com/onnx/onnx-tensorrt\">ONNX-TensorRT</a></p>\n<p><a href=\"https://github.com/NVIDIA/Torch-TensorRT\">Torch-TensorRT</a></p>\n"},{"title":"macOS tips","date":"2021-12-08T22:31:52.000Z","_content":"\n# macOS python load Framework\nhttps://docs.python.org/3/library/ctypes.html\n```\nfrom ctypes.util import find_library\nfrom ctypes import cdll\nimport os\n\nprint(os.name)\nmetal_library = find_library(\"Metal\")\ncore_graphics_library = find_library(\"CoreGraphics\")\nmps_library = find_library(\"MetalPerformanceShaders\")\n\nprint(cdll.LoadLibrary(metal_library))\nprint(cdll.LoadLibrary(core_graphics_library))\nprint(cdll.LoadLibrary(mps_library))\n```","source":"_posts/article/use-macos.md","raw":"---\ntitle: macOS tips\ndate: 2021-12-09 06:31:52\ntags:\n---\n\n# macOS python load Framework\nhttps://docs.python.org/3/library/ctypes.html\n```\nfrom ctypes.util import find_library\nfrom ctypes import cdll\nimport os\n\nprint(os.name)\nmetal_library = find_library(\"Metal\")\ncore_graphics_library = find_library(\"CoreGraphics\")\nmps_library = find_library(\"MetalPerformanceShaders\")\n\nprint(cdll.LoadLibrary(metal_library))\nprint(cdll.LoadLibrary(core_graphics_library))\nprint(cdll.LoadLibrary(mps_library))\n```","slug":"article/use-macos","published":1,"updated":"2025-09-29T14:59:49.937Z","comments":1,"layout":"post","photos":[],"_id":"cmg59905z000ootzka5e5e0he","content":"<h1 id=\"macOS-python-load-Framework\"><a href=\"#macOS-python-load-Framework\" class=\"headerlink\" title=\"macOS python load Framework\"></a>macOS python load Framework</h1><p><a href=\"https://docs.python.org/3/library/ctypes.html\">https://docs.python.org/3/library/ctypes.html</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from ctypes.util import find_library</span><br><span class=\"line\">from ctypes import cdll</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">print(os.name)</span><br><span class=\"line\">metal_library = find_library(&quot;Metal&quot;)</span><br><span class=\"line\">core_graphics_library = find_library(&quot;CoreGraphics&quot;)</span><br><span class=\"line\">mps_library = find_library(&quot;MetalPerformanceShaders&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(cdll.LoadLibrary(metal_library))</span><br><span class=\"line\">print(cdll.LoadLibrary(core_graphics_library))</span><br><span class=\"line\">print(cdll.LoadLibrary(mps_library))</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"macOS-python-load-Framework\"><a href=\"#macOS-python-load-Framework\" class=\"headerlink\" title=\"macOS python load Framework\"></a>macOS python load Framework</h1><p><a href=\"https://docs.python.org/3/library/ctypes.html\">https://docs.python.org/3/library/ctypes.html</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from ctypes.util import find_library</span><br><span class=\"line\">from ctypes import cdll</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">print(os.name)</span><br><span class=\"line\">metal_library = find_library(&quot;Metal&quot;)</span><br><span class=\"line\">core_graphics_library = find_library(&quot;CoreGraphics&quot;)</span><br><span class=\"line\">mps_library = find_library(&quot;MetalPerformanceShaders&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">print(cdll.LoadLibrary(metal_library))</span><br><span class=\"line\">print(cdll.LoadLibrary(core_graphics_library))</span><br><span class=\"line\">print(cdll.LoadLibrary(mps_library))</span><br></pre></td></tr></table></figure>"},{"title":"线程池","date":"2022-04-04T21:44:43.000Z","_content":"\n# 线程池\n支持单例使用，支持任意参数的任务提交\n\n```\n#pragma once\n\n#include <condition_variable>\n#include <functional>\n#include <future>\n#include <memory>\n#include <mutex>\n#include <queue>\n#include <thread>\n#include <vector>\n\nclass ThreadPool {\n public:\n  using Task = std::function<void()>;\n  explicit ThreadPool(int num_threads): running_(true) {\n    threads_.resize(num_threads);\n    for (auto& thread : threads_) {\n      thread.reset(new std::thread(&ThreadPool::TaskLoop, this));\n    }\n  }\n  ~ThreadPool() {\n    {\n      std::unique_lock<std::mutex> lock(mutex_);\n      running_ = false;\n    }\n    scheduled_.notify_all();\n    for (auto& thread : threads_) {\n      thread->join();\n      thread.reset(nullptr);\n    }\n  }\n  static ThreadPool* GetInstance() {\n    std::call_once(init_flag_, &ThreadPool::Init);\n    return threadpool_.get();\n  }\n  \n\n  ThreadPool(const ThreadPool& pool) = delete;\n  ThreadPool& operator=(const ThreadPool& pool) = delete;\n  template<class F, class... Args>\n  auto Commit(F&& f, Args&&... args) -> std::future<decltype(f(args...))> {\n    if (!running_) {\n      throw std::runtime_error(\"ThreadPool is not running\");  \n    }\n    using RetType = decltype(f(args...));\n    auto task = std::make_shared<std::packaged_task<RetType()>>(\n      std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n    );\n    std::future<RetType> future = task -> get_future();\n    {\n        std::lock_guard<std::mutex> lock(mutex_);\n        tasks_.emplace([task]() {\n            (*task)();\n        });\n    }\n    scheduled_.notify_one();\n    return std::move(future);\n  }\n\n private:\n  void TaskLoop() {\n    while (true) {\n      Task task;\n      {\n        std::unique_lock<std::mutex> lock(mutex_);\n        scheduled_.wait(\n            lock, [this] { return !this->tasks_.empty() || !this->running_; });\n        if (!running_ && tasks_.empty()) {\n          return;\n        }\n        task = std::move(tasks_.front());\n        tasks_.pop();\n      }\n      task();\n    }\n  }\n  static void Init() {\n    if (threadpool_ == nullptr) {\n      int num_threads = std::thread::hardware_concurrency();\n      threadpool_.reset(new ThreadPool(num_threads));\n    }\n  }\n\n private:\n  static std::unique_ptr<ThreadPool> threadpool_;\n  static std::once_flag init_flag_;\n\n  std::vector<std::unique_ptr<std::thread>> threads_;\n  std::queue<Task> tasks_;\n  std::mutex mutex_;\n  bool running_;\n  std::condition_variable scheduled_;\n};\n\nstd::unique_ptr<ThreadPool> ThreadPool::threadpool_ = nullptr;\nstd::once_flag ThreadPool::init_flag_;\n```\n\n# 使用示例\n```\n#include <iostream>\n\n#include \"thread_pool.h\"\n\nstruct sum {\n  int operator()(int a, int b) {\n      int res = a + b;\n      return res;\n  }\n};\n\nint print(int a) {\n    return a;\n}\n\nclass A {\npublic:\n  static int calc(int val) {\n      return val;\n  }\n};\nint main() {\n  ThreadPool executor(10);\n  auto result = executor.Commit(&print, 3);\n  std::cout << \"result: \" << result.get() << std::endl;\n\n  // auto result = executor.Commit(A::calc, 3);\n  // std::cout << \"result: \" << result.get() << std::endl;\n  ThreadPool pool(4);\n  std::vector<std::future<int>> results;\n  std::chrono::seconds span(1);\n  for (int i = 0; i < 2; ++ i) {\n    results.emplace_back(\n        pool.Commit([i, span] {\n            std::cout << \"run \" << i << std::endl;\n            std::this_thread::sleep_for(span);\n            return i * i;\n        })\n    );\n  }\n\n  for (auto && item : results) {\n      if(item.wait_for(span) == std::future_status::ready) {\n        std::cout << item.get() << std::endl;\n      }\n  }\n\n  return 0;\n}\n```\n# 核心要点\n* 工作队列 work queu\n* thread factory\n* 饱和策略 handler","source":"_posts/article/thread-pool.md","raw":"---\ntitle: 线程池\ndate: 2022-04-05 05:44:43\ntags:\n---\n\n# 线程池\n支持单例使用，支持任意参数的任务提交\n\n```\n#pragma once\n\n#include <condition_variable>\n#include <functional>\n#include <future>\n#include <memory>\n#include <mutex>\n#include <queue>\n#include <thread>\n#include <vector>\n\nclass ThreadPool {\n public:\n  using Task = std::function<void()>;\n  explicit ThreadPool(int num_threads): running_(true) {\n    threads_.resize(num_threads);\n    for (auto& thread : threads_) {\n      thread.reset(new std::thread(&ThreadPool::TaskLoop, this));\n    }\n  }\n  ~ThreadPool() {\n    {\n      std::unique_lock<std::mutex> lock(mutex_);\n      running_ = false;\n    }\n    scheduled_.notify_all();\n    for (auto& thread : threads_) {\n      thread->join();\n      thread.reset(nullptr);\n    }\n  }\n  static ThreadPool* GetInstance() {\n    std::call_once(init_flag_, &ThreadPool::Init);\n    return threadpool_.get();\n  }\n  \n\n  ThreadPool(const ThreadPool& pool) = delete;\n  ThreadPool& operator=(const ThreadPool& pool) = delete;\n  template<class F, class... Args>\n  auto Commit(F&& f, Args&&... args) -> std::future<decltype(f(args...))> {\n    if (!running_) {\n      throw std::runtime_error(\"ThreadPool is not running\");  \n    }\n    using RetType = decltype(f(args...));\n    auto task = std::make_shared<std::packaged_task<RetType()>>(\n      std::bind(std::forward<F>(f), std::forward<Args>(args)...)\n    );\n    std::future<RetType> future = task -> get_future();\n    {\n        std::lock_guard<std::mutex> lock(mutex_);\n        tasks_.emplace([task]() {\n            (*task)();\n        });\n    }\n    scheduled_.notify_one();\n    return std::move(future);\n  }\n\n private:\n  void TaskLoop() {\n    while (true) {\n      Task task;\n      {\n        std::unique_lock<std::mutex> lock(mutex_);\n        scheduled_.wait(\n            lock, [this] { return !this->tasks_.empty() || !this->running_; });\n        if (!running_ && tasks_.empty()) {\n          return;\n        }\n        task = std::move(tasks_.front());\n        tasks_.pop();\n      }\n      task();\n    }\n  }\n  static void Init() {\n    if (threadpool_ == nullptr) {\n      int num_threads = std::thread::hardware_concurrency();\n      threadpool_.reset(new ThreadPool(num_threads));\n    }\n  }\n\n private:\n  static std::unique_ptr<ThreadPool> threadpool_;\n  static std::once_flag init_flag_;\n\n  std::vector<std::unique_ptr<std::thread>> threads_;\n  std::queue<Task> tasks_;\n  std::mutex mutex_;\n  bool running_;\n  std::condition_variable scheduled_;\n};\n\nstd::unique_ptr<ThreadPool> ThreadPool::threadpool_ = nullptr;\nstd::once_flag ThreadPool::init_flag_;\n```\n\n# 使用示例\n```\n#include <iostream>\n\n#include \"thread_pool.h\"\n\nstruct sum {\n  int operator()(int a, int b) {\n      int res = a + b;\n      return res;\n  }\n};\n\nint print(int a) {\n    return a;\n}\n\nclass A {\npublic:\n  static int calc(int val) {\n      return val;\n  }\n};\nint main() {\n  ThreadPool executor(10);\n  auto result = executor.Commit(&print, 3);\n  std::cout << \"result: \" << result.get() << std::endl;\n\n  // auto result = executor.Commit(A::calc, 3);\n  // std::cout << \"result: \" << result.get() << std::endl;\n  ThreadPool pool(4);\n  std::vector<std::future<int>> results;\n  std::chrono::seconds span(1);\n  for (int i = 0; i < 2; ++ i) {\n    results.emplace_back(\n        pool.Commit([i, span] {\n            std::cout << \"run \" << i << std::endl;\n            std::this_thread::sleep_for(span);\n            return i * i;\n        })\n    );\n  }\n\n  for (auto && item : results) {\n      if(item.wait_for(span) == std::future_status::ready) {\n        std::cout << item.get() << std::endl;\n      }\n  }\n\n  return 0;\n}\n```\n# 核心要点\n* 工作队列 work queu\n* thread factory\n* 饱和策略 handler","slug":"article/thread-pool","published":1,"updated":"2025-09-29T14:59:49.936Z","comments":1,"layout":"post","photos":[],"_id":"cmg599060000qotzkcp6t3vq6","content":"<h1 id=\"线程池\"><a href=\"#线程池\" class=\"headerlink\" title=\"线程池\"></a>线程池</h1><p>支持单例使用，支持任意参数的任务提交</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#pragma once</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;condition_variable&gt;</span><br><span class=\"line\">#include &lt;functional&gt;</span><br><span class=\"line\">#include &lt;future&gt;</span><br><span class=\"line\">#include &lt;memory&gt;</span><br><span class=\"line\">#include &lt;mutex&gt;</span><br><span class=\"line\">#include &lt;queue&gt;</span><br><span class=\"line\">#include &lt;thread&gt;</span><br><span class=\"line\">#include &lt;vector&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">class ThreadPool &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  using Task = std::function&lt;void()&gt;;</span><br><span class=\"line\">  explicit ThreadPool(int num_threads): running_(true) &#123;</span><br><span class=\"line\">    threads_.resize(num_threads);</span><br><span class=\"line\">    for (auto&amp; thread : threads_) &#123;</span><br><span class=\"line\">      thread.reset(new std::thread(&amp;ThreadPool::TaskLoop, this));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ~ThreadPool() &#123;</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">      running_ = false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    scheduled_.notify_all();</span><br><span class=\"line\">    for (auto&amp; thread : threads_) &#123;</span><br><span class=\"line\">      thread-&gt;join();</span><br><span class=\"line\">      thread.reset(nullptr);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  static ThreadPool* GetInstance() &#123;</span><br><span class=\"line\">    std::call_once(init_flag_, &amp;ThreadPool::Init);</span><br><span class=\"line\">    return threadpool_.get();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">  ThreadPool(const ThreadPool&amp; pool) = delete;</span><br><span class=\"line\">  ThreadPool&amp; operator=(const ThreadPool&amp; pool) = delete;</span><br><span class=\"line\">  template&lt;class F, class... Args&gt;</span><br><span class=\"line\">  auto Commit(F&amp;&amp; f, Args&amp;&amp;... args) -&gt; std::future&lt;decltype(f(args...))&gt; &#123;</span><br><span class=\"line\">    if (!running_) &#123;</span><br><span class=\"line\">      throw std::runtime_error(&quot;ThreadPool is not running&quot;);  </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    using RetType = decltype(f(args...));</span><br><span class=\"line\">    auto task = std::make_shared&lt;std::packaged_task&lt;RetType()&gt;&gt;(</span><br><span class=\"line\">      std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...)</span><br><span class=\"line\">    );</span><br><span class=\"line\">    std::future&lt;RetType&gt; future = task -&gt; get_future();</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        std::lock_guard&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">        tasks_.emplace([task]() &#123;</span><br><span class=\"line\">            (*task)();</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    scheduled_.notify_one();</span><br><span class=\"line\">    return std::move(future);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  void TaskLoop() &#123;</span><br><span class=\"line\">    while (true) &#123;</span><br><span class=\"line\">      Task task;</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">        scheduled_.wait(</span><br><span class=\"line\">            lock, [this] &#123; return !this-&gt;tasks_.empty() || !this-&gt;running_; &#125;);</span><br><span class=\"line\">        if (!running_ &amp;&amp; tasks_.empty()) &#123;</span><br><span class=\"line\">          return;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        task = std::move(tasks_.front());</span><br><span class=\"line\">        tasks_.pop();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      task();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  static void Init() &#123;</span><br><span class=\"line\">    if (threadpool_ == nullptr) &#123;</span><br><span class=\"line\">      int num_threads = std::thread::hardware_concurrency();</span><br><span class=\"line\">      threadpool_.reset(new ThreadPool(num_threads));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  static std::unique_ptr&lt;ThreadPool&gt; threadpool_;</span><br><span class=\"line\">  static std::once_flag init_flag_;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;std::thread&gt;&gt; threads_;</span><br><span class=\"line\">  std::queue&lt;Task&gt; tasks_;</span><br><span class=\"line\">  std::mutex mutex_;</span><br><span class=\"line\">  bool running_;</span><br><span class=\"line\">  std::condition_variable scheduled_;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">std::unique_ptr&lt;ThreadPool&gt; ThreadPool::threadpool_ = nullptr;</span><br><span class=\"line\">std::once_flag ThreadPool::init_flag_;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"使用示例\"><a href=\"#使用示例\" class=\"headerlink\" title=\"使用示例\"></a>使用示例</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;iostream&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &quot;thread_pool.h&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">struct sum &#123;</span><br><span class=\"line\">  int operator()(int a, int b) &#123;</span><br><span class=\"line\">      int res = a + b;</span><br><span class=\"line\">      return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">int print(int a) &#123;</span><br><span class=\"line\">    return a;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class A &#123;</span><br><span class=\"line\">public:</span><br><span class=\"line\">  static int calc(int val) &#123;</span><br><span class=\"line\">      return val;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">int main() &#123;</span><br><span class=\"line\">  ThreadPool executor(10);</span><br><span class=\"line\">  auto result = executor.Commit(&amp;print, 3);</span><br><span class=\"line\">  std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class=\"line\"></span><br><span class=\"line\">  // auto result = executor.Commit(A::calc, 3);</span><br><span class=\"line\">  // std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class=\"line\">  ThreadPool pool(4);</span><br><span class=\"line\">  std::vector&lt;std::future&lt;int&gt;&gt; results;</span><br><span class=\"line\">  std::chrono::seconds span(1);</span><br><span class=\"line\">  for (int i = 0; i &lt; 2; ++ i) &#123;</span><br><span class=\"line\">    results.emplace_back(</span><br><span class=\"line\">        pool.Commit([i, span] &#123;</span><br><span class=\"line\">            std::cout &lt;&lt; &quot;run &quot; &lt;&lt; i &lt;&lt; std::endl;</span><br><span class=\"line\">            std::this_thread::sleep_for(span);</span><br><span class=\"line\">            return i * i;</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  for (auto &amp;&amp; item : results) &#123;</span><br><span class=\"line\">      if(item.wait_for(span) == std::future_status::ready) &#123;</span><br><span class=\"line\">        std::cout &lt;&lt; item.get() &lt;&lt; std::endl;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  return 0;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"核心要点\"><a href=\"#核心要点\" class=\"headerlink\" title=\"核心要点\"></a>核心要点</h1><ul>\n<li>工作队列 work queu</li>\n<li>thread factory</li>\n<li>饱和策略 handler</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"线程池\"><a href=\"#线程池\" class=\"headerlink\" title=\"线程池\"></a>线程池</h1><p>支持单例使用，支持任意参数的任务提交</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#pragma once</span><br><span class=\"line\"></span><br><span class=\"line\">#include &lt;condition_variable&gt;</span><br><span class=\"line\">#include &lt;functional&gt;</span><br><span class=\"line\">#include &lt;future&gt;</span><br><span class=\"line\">#include &lt;memory&gt;</span><br><span class=\"line\">#include &lt;mutex&gt;</span><br><span class=\"line\">#include &lt;queue&gt;</span><br><span class=\"line\">#include &lt;thread&gt;</span><br><span class=\"line\">#include &lt;vector&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">class ThreadPool &#123;</span><br><span class=\"line\"> public:</span><br><span class=\"line\">  using Task = std::function&lt;void()&gt;;</span><br><span class=\"line\">  explicit ThreadPool(int num_threads): running_(true) &#123;</span><br><span class=\"line\">    threads_.resize(num_threads);</span><br><span class=\"line\">    for (auto&amp; thread : threads_) &#123;</span><br><span class=\"line\">      thread.reset(new std::thread(&amp;ThreadPool::TaskLoop, this));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ~ThreadPool() &#123;</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">      running_ = false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    scheduled_.notify_all();</span><br><span class=\"line\">    for (auto&amp; thread : threads_) &#123;</span><br><span class=\"line\">      thread-&gt;join();</span><br><span class=\"line\">      thread.reset(nullptr);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  static ThreadPool* GetInstance() &#123;</span><br><span class=\"line\">    std::call_once(init_flag_, &amp;ThreadPool::Init);</span><br><span class=\"line\">    return threadpool_.get();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br><span class=\"line\">  ThreadPool(const ThreadPool&amp; pool) = delete;</span><br><span class=\"line\">  ThreadPool&amp; operator=(const ThreadPool&amp; pool) = delete;</span><br><span class=\"line\">  template&lt;class F, class... Args&gt;</span><br><span class=\"line\">  auto Commit(F&amp;&amp; f, Args&amp;&amp;... args) -&gt; std::future&lt;decltype(f(args...))&gt; &#123;</span><br><span class=\"line\">    if (!running_) &#123;</span><br><span class=\"line\">      throw std::runtime_error(&quot;ThreadPool is not running&quot;);  </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    using RetType = decltype(f(args...));</span><br><span class=\"line\">    auto task = std::make_shared&lt;std::packaged_task&lt;RetType()&gt;&gt;(</span><br><span class=\"line\">      std::bind(std::forward&lt;F&gt;(f), std::forward&lt;Args&gt;(args)...)</span><br><span class=\"line\">    );</span><br><span class=\"line\">    std::future&lt;RetType&gt; future = task -&gt; get_future();</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        std::lock_guard&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">        tasks_.emplace([task]() &#123;</span><br><span class=\"line\">            (*task)();</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    scheduled_.notify_one();</span><br><span class=\"line\">    return std::move(future);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  void TaskLoop() &#123;</span><br><span class=\"line\">    while (true) &#123;</span><br><span class=\"line\">      Task task;</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        std::unique_lock&lt;std::mutex&gt; lock(mutex_);</span><br><span class=\"line\">        scheduled_.wait(</span><br><span class=\"line\">            lock, [this] &#123; return !this-&gt;tasks_.empty() || !this-&gt;running_; &#125;);</span><br><span class=\"line\">        if (!running_ &amp;&amp; tasks_.empty()) &#123;</span><br><span class=\"line\">          return;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        task = std::move(tasks_.front());</span><br><span class=\"line\">        tasks_.pop();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      task();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  static void Init() &#123;</span><br><span class=\"line\">    if (threadpool_ == nullptr) &#123;</span><br><span class=\"line\">      int num_threads = std::thread::hardware_concurrency();</span><br><span class=\"line\">      threadpool_.reset(new ThreadPool(num_threads));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private:</span><br><span class=\"line\">  static std::unique_ptr&lt;ThreadPool&gt; threadpool_;</span><br><span class=\"line\">  static std::once_flag init_flag_;</span><br><span class=\"line\"></span><br><span class=\"line\">  std::vector&lt;std::unique_ptr&lt;std::thread&gt;&gt; threads_;</span><br><span class=\"line\">  std::queue&lt;Task&gt; tasks_;</span><br><span class=\"line\">  std::mutex mutex_;</span><br><span class=\"line\">  bool running_;</span><br><span class=\"line\">  std::condition_variable scheduled_;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">std::unique_ptr&lt;ThreadPool&gt; ThreadPool::threadpool_ = nullptr;</span><br><span class=\"line\">std::once_flag ThreadPool::init_flag_;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"使用示例\"><a href=\"#使用示例\" class=\"headerlink\" title=\"使用示例\"></a>使用示例</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;iostream&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">#include &quot;thread_pool.h&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">struct sum &#123;</span><br><span class=\"line\">  int operator()(int a, int b) &#123;</span><br><span class=\"line\">      int res = a + b;</span><br><span class=\"line\">      return res;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">int print(int a) &#123;</span><br><span class=\"line\">    return a;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">class A &#123;</span><br><span class=\"line\">public:</span><br><span class=\"line\">  static int calc(int val) &#123;</span><br><span class=\"line\">      return val;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">int main() &#123;</span><br><span class=\"line\">  ThreadPool executor(10);</span><br><span class=\"line\">  auto result = executor.Commit(&amp;print, 3);</span><br><span class=\"line\">  std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class=\"line\"></span><br><span class=\"line\">  // auto result = executor.Commit(A::calc, 3);</span><br><span class=\"line\">  // std::cout &lt;&lt; &quot;result: &quot; &lt;&lt; result.get() &lt;&lt; std::endl;</span><br><span class=\"line\">  ThreadPool pool(4);</span><br><span class=\"line\">  std::vector&lt;std::future&lt;int&gt;&gt; results;</span><br><span class=\"line\">  std::chrono::seconds span(1);</span><br><span class=\"line\">  for (int i = 0; i &lt; 2; ++ i) &#123;</span><br><span class=\"line\">    results.emplace_back(</span><br><span class=\"line\">        pool.Commit([i, span] &#123;</span><br><span class=\"line\">            std::cout &lt;&lt; &quot;run &quot; &lt;&lt; i &lt;&lt; std::endl;</span><br><span class=\"line\">            std::this_thread::sleep_for(span);</span><br><span class=\"line\">            return i * i;</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  for (auto &amp;&amp; item : results) &#123;</span><br><span class=\"line\">      if(item.wait_for(span) == std::future_status::ready) &#123;</span><br><span class=\"line\">        std::cout &lt;&lt; item.get() &lt;&lt; std::endl;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  return 0;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"核心要点\"><a href=\"#核心要点\" class=\"headerlink\" title=\"核心要点\"></a>核心要点</h1><ul>\n<li>工作队列 work queu</li>\n<li>thread factory</li>\n<li>饱和策略 handler</li>\n</ul>\n"},{"title":"可变模版参数","date":"2021-04-04T06:06:42.000Z","_content":"\n# c++11 可变模版参数\n```\ntemplate <class... T>\nvoid f(T... args);\n```\n* 递归函数展开参数包\n```\n#include <iostream>\n\ntemplate <class... T>\nvoid f(T... args)\n{    \n  std::cout << sizeof...(args) << std::endl;\n}\n\nvoid func() {}\ntemplate <class T, class... Args>\nvoid func(T first, Args... remain) {\n  std::cout << first << \" \";\n  if (sizeof...(remain) == 0) return;\n  func(remain...);\n}\n\nint main() {\n  func(2, 3, 9);\n  return 0;\n}\n```\n* 逗号表达式展开参数包","source":"_posts/article/variadic-templates.md","raw":"---\ntitle: 可变模版参数\ndate: 2021-04-04 14:06:42\ntags: c++\n---\n\n# c++11 可变模版参数\n```\ntemplate <class... T>\nvoid f(T... args);\n```\n* 递归函数展开参数包\n```\n#include <iostream>\n\ntemplate <class... T>\nvoid f(T... args)\n{    \n  std::cout << sizeof...(args) << std::endl;\n}\n\nvoid func() {}\ntemplate <class T, class... Args>\nvoid func(T first, Args... remain) {\n  std::cout << first << \" \";\n  if (sizeof...(remain) == 0) return;\n  func(remain...);\n}\n\nint main() {\n  func(2, 3, 9);\n  return 0;\n}\n```\n* 逗号表达式展开参数包","slug":"article/variadic-templates","published":1,"updated":"2025-09-29T14:59:49.936Z","comments":1,"layout":"post","photos":[],"_id":"cmg599060000rotzk9lhs15p3","content":"<h1 id=\"c-11-可变模版参数\"><a href=\"#c-11-可变模版参数\" class=\"headerlink\" title=\"c++11 可变模版参数\"></a>c++11 可变模版参数</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class... T&gt;</span><br><span class=\"line\">void f(T... args);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>递归函数展开参数包<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;iostream&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">template &lt;class... T&gt;</span><br><span class=\"line\">void f(T... args)</span><br><span class=\"line\">&#123;    </span><br><span class=\"line\">  std::cout &lt;&lt; sizeof...(args) &lt;&lt; std::endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">void func() &#123;&#125;</span><br><span class=\"line\">template &lt;class T, class... Args&gt;</span><br><span class=\"line\">void func(T first, Args... remain) &#123;</span><br><span class=\"line\">  std::cout &lt;&lt; first &lt;&lt; &quot; &quot;;</span><br><span class=\"line\">  if (sizeof...(remain) == 0) return;</span><br><span class=\"line\">  func(remain...);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">int main() &#123;</span><br><span class=\"line\">  func(2, 3, 9);</span><br><span class=\"line\">  return 0;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>逗号表达式展开参数包</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"c-11-可变模版参数\"><a href=\"#c-11-可变模版参数\" class=\"headerlink\" title=\"c++11 可变模版参数\"></a>c++11 可变模版参数</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template &lt;class... T&gt;</span><br><span class=\"line\">void f(T... args);</span><br></pre></td></tr></table></figure>\n<ul>\n<li>递归函数展开参数包<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;iostream&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">template &lt;class... T&gt;</span><br><span class=\"line\">void f(T... args)</span><br><span class=\"line\">&#123;    </span><br><span class=\"line\">  std::cout &lt;&lt; sizeof...(args) &lt;&lt; std::endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">void func() &#123;&#125;</span><br><span class=\"line\">template &lt;class T, class... Args&gt;</span><br><span class=\"line\">void func(T first, Args... remain) &#123;</span><br><span class=\"line\">  std::cout &lt;&lt; first &lt;&lt; &quot; &quot;;</span><br><span class=\"line\">  if (sizeof...(remain) == 0) return;</span><br><span class=\"line\">  func(remain...);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">int main() &#123;</span><br><span class=\"line\">  func(2, 3, 9);</span><br><span class=\"line\">  return 0;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li>逗号表达式展开参数包</li>\n</ul>\n"},{"title":"VPN service","date":"2023-01-02T03:26:53.000Z","_content":"\n# 使用VPS搭建VPN代理\n## 原理\n<img width=\"572\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1312389/210215276-f2394f9e-32b3-47ab-8925-0dcc104adadb.png\">\n\n## 准备工作\n### 1、免费域名\n- freenom免费域名申请\n   申请地址：https://my.freenom.com/\n   申请时需要保证个人资料地址信息与网络ip地址信息一致；国内ip环境，需使用Gooreplacer chrome插件将www.google.com/recaptcha 重定向recaptcha.net/recaptcha\n- eu.org免费域名申请\n  申请地址：https://nic.eu.org/arf/en (需使用代理)\n- https://pp.ua/ 免费域名申请\n\n### 2、域名解析\n- https://topdn.net\n   配置简单，更新快速\n- cloudflare\n  个人推荐cloudflare，功能齐全，同时能实现ip地址隐藏\n\n### 3、CDN（可选）\n   https://www.cloudflare.com/ 可以实现vps ip地址隐藏，同时也可以解析到境外已被墙ip（例如阿里云香港主机）\n## 搭建步骤\n### 1、v2ray或者trojan服务器伪装\n### 2、客户端v2rayNG配置\n### 3、CDN流量中转（可选）\n   流量中转目的：1、隐藏VPS ip；2、解救被海外封ip\n\n## 参考链接\n- [v2ray使用cloudflare中转流量，拯救被墙ip](https://v2xtls.org/v2ray%E4%BD%BF%E7%94%A8cloudflare%E4%B8%AD%E8%BD%AC%E6%B5%81%E9%87%8F%EF%BC%8C%E6%8B%AF%E6%95%91%E8%A2%AB%E5%A2%99ip/)\n- [V2Ray高级技巧：流量伪装](https://itlanyan.com/v2ray-traffic-mask/)\n- [拯救被墙的服务器](https://itlanyan.com/recovery-blocked-ip/)\n- [V2ray的VLESS协议介绍和使用教程](https://itlanyan.com/introduce-v2ray-vless-protocol/)\n- [trojan教程](https://itlanyan.com/trojan-tutorial/)\n- [未来的霸主选项：Xray（XTLS+V2ray)](https://www.vjsun.com/656.html)\n- [Trojan-Go一键安装脚本（Debian/Ubuntu） Trojan-Go搭建/支持Cloudflare CDN](https://ssrvps.org/archives/7772)\n","source":"_posts/article/vpn-service.md","raw":"---\ntitle: VPN service\ndate: 2023-01-02 11:26:53\ntags:\n---\n\n# 使用VPS搭建VPN代理\n## 原理\n<img width=\"572\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1312389/210215276-f2394f9e-32b3-47ab-8925-0dcc104adadb.png\">\n\n## 准备工作\n### 1、免费域名\n- freenom免费域名申请\n   申请地址：https://my.freenom.com/\n   申请时需要保证个人资料地址信息与网络ip地址信息一致；国内ip环境，需使用Gooreplacer chrome插件将www.google.com/recaptcha 重定向recaptcha.net/recaptcha\n- eu.org免费域名申请\n  申请地址：https://nic.eu.org/arf/en (需使用代理)\n- https://pp.ua/ 免费域名申请\n\n### 2、域名解析\n- https://topdn.net\n   配置简单，更新快速\n- cloudflare\n  个人推荐cloudflare，功能齐全，同时能实现ip地址隐藏\n\n### 3、CDN（可选）\n   https://www.cloudflare.com/ 可以实现vps ip地址隐藏，同时也可以解析到境外已被墙ip（例如阿里云香港主机）\n## 搭建步骤\n### 1、v2ray或者trojan服务器伪装\n### 2、客户端v2rayNG配置\n### 3、CDN流量中转（可选）\n   流量中转目的：1、隐藏VPS ip；2、解救被海外封ip\n\n## 参考链接\n- [v2ray使用cloudflare中转流量，拯救被墙ip](https://v2xtls.org/v2ray%E4%BD%BF%E7%94%A8cloudflare%E4%B8%AD%E8%BD%AC%E6%B5%81%E9%87%8F%EF%BC%8C%E6%8B%AF%E6%95%91%E8%A2%AB%E5%A2%99ip/)\n- [V2Ray高级技巧：流量伪装](https://itlanyan.com/v2ray-traffic-mask/)\n- [拯救被墙的服务器](https://itlanyan.com/recovery-blocked-ip/)\n- [V2ray的VLESS协议介绍和使用教程](https://itlanyan.com/introduce-v2ray-vless-protocol/)\n- [trojan教程](https://itlanyan.com/trojan-tutorial/)\n- [未来的霸主选项：Xray（XTLS+V2ray)](https://www.vjsun.com/656.html)\n- [Trojan-Go一键安装脚本（Debian/Ubuntu） Trojan-Go搭建/支持Cloudflare CDN](https://ssrvps.org/archives/7772)\n","slug":"article/vpn-service","published":1,"updated":"2025-09-29T14:59:49.934Z","comments":1,"layout":"post","photos":[],"_id":"cmg599060000sotzkbb6ugdue","content":"<h1 id=\"使用VPS搭建VPN代理\"><a href=\"#使用VPS搭建VPN代理\" class=\"headerlink\" title=\"使用VPS搭建VPN代理\"></a>使用VPS搭建VPN代理</h1><h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><img width=\"572\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1312389/210215276-f2394f9e-32b3-47ab-8925-0dcc104adadb.png\">\n\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><h3 id=\"1、免费域名\"><a href=\"#1、免费域名\" class=\"headerlink\" title=\"1、免费域名\"></a>1、免费域名</h3><ul>\n<li>freenom免费域名申请<br> 申请地址：<a href=\"https://my.freenom.com/\">https://my.freenom.com/</a><br> 申请时需要保证个人资料地址信息与网络ip地址信息一致；国内ip环境，需使用Gooreplacer chrome插件将<a href=\"http://www.google.com/recaptcha\">www.google.com/recaptcha</a> 重定向recaptcha.net&#x2F;recaptcha</li>\n<li>eu.org免费域名申请<br>申请地址：<a href=\"https://nic.eu.org/arf/en\">https://nic.eu.org/arf/en</a> (需使用代理)</li>\n<li><a href=\"https://pp.ua/\">https://pp.ua/</a> 免费域名申请</li>\n</ul>\n<h3 id=\"2、域名解析\"><a href=\"#2、域名解析\" class=\"headerlink\" title=\"2、域名解析\"></a>2、域名解析</h3><ul>\n<li><a href=\"https://topdn.net/\">https://topdn.net</a><br> 配置简单，更新快速</li>\n<li>cloudflare<br>个人推荐cloudflare，功能齐全，同时能实现ip地址隐藏</li>\n</ul>\n<h3 id=\"3、CDN（可选）\"><a href=\"#3、CDN（可选）\" class=\"headerlink\" title=\"3、CDN（可选）\"></a>3、CDN（可选）</h3><p>   <a href=\"https://www.cloudflare.com/\">https://www.cloudflare.com/</a> 可以实现vps ip地址隐藏，同时也可以解析到境外已被墙ip（例如阿里云香港主机）</p>\n<h2 id=\"搭建步骤\"><a href=\"#搭建步骤\" class=\"headerlink\" title=\"搭建步骤\"></a>搭建步骤</h2><h3 id=\"1、v2ray或者trojan服务器伪装\"><a href=\"#1、v2ray或者trojan服务器伪装\" class=\"headerlink\" title=\"1、v2ray或者trojan服务器伪装\"></a>1、v2ray或者trojan服务器伪装</h3><h3 id=\"2、客户端v2rayNG配置\"><a href=\"#2、客户端v2rayNG配置\" class=\"headerlink\" title=\"2、客户端v2rayNG配置\"></a>2、客户端v2rayNG配置</h3><h3 id=\"3、CDN流量中转（可选）\"><a href=\"#3、CDN流量中转（可选）\" class=\"headerlink\" title=\"3、CDN流量中转（可选）\"></a>3、CDN流量中转（可选）</h3><p>   流量中转目的：1、隐藏VPS ip；2、解救被海外封ip</p>\n<h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul>\n<li><a href=\"https://v2xtls.org/v2ray%E4%BD%BF%E7%94%A8cloudflare%E4%B8%AD%E8%BD%AC%E6%B5%81%E9%87%8F%EF%BC%8C%E6%8B%AF%E6%95%91%E8%A2%AB%E5%A2%99ip/\">v2ray使用cloudflare中转流量，拯救被墙ip</a></li>\n<li><a href=\"https://itlanyan.com/v2ray-traffic-mask/\">V2Ray高级技巧：流量伪装</a></li>\n<li><a href=\"https://itlanyan.com/recovery-blocked-ip/\">拯救被墙的服务器</a></li>\n<li><a href=\"https://itlanyan.com/introduce-v2ray-vless-protocol/\">V2ray的VLESS协议介绍和使用教程</a></li>\n<li><a href=\"https://itlanyan.com/trojan-tutorial/\">trojan教程</a></li>\n<li><a href=\"https://www.vjsun.com/656.html\">未来的霸主选项：Xray（XTLS+V2ray)</a></li>\n<li><a href=\"https://ssrvps.org/archives/7772\">Trojan-Go一键安装脚本（Debian&#x2F;Ubuntu） Trojan-Go搭建&#x2F;支持Cloudflare CDN</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"使用VPS搭建VPN代理\"><a href=\"#使用VPS搭建VPN代理\" class=\"headerlink\" title=\"使用VPS搭建VPN代理\"></a>使用VPS搭建VPN代理</h1><h2 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h2><img width=\"572\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1312389/210215276-f2394f9e-32b3-47ab-8925-0dcc104adadb.png\">\n\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><h3 id=\"1、免费域名\"><a href=\"#1、免费域名\" class=\"headerlink\" title=\"1、免费域名\"></a>1、免费域名</h3><ul>\n<li>freenom免费域名申请<br> 申请地址：<a href=\"https://my.freenom.com/\">https://my.freenom.com/</a><br> 申请时需要保证个人资料地址信息与网络ip地址信息一致；国内ip环境，需使用Gooreplacer chrome插件将<a href=\"http://www.google.com/recaptcha\">www.google.com/recaptcha</a> 重定向recaptcha.net&#x2F;recaptcha</li>\n<li>eu.org免费域名申请<br>申请地址：<a href=\"https://nic.eu.org/arf/en\">https://nic.eu.org/arf/en</a> (需使用代理)</li>\n<li><a href=\"https://pp.ua/\">https://pp.ua/</a> 免费域名申请</li>\n</ul>\n<h3 id=\"2、域名解析\"><a href=\"#2、域名解析\" class=\"headerlink\" title=\"2、域名解析\"></a>2、域名解析</h3><ul>\n<li><a href=\"https://topdn.net/\">https://topdn.net</a><br> 配置简单，更新快速</li>\n<li>cloudflare<br>个人推荐cloudflare，功能齐全，同时能实现ip地址隐藏</li>\n</ul>\n<h3 id=\"3、CDN（可选）\"><a href=\"#3、CDN（可选）\" class=\"headerlink\" title=\"3、CDN（可选）\"></a>3、CDN（可选）</h3><p>   <a href=\"https://www.cloudflare.com/\">https://www.cloudflare.com/</a> 可以实现vps ip地址隐藏，同时也可以解析到境外已被墙ip（例如阿里云香港主机）</p>\n<h2 id=\"搭建步骤\"><a href=\"#搭建步骤\" class=\"headerlink\" title=\"搭建步骤\"></a>搭建步骤</h2><h3 id=\"1、v2ray或者trojan服务器伪装\"><a href=\"#1、v2ray或者trojan服务器伪装\" class=\"headerlink\" title=\"1、v2ray或者trojan服务器伪装\"></a>1、v2ray或者trojan服务器伪装</h3><h3 id=\"2、客户端v2rayNG配置\"><a href=\"#2、客户端v2rayNG配置\" class=\"headerlink\" title=\"2、客户端v2rayNG配置\"></a>2、客户端v2rayNG配置</h3><h3 id=\"3、CDN流量中转（可选）\"><a href=\"#3、CDN流量中转（可选）\" class=\"headerlink\" title=\"3、CDN流量中转（可选）\"></a>3、CDN流量中转（可选）</h3><p>   流量中转目的：1、隐藏VPS ip；2、解救被海外封ip</p>\n<h2 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h2><ul>\n<li><a href=\"https://v2xtls.org/v2ray%E4%BD%BF%E7%94%A8cloudflare%E4%B8%AD%E8%BD%AC%E6%B5%81%E9%87%8F%EF%BC%8C%E6%8B%AF%E6%95%91%E8%A2%AB%E5%A2%99ip/\">v2ray使用cloudflare中转流量，拯救被墙ip</a></li>\n<li><a href=\"https://itlanyan.com/v2ray-traffic-mask/\">V2Ray高级技巧：流量伪装</a></li>\n<li><a href=\"https://itlanyan.com/recovery-blocked-ip/\">拯救被墙的服务器</a></li>\n<li><a href=\"https://itlanyan.com/introduce-v2ray-vless-protocol/\">V2ray的VLESS协议介绍和使用教程</a></li>\n<li><a href=\"https://itlanyan.com/trojan-tutorial/\">trojan教程</a></li>\n<li><a href=\"https://www.vjsun.com/656.html\">未来的霸主选项：Xray（XTLS+V2ray)</a></li>\n<li><a href=\"https://ssrvps.org/archives/7772\">Trojan-Go一键安装脚本（Debian&#x2F;Ubuntu） Trojan-Go搭建&#x2F;支持Cloudflare CDN</a></li>\n</ul>\n"},{"title":"work stealing","date":"2022-03-19T08:00:35.000Z","_content":"\n# work stealing","source":"_posts/article/workstealing.md","raw":"---\ntitle: work stealing\ndate: 2022-03-19 16:00:35\ntags:\n---\n\n# work stealing","slug":"article/workstealing","published":1,"updated":"2025-09-29T14:59:49.937Z","comments":1,"layout":"post","photos":[],"_id":"cmg599060000votzk8pia304o","content":"<h1 id=\"work-stealing\"><a href=\"#work-stealing\" class=\"headerlink\" title=\"work stealing\"></a>work stealing</h1>","excerpt":"","more":"<h1 id=\"work-stealing\"><a href=\"#work-stealing\" class=\"headerlink\" title=\"work stealing\"></a>work stealing</h1>"},{"title":"gcc","date":"2021-04-13T19:51:57.000Z","_content":"\n# content\n\n## RVO(Return Value Optimization) \n## NRVO(Named Return Value Optimization)\n\n## code snipts\n```\n#include <algorithm>\n#include <cctype>\n\nstd::string lower(const std::string &data) {\n  std::string result = data;\n  std::transform(result.begin(), result.end(), result.begin(),\n    [](unsigned char c){ return std::tolower(c); });\n  return result;\n}\n```","source":"_posts/article/cplusplus/gcc.md","raw":"---\ntitle: gcc\ndate: 2021-04-14 03:51:57\ntags:\n---\n\n# content\n\n## RVO(Return Value Optimization) \n## NRVO(Named Return Value Optimization)\n\n## code snipts\n```\n#include <algorithm>\n#include <cctype>\n\nstd::string lower(const std::string &data) {\n  std::string result = data;\n  std::transform(result.begin(), result.end(), result.begin(),\n    [](unsigned char c){ return std::tolower(c); });\n  return result;\n}\n```","slug":"article/cplusplus/gcc","published":1,"updated":"2025-09-29T14:59:49.934Z","comments":1,"layout":"post","photos":[],"_id":"cmg599060000wotzkewo1glbz","content":"<h1 id=\"content\"><a href=\"#content\" class=\"headerlink\" title=\"content\"></a>content</h1><h2 id=\"RVO-Return-Value-Optimization\"><a href=\"#RVO-Return-Value-Optimization\" class=\"headerlink\" title=\"RVO(Return Value Optimization)\"></a>RVO(Return Value Optimization)</h2><h2 id=\"NRVO-Named-Return-Value-Optimization\"><a href=\"#NRVO-Named-Return-Value-Optimization\" class=\"headerlink\" title=\"NRVO(Named Return Value Optimization)\"></a>NRVO(Named Return Value Optimization)</h2><h2 id=\"code-snipts\"><a href=\"#code-snipts\" class=\"headerlink\" title=\"code snipts\"></a>code snipts</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;algorithm&gt;</span><br><span class=\"line\">#include &lt;cctype&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">std::string lower(const std::string &amp;data) &#123;</span><br><span class=\"line\">  std::string result = data;</span><br><span class=\"line\">  std::transform(result.begin(), result.end(), result.begin(),</span><br><span class=\"line\">    [](unsigned char c)&#123; return std::tolower(c); &#125;);</span><br><span class=\"line\">  return result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"content\"><a href=\"#content\" class=\"headerlink\" title=\"content\"></a>content</h1><h2 id=\"RVO-Return-Value-Optimization\"><a href=\"#RVO-Return-Value-Optimization\" class=\"headerlink\" title=\"RVO(Return Value Optimization)\"></a>RVO(Return Value Optimization)</h2><h2 id=\"NRVO-Named-Return-Value-Optimization\"><a href=\"#NRVO-Named-Return-Value-Optimization\" class=\"headerlink\" title=\"NRVO(Named Return Value Optimization)\"></a>NRVO(Named Return Value Optimization)</h2><h2 id=\"code-snipts\"><a href=\"#code-snipts\" class=\"headerlink\" title=\"code snipts\"></a>code snipts</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#include &lt;algorithm&gt;</span><br><span class=\"line\">#include &lt;cctype&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">std::string lower(const std::string &amp;data) &#123;</span><br><span class=\"line\">  std::string result = data;</span><br><span class=\"line\">  std::transform(result.begin(), result.end(), result.begin(),</span><br><span class=\"line\">    [](unsigned char c)&#123; return std::tolower(c); &#125;);</span><br><span class=\"line\">  return result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cmg59905z000notzkbo73dhat","tag_id":"cmg59905z000potzk74syav7m","_id":"cmg599060000uotzk9zggf9i8"},{"post_id":"cmg599060000rotzk9lhs15p3","tag_id":"cmg599060000totzk41vvesqh","_id":"cmg599061000xotzk4e3h8rdc"}],"Tag":[{"name":"GPU, TensorRT","_id":"cmg59905z000potzk74syav7m"},{"name":"c++","_id":"cmg599060000totzk41vvesqh"}]}}